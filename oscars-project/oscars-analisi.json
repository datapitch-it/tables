[
  {
    "id": "01jtz6rb7ww6vczda0t8nqft10",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "Ho un'idea di progetto. Voglio applicare a un bando di finanziamento. Ho i documenti che definiscono i requisiti del bando. Il bando ha finanziato in basso altri progetti. Il tuo compito è definire una tattica rigorosa (todo list) per verificare se la mia idea progettuale è competitiva per quseto bando",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Ottima idea! Procedere in modo strutturato è fondamentale. Ecco una \"todo list\" rigorosa per verificare la competitività della tua idea progettuale rispetto al bando:\n\n**Fase 1: Immersione Totale nel Bando (Comprendere le Regole del Gioco)**\n\n1.  **[ ] Lettura Analitica Completa:** Leggi e rileggi *ogni singola pagina* dei documenti del bando (bando integrale, allegati, FAQ, guide alla compilazione, ecc.). Non saltare nulla.\n2.  **[ ] Scomposizione degli Obiettivi del Bando:**\n    *   Identifica e lista gli **obiettivi generali e specifici** che il bando intende finanziare. Quali problemi vuole risolvere? Quali risultati si aspetta?\n    *   Sottolinea le **parole chiave** e i concetti ricorrenti.\n3.  **[ ] Analisi Dettagliata dei Criteri di Valutazione:**\n    *   Lista ogni singolo criterio di valutazione (es. innovatività, impatto, fattibilità, qualità del partenariato, coerenza col bando, sostenibilità).\n    *   Per ogni criterio, identifica il **punteggio massimo** attribuibile.\n    *   Comprendi *come* ogni criterio verrà valutato (descrittori, indicatori specifici menzionati).\n4.  **[ ] Verifica dei Requisiti di Ammissibilità (Formale):**\n    *   Lista tutti i requisiti formali per il proponente (natura giuridica, sede, ecc.) e per il progetto (durata, budget min/max, aree tematiche, tipologia di attività ammissibili/non ammissibili, ecc.).\n    *   Segna quelli che la tua idea/organizzazione soddisfa immediatamente e quelli che richiedono verifiche o adeguamenti.\n5.  **[ ] Studio delle Condizioni Finanziarie:**\n    *   Percentuale di cofinanziamento richiesta.\n    *   Tipologie di spese ammissibili e non ammissibili.\n    *   Massimali per singole voci di costo o categorie.\n6.  **[ ] Identificazione del Target e dei Beneficiari Attesi dal Bando:**\n    *   Chi sono i destinatari finali che il bando vuole raggiungere?\n    *   Quali tipologie di impatto (sociale, economico, ambientale, tecnologico) sono privilegiate?\n7.  **[ ] Definizione delle Scadenze Chiave:** Non solo la deadline di presentazione, ma anche eventuali scadenze intermedie per quesiti, ecc.\n\n**Fase 2: Auto-Analisi Critica della Tua Idea Progettuale**\n\n1.  **[ ] Descrizione Chiara e Concisa dell'Idea:** Scrivi un abstract del tuo progetto (problema, soluzione proposta, attività principali, risultati attesi, innovatività).\n2.  **[ ] Mappatura Idea vs. Obiettivi del Bando:**\n    *   Crea una tabella: colonna 1 = obiettivi del bando (dal punto 1.2); colonna 2 = come la tua idea risponde a ciascun obiettivo.\n    *   Sii onesto: dove c'è forte allineamento? Dove è debole o assente?\n3.  **[ ] Valutazione Preliminare sui Criteri del Bando:**\n    *   Riprendi la lista dei criteri di valutazione (dal punto 1.3).\n    *   Per ogni criterio, fai una prima autovalutazione (anche qualitativa: alto, medio, basso) di come la tua idea si posiziona. Motiva brevemente.\n4.  **[ ] Identificazione dell'Innovazione e del Valore Aggiunto:**\n    *   Cosa rende la tua idea *nuova* o significativamente *migliorativa* rispetto all'esistente (stato dell'arte, soluzioni attuali)?\n    *   Qual è il suo specifico valore aggiunto per i beneficiari target?\n5.  **[ ] Analisi di Fattibilità Preliminare:**\n    *   **Tecnica/Operativa:** Hai le competenze, le tecnologie, le metodologie per realizzarla?\n    *   **Gestionale:** Hai un team (o puoi formarlo) con le capacità per gestire il progetto?\n    *   **Finanziaria:** Il budget ipotizzato è realistico e coerente con le attività? Riesci a coprire l'eventuale cofinanziamento?\n6.  **[ ] Verifica della Coerenza con i Requisiti di Ammissibilità:** La tua idea e la tua organizzazione rispettano *tutti* i paletti formali (dal punto 1.4)?\n\n**Fase 3: Analisi Comparativa (Studio dei Progetti Già Finanziati)**\n\n1.  **[ ] Raccolta Informazioni sui Progetti Vincitori:**\n    *   Cerca sul sito dell'ente finanziatore, comunicati stampa, notizie, eventuali database pubblici.\n    *   Focalizzati sui progetti finanziati nelle edizioni più recenti dello stesso bando o bandi simili dello stesso ente.\n2.  **[ ] Analisi dei Progetti Finanziati:** Per ciascun progetto identificato:\n    *   **Tema/Settore:** In che ambito opera?\n    *   **Obiettivi Dichiarati:** Quali problemi affrontavano?\n    *   **Tipologia di Attività:** Cosa hanno fatto concretamente?\n    *   **Partenariato (se noto):** Chi erano i partner? Che ruolo avevano?\n    *   **Budget Approssimativo (se noto):** Qual era l'ordine di grandezza del finanziamento?\n    *   **Elementi di Innovazione/Forza (deducibili):** Cosa li ha resi vincenti, secondo te, alla luce dei criteri del bando?\n3.  **[ ] Identificazione di Pattern e Tendenze:**\n    *   Ci sono tipologie di progetti o approcci che vengono finanziati più frequentemente?\n    *   Quali sono le dimensioni medie dei progetti (budget, durata, numero partner)?\n    *   L'ente finanziatore mostra preferenze per certi tipi di innovazione o impatto?\n\n**Fase 4: Gap Analysis e Raffinamento Strategico**\n\n1.  **[ ] Confronto Diretto: Tua Idea vs. Progetti Finanziati:**\n    *   In cosa la tua idea è simile ai progetti vincenti?\n    *   In cosa si differenzia (punti di forza unici o potenziali debolezze)?\n    *   La tua idea è *sufficientemente* innovativa o impattante rispetto al \"benchmark\" dei vincitori?\n2.  **[ ] Tabella di Corrispondenza Finale (Punteggio Atteso):**\n    *   Crea una tabella dettagliata:\n        *   Colonna 1: Criterio di Valutazione del Bando\n        *   Colonna 2: Punteggio Massimo Possibile\n        *   Colonna 3: Come la TUA IDEA risponde a quel criterio (motivazione dettagliata)\n        *   Colonna 4: Punti di Forza specifici della tua idea per quel criterio\n        *   Colonna 5: Potenziali Debolezze/Rischi della tua idea per quel criterio\n        *   Colonna 6: Stima Punteggio Ottenibile (sii brutalmente onesto)\n3.  **[ ] Identificazione dei Gap Critici:**\n    *   Quali sono i criteri dove la tua idea rischia di prendere un punteggio basso?\n    *   Ci sono aspetti del bando (obiettivi, priorità) che la tua idea non copre adeguatamente?\n4.  **[ ] Brainstorming per il Miglioramento/Adattamento:**\n    *   È possibile modificare/adattare l'idea per colmare i gap identificati senza snaturarla? (es. rimodulare obiettivi, aggiungere partner strategici, enfatizzare certi aspetti innovativi, dettagliare meglio l'impatto).\n    *   Se l'allineamento è scarso, valuta se questo è il bando giusto. *Non forzare un'idea in un bando non adatto.*\n5.  **[ ] Considera il \"Valore Differenziale\":** Anche se simile ad altri, cosa rende la tua proposta *unica* e particolarmente meritevole di finanziamento *ora*, per *questo* bando?\n\n**Fase 5: Decisione e Prossimi Passi**\n\n1.  **[ ] Simulazione della Valutazione (Piedi per Terra):** Mettiti nei panni di un valutatore esterno. Con le informazioni raccolte e l'analisi fatta, daresti un punteggio sufficiente alla tua idea per essere finanziata, considerando la competizione?\n2.  **[ ] Richiedi Feedback Esterno (Opzionale ma Consigliato):** Se possibile, presenta la tua idea (e la tua analisi di competitività) a colleghi esperti o consulenti che non siano coinvolti emotivamente, per un parere oggettivo.\n3.  **[ ] Decisione Go/No-Go:**\n    *   **GO:** L'idea è competitiva, i gap sono colmabili, il potenziale punteggio è alto. Procedi con la stesura della proposta.\n    *   **REVISION NEEDED:** L'idea ha potenziale ma necessita di significative modifiche per allinearsi meglio. Torna alla Fase 4.\n    *   **NO-GO (per questo bando):** L'idea non è sufficientemente allineata, i gap sono troppo grandi, o la competizione percepita è troppo alta per le risorse attuali. Considera di cercare un bando più adatto o di rafforzare l'idea per future opportunità.\n\nQuesta tattica richiede tempo e onestà intellettuale, ma aumenterà significativamente le tue possibilità di successo, o ti eviterà di sprecare energie su un bando non adatto. In bocca al lupo!",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 42283,
    "datetime_utc": "2025-05-11T08:17:20.325930+00:00",
    "input_tokens": 76,
    "output_tokens": 2208,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 76}], \"thoughtsTokenCount\": 1525}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  },
  {
    "id": "01jtz6yw6bqwaadb2pbt7r1vg4",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "voglio che esegui questa tattica per me. Passiamo alla fase 1. Per prima cosa ti darò i documenti del bando. Analizzali e memorizzali. Se ok, rispondi: ok, fatto, hai altri documenti da passarmi?",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Ok, fatto. Ho analizzato e memorizzato il contenuto del documento Q&A che mi hai fornito.\n\nSi tratta di un Q&A relativo al \"2nd OSCARS Open Call for Open Science projects and services\", con apertura prevista per il 14 Marzo 2025 e scadenza il 14 Maggio 2025.\n\nHo estratto informazioni preliminari su:\n*   **Budget:** Limite di 250.000 EUR per progetto, budget totale della call 3 milioni EUR.\n*   **Consorzio:** Flessibilità nella composizione, possibilità di partner singoli, non-EU, ruolo degli advisor, etc.\n*   **Costi Ammissibili:** Personale, hardware (deprezzamento), costi indiretti (da includere nel totale, seguendo pratiche istituzionali e non necessariamente il 25% flat rate EC).\n*   **Criteri di Valutazione:** Indicazione che sono gli stessi della prima call (viene fornito un link `https://oscars-project.eu/open-calls#evaluationcriteria`) e che si baseranno su 7 categorie, tenendo conto anche della diversità.\n*   **Ammissibilità:** Dottorandi come PI (se legati a entità legale), impatto scientifico (anche per ricercatori del global south se affrontano una sfida Open Science).\n*   **Scadenze:** Apertura 14 Marzo 2025, chiusura 14 Maggio 2025.\n*   **Submission:** Possibilità di allegare documenti addizionali; il template del modulo di domanda sarà disponibile.\n\nHai altri documenti da passarmi? Ad esempio, il testo completo del bando (\"call text\") della prima o della seconda call (se già disponibile in bozza), eventuali allegati specifici, la guida per i proponenti, o il documento a cui rimanda il link per i criteri di valutazione (`https://oscars-project.eu/open-calls#evaluationcriteria`), se lo hai già scaricato? Più informazioni dettagliate ho, meglio potrò eseguire la Fase 1.",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 24481,
    "datetime_utc": "2025-05-11T08:21:12.104482+00:00",
    "input_tokens": 3892,
    "output_tokens": 461,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 2344}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 1548}], \"thoughtsTokenCount\": 1608}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": [
      {
        "id": "1875d3b6c91c5f0361e380da511b979c946373d9b29e8f4ab54967a162724dca",
        "type": "application/pdf",
        "path": "/home/nelson/Documents/Dataground srls/Bandi EU e non/20250514 - Bando Oscar/FAQs-2ndOpenCall.pdf",
        "url": null,
        "content_length": null
      }
    ]
  },
  {
    "id": "01jtz72cewjqh032w5pb80m14s",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "ecco un altro documento",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Ok, fatto. Ho analizzato e memorizzato il contenuto del secondo documento (la pagina web \"About OSCARS Open Calls\" dal sito `oscars-project.eu`).\n\nQuesto documento è molto ricco e ha fornito dettagli cruciali, in particolare su:\n*   **Obiettivi generali e specifici del bando:** Chiarisce ulteriormente lo scopo di OSCARS, il focus su Open Science, FAIR data, l'integrazione con EOSC e i Science Clusters.\n*   **Criteri di valutazione:** Fornisce un elenco dettagliato di sotto-criteri per ogni sezione della proposta (Project description, Scientific impacts, Digital resources, Project Implementation and Final Deliverables).\n*   **Requisiti di ammissibilità (Formale):** Dettagli su chi può applicare (inclusi enti non-EU e la non necessità di un PIC), la durata del progetto, e le regole per le organizzazioni partner di OSCARS.\n*   **Condizioni Finanziarie:** Conferma il range di finanziamento, il tipo (lump sum), e la gestione dei costi indiretti e del cofinanziamento.\n*   **Target e Beneficiari:** Specifica le comunità target e l'impatto atteso.\n*   **Scadenze Chiave:** Conferma le date di apertura, chiusura e inizio progetti.\n*   **Struttura della Proposta:** Offre una guida chiara sulle sezioni richieste e i limiti di caratteri.\n*   **Processo di Valutazione e Selezione:** Dettagli su IEC, PMB, SCB e la priorità per nuove organizzazioni/temi nella seconda call.\n*   **Aspetti Pratici:** Informazioni su endorsement letters (non mandatorie), gestione dei dati (deposito in repository EOSC), e la natura del finanziamento (lump sum senza report finanziari dettagliati, ma con focus sui risultati e deliverable).\n\nQuesto documento è una fonte primaria molto importante per la Fase 1.\n\nHai altri documenti da passarmi? Penso in particolare a:\n1.  Il **testo completo della \"Prima Open Call\" di OSCARS**, dato che i criteri di valutazione e molte condizioni sono dichiarati essere gli stessi. Avere quel documento originale potrebbe fornire ulteriori sfumature o dettagli.\n2.  Se hai già scaricato il contenuto della pagina `https://oscars-project.eu/open-calls#evaluationcriteria` (a cui il primo Q&A faceva riferimento), anche se molte informazioni sui criteri sono già presenti nel documento appena analizzato, potrebbe esserci qualche dettaglio aggiuntivo.\n3.  Eventuali **guide per i proponenti** (se esistevano per la prima call o se ci sono bozze per la seconda).\n\nSe non hai altri documenti, possiamo considerare la raccolta di informazioni dal bando come sufficientemente completa per iniziare a strutturare i punti della Fase 1.",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 51589,
    "datetime_utc": "2025-05-11T08:22:39.958325+00:00",
    "input_tokens": 14164,
    "output_tokens": 627,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 2812}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 11352}], \"thoughtsTokenCount\": 3528}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": [
      {
        "id": "f6c5041951440063a017bead943c30e85b839208e681f5043b125a0647aa890f",
        "type": "application/pdf",
        "path": "/home/nelson/Documents/Dataground srls/Bandi EU e non/20250514 - Bando Oscar/Homepage-Oscar.pdf",
        "url": null,
        "content_length": null
      }
    ]
  },
  {
    "id": "01jtz7ct8171mapejhw2y8spw9",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "ecco un altro documento",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Ok, fatto. Ho analizzato e memorizzato il contenuto di questo terzo documento (\"OSCARS 2nd Open Call for Open Science Projects and Services\").\n\nQuesto documento è **estremamente importante** in quanto sembra essere il **testo ufficiale della 2nd Open Call**, e fornisce il massimo livello di dettaglio finora.\n\nIntegrazioni e conferme chiave rispetto ai documenti precedenti:\n*   **Budget preciso:** 2,980,665.00 € disponibili.\n*   **Open Call Topics e priorità per la 2nd Call:** Viene esplicitato un forte interesse per \"applicazioni che facciano uso di **esistenti FAIR data objects o servizi** da uno o più domini per rispondere a specifiche questioni scientifiche, con l'aspettativa di produrre nuovi risultati scientifici\". Inoltre, viene data **priorità ad \"applicazioni di alta qualità da organizzazioni che non hanno ricevuto finanziamenti nella 1st OSCARS Open Call, per temi distinti da quelli precedentemente supportati\"**. Questo è un punto strategico cruciale.\n*   **Struttura della Proposta (implicita nella griglia e nei temi):** La griglia che incrocia i \"Domain\" (Astrophysics, Social Science, etc.) con le \"Open Science / Data FAIRNESS challenge(s)\" (Open Science Project, Open Science Service, Industry cooperation, etc.) è fondamentale per posizionare il progetto.\n*   **Criteri di Valutazione con PESI e PUNTEGGI MASSIMI:** Questo è l'elemento più significativo di questo documento. Fornisce una scomposizione dettagliata:\n    *   **Proposal Section 1: Project Description (Max 40 punti)** - con pesi doppi per alcune sotto-domande.\n    *   **Proposal Section 2: Scientific impacts (Max 30 punti)** - con pesi doppi per tutte le sotto-domande.\n    *   **Proposal Section 3: Digital resources (Max 10 punti)** - con pesi singoli.\n    *   **Proposal Section 4: Project Implementation, Budget Breakdown and Final Deliverables (Max 20 punti)** - con pesi singoli.\n    La conoscenza di questi pesi e punteggi è **vitale** per la \"Tattica Rigorosa\".\n*   **Processo di Valutazione:** Ulteriori dettagli sul ruolo dell'IEC (due esperti esterni indipendenti), la gestione dei reclami (solo su aspetti procedurali), e il processo di approvazione (PMB, SCB).\n*   **Obblighi Contrattuali e Tempistiche:** Dettagli sul TPPA, l'estensione di obblighi dall'HE Model Grant Agreement, e le tempistiche per la firma e l'avvio del progetto.\n*   **Etica:** Impegno verso il \"European Code of Conduct for Research Integrity\" e processo di pre-screening etico.\n\nQuesto documento, insieme al precedente \"About OSCARS Open Calls\", fornisce una base informativa molto solida e dettagliata.\n\nHai altri documenti da passarmi?\nAnche se ora abbiamo informazioni molto dettagliate, il testo originale della \"Prima Open Call\" potrebbe ancora contenere qualche ulteriore contesto o sfumatura, ma i documenti attuali, specialmente quest'ultimo, sono chiaramente i più rilevanti per la seconda call. La guida all'uso della piattaforma \"Grants Platform\" (se esistesse) potrebbe essere utile in fase di sottomissione, ma non è strettamente necessaria per questa analisi preliminare di competitività.\n\nSe non ci sono altri documenti, direi che siamo pronti per procedere con la prima parte della Fase 1 della \"tattica rigorosa\", ovvero la **scomposizione dettagliata degli obiettivi del bando, l'analisi dei criteri di valutazione (ora con pesi e punteggi), la verifica dei requisiti di ammissibilità, ecc., basandoci principalmente su questi ultimi due documenti.**",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 40199,
    "datetime_utc": "2025-05-11T08:28:33.145376+00:00",
    "input_tokens": 18148,
    "output_tokens": 850,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 3442}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 14706}], \"thoughtsTokenCount\": 2059}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": [
      {
        "id": "321dbf2bb30a10931f2c63ae5b0442d808f7009f94d2a2aca3c65839521c3398",
        "type": "application/pdf",
        "path": "/home/nelson/Documents/Dataground srls/Bandi EU e non/20250514 - Bando Oscar/Call-OSCARS.pdf",
        "url": null,
        "content_length": null
      }
    ]
  },
  {
    "id": "01jtz7jvvad07phw11489fb1t6",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "procedi",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Perfetto. Procediamo con la **Fase 1: Immersione Totale nel Bando (Comprendere le Regole del Gioco)**, utilizzando i documenti che mi hai fornito, in particolare gli ultimi due che sono i più completi.\n\nEcco l'analisi punto per punto:\n\n**1.1 Lettura Analitica Completa:**\n*Completata per i 3 documenti forniti.*\n\n**1.2 Scomposizione degli Obiettivi del Bando:**\n\nL'obiettivo generale del bando OSCARS è:\n*   Supportare le comunità di ricerca nell'adozione della **Open Science** e promuovere il coinvolgimento degli scienziati in **EOSC (European Open Science Cloud)**.\n*   Selezionare terze parti per lo sviluppo di **nuovi e innovativi progetti o servizi di Open Science** che guidino l'adozione della ricerca **FAIR-data-intensive** nell'Area Europea della Ricerca (ERA).\n*   Consolidare i risultati passati dei **Science Clusters** in servizi FAIR interdisciplinari duraturi e pratiche di lavoro.\n*   Dimostrare il ruolo che i Science Clusters, come nodi tematici per \"Open Research\" in Europa, potrebbero svolgere a beneficio dell'impatto scientifico di EOSC.\n\nObiettivi specifici che i progetti finanziati devono perseguire:\n*   Sviluppare o impegnarsi in **Open Data Research**.\n*   Condurre progetti scientifici che facilitino e promuovano la **condivisione di dati e risultati di ricerca basati sui principi FAIR**.\n*   Sviluppare e/o rendere disponibili **servizi o strumenti** (software, hardware, middleware, protocolli, standard, benchmark, documentazione, materiali di training/mentoring, attività e risorse) che abilitino l'Open Research in un campo specifico e incoraggino la pratica della scienza aperta basata sui principi FAIR.\n*   Sfruttare e/o **sperimentare i servizi dei Science Clusters** mirati a dimostrare e pilotare l'uso delle risorse EOSC.\n*   Essere **cross-dominio** e/o avere un'**alta rilevanza sociale** (potenzialmente multidisciplinari e/o cross-cluster).\n*   **Estendersi oltre l'attuale gruppo di RI (Research Infrastructures)** interne ai Science Clusters, includendo nuove comunità di RI (es. nuove RI nella Roadmap ESFRI o altre infrastrutture scientifiche di livello mondiale).\n*   Sfruttare il contesto di lavoro di **cross-fertilizzazione dei Science Clusters** per migliorare il coinvolgimento dei ricercatori nell'Open Science, raggiungendo molteplici comunità scientifiche (accademiche, industriali, University Associations/Learned Societies, citizen scientists, \"long tail of science\").\n*   Prevedere l'**uso di dati e servizi già onboardati sulla piattaforma EOSC**, sulle risorse dei Science Cluster e/o **portare nuovi oggetti di ricerca digitali e servizi RI sulla piattaforma EOSC Exchange**.\n*   Supportare i **principi FAIR** e adottare metriche FAIR rilevanti e policy EOSC.\n*   **Depositare i dati di ricerca digitali** generati nei cataloghi fidati dei Science Clusters o in altri repository fidati federati in EOSC.\n*   Aumentare il coinvolgimento dei ricercatori nell'Open Science.\n*   **Priorità per la 2nd Call:** Un'enfasi particolare è posta su applicazioni che facciano uso di **esistenti FAIR data objects o servizi** da uno o più domini per rispondere a specifiche questioni scientifiche, con l'aspettativa di produrre nuovi risultati scientifici entro la fine del progetto.\n\n**Parole chiave e concetti ricorrenti:** Open Science, FAIR data, EOSC, Science Clusters, Open Data Research, innovazione, impatto scientifico, cross-dominio, multidisciplinarietà, servizi, strumenti, condivisione dati, European Research Area (ERA).\n\n**1.3 Analisi Dettagliata dei Criteri di Valutazione (come da documento \"OSCARS 2nd Open Call for Open Science Projects and Services\"):**\n\nI progetti saranno valutati con un punteggio da 1 (lowest) a 5 per ogni sotto-criterio.\n\n*   **Proposal Section 1: Project Description (Massimo 40 punti)**\n    *   Subcriteria:\n        1.  Gli obiettivi e le finalità di Open Science del progetto/servizio scientifico proposto sono adeguatamente e chiaramente spiegati? *(Peso doppio)*\n        2.  Gli obiettivi scientifici e gli impatti attesi della proposta sono adeguatamente e chiaramente definiti? *(Peso singolo)*\n        3.  L'impegno del progetto proposto con una comunità o il contributo alla Open Data Research sono adeguatamente e chiaramente spiegati? *(Peso doppio)*\n        4.  Il contributo o lo sfruttamento del programma di lavoro dei Science Cluster è chiaramente spiegato? *(Peso doppio)*\n        5.  La rilevanza per ESFRI e altre RI del progetto/servizio scientifico proposto è adeguatamente affrontata? *(Peso singolo)*\n\n*   **Proposal Section 2: Scientific impacts (Massimo 30 punti)**\n    *   Subcriteria:\n        1.  Il progetto scientifico e/o lo sviluppo del servizio proposto vanno chiaramente oltre lo stato dell'arte? *(Peso doppio)*\n        2.  Il potenziale grande impatto scientifico è adeguatamente spiegato? *(Peso doppio)*\n        3.  L'impatto atteso del progetto proposto su una ESFRI RI o su molteplici RI, o la sua capacità di impegnarsi attraverso i Science Clusters con molteplici RI, è chiaramente illustrato? *(Peso doppio)*\n\n*   **Proposal Section 3: Digital resources (Massimo 10 punti)**\n    *   Subcriteria:\n        1.  Il progetto proposto fa adeguatamente riferimento alle piattaforme dei Science Clusters? *(Peso singolo)*\n        2.  Le risorse digitali richieste e/o la connessione ai servizi EOSC previsti sono chiaramente identificati? *(Peso singolo)*\n\n*   **Proposal Section 4: Project Implementation, Budget Breakdown and Final Deliverables (Massimo 20 punti)**\n    *   Subcriteria:\n        1.  Le fasi di implementazione del progetto sono concrete e ben spiegate? *(Peso singolo)*\n        2.  I risultati attesi sono realizzabili con le fasi di implementazione messe in atto e nel timeframe suggerito? *(Peso singolo)*\n        3.  La scomposizione del budget corrisponde alle fasi di implementazione presentate ed è ragionevole? *(Peso singolo)*\n        4.  La proposta si impegna adeguatamente alla conformità con EOSC e all'apertura dei suoi risultati? *(Peso singolo)*\n\n**Nota sulla Valutazione:**\n*   Le proposte saranno prima verificate per l'ammissibilità.\n*   La valutazione sarà condotta da due esperti esterni (IEC) indipendenti dal Consorzio OSCARS.\n*   Le decisioni dell'IEC sono vincolanti. Eventuali ricorsi saranno trattati solo in relazione agli aspetti procedurali della valutazione, non sul merito del giudizio scientifico o tecnico.\n*   **Priorità nella 2nd Call:** Saranno prioritizzate proposte di alta qualità da organizzazioni che non hanno ricevuto finanziamenti nella 1st OSCARS Open Call, per temi distinti da quelli precedentemente supportati. Le proposte già sottomesse nella prima call e ripresentate senza revisioni non saranno rivalutate ma useranno il punteggio precedente.\n\n**1.4 Verifica dei Requisiti di Ammissibilità (Formale):**\n\n*   **Proponente (Chi può ricevere supporto finanziario):**\n    *   Research Infrastructures (RIs), Università, Istituti, PMI e grandi aziende, consorzi, ricercatori individuali.\n    *   I terzi che ricevono supporto finanziario **non sono soggetti** alle regole di ammissibilità per il finanziamento previste dall'Articolo 23 della Regolamentazione 2021/695 (Horizon Europe) per quanto riguarda l'origine geografica (es. anche UK e Svizzera sono ammissibili).\n    *   Sono escluse entità soggette a misure restrittive UE, o entità legali stabilite in Russia, Bielorussia, o territori ucraini non controllati dal governo.\n    *   Un ricercatore individuale deve comunque essere affiliato a un'entità legale (università, istituto di ricerca) per la firma del contratto (TPPA) e per la gestione dei fondi.\n    *   Uno Science Cluster di per sé non può candidarsi.\n    *   L'organizzazione non necessita di un PIC number o di essere registrata presso la EC.\n    *   **OSCARS partners:** Le organizzazioni coinvolte nel consorzio OSCARS possono partecipare (e preparare) proposte, ma le persone di queste organizzazioni che sono *attivamente coinvolte* in OSCARS non possono. Staff members direttamente coinvolti in OSCARS non dovrebbero partecipare.\n    *   Un'organizzazione può coordinare una sola proposta che richieda tra 100.000 e 250.000 EUR (per supportare un'ampia gamma di comunità e scienze). È possibile per una stessa entità legale (es. grandi organizzazioni multidisciplinari come CNRS) partecipare a più proposte se provenienti da dipartimenti/discipline diverse.\n*   **Progetto:**\n    *   **Durata:** da 12 a 24 mesi.\n    *   **Budget per progetto:** Minimo €100.000, massimo €250.000 (lump sum).\n    *   **Tematiche:** Deve rispondere a sfide di Open Science / Data FAIRNESS nei domini specificati (Astrophysics, cosmology, particle or nuclear physics; Social sciences and humanities; Photon/neutron sources-based experimental research; Life sciences; Earth and environmental sciences; Other).\n    *   **Lingua della proposta:** Inglese.\n    *   **Lunghezza della proposta:** Max 10 pagine (da sottomettere online tramite webform).\n    *   **Obbligo di deposito dati:** Tutti i dati di ricerca digitali generati dal progetto dovranno essere depositati in un repository fidato federato in EOSC, in conformità con i requisiti EOSC.\n    *   **Etica:** Adesione allo European Code of Conduct for Research Integrity. Auto-valutazione etica richiesta per dataset e metodi.\n    *   **Inizio Progetto:** Entro 4 mesi dalla firma del TPPA (Third-Party Project Agreement), non prima del 1 Ottobre 2025 e non oltre il 1 Febbraio 2026.\n\n**1.5 Studio delle Condizioni Finanziarie:**\n\n*   **Finanziamento disponibile totale per la 2nd call:** 2,980,665.00 €.\n*   **Finanziamento per progetto:** Da €100.000 a €250.000.\n*   **Forma del finanziamento:** Lump sum (somma forfettaria). Questo implica che non ci sarà una rendicontazione finanziaria dettagliata delle spese, ma il focus sarà sul raggiungimento dei risultati e dei deliverable dichiarati.\n*   **Cofinanziamento:** Il finanziamento copre il 100% dei costi. Tuttavia, il cofinanziamento da parte del proponente o di terzi è possibile e, se ben descritto, può aggiungere valore alla proposta evidenziando l'impatto atteso.\n*   **Costi Ammissibili:**\n    *   Devono rispettare i criteri di ammissibilità per il finanziamento della Commissione Europea (Capitolo 3, Articolo 6 del EU Model Grant Agreement).\n    *   **Personale:** Costi del personale, Person Months (PMs).\n    *   **Hardware/Attrezzature:** Non è possibile richiedere il costo pieno dell'attrezzatura, ma solo i costi di ammortamento per la durata del progetto e per la percentuale di utilizzo specifica per il progetto.\n    *   **Viaggi:** Ammissibili (es. per partecipare agli eventi OSCARS, conferenze).\n    *   **Costi Indiretti (Overhead):** Devono essere inclusi nel budget totale richiesto (entro il limite dei €250.000). Non è necessariamente il 25% flat rate della CE; può essere una percentuale diversa basata sulla pratica usuale dell'organizzazione.\n    *   **Subcontracting:** Ammesso, senza un limite di budget suggerito. I costi vanno inclusi negli \"altri costi diretti\" e la situazione spiegata nella parte implementativa.\n    *   **APC (Article Processing Charges) per Open Access:** Ammissibili.\n    *   **Costi per eventi:** Ammissibili.\n*   **Pagamenti:** Una parte del finanziamento (15% secondo le FAQ generali, anche se non esplicitato nel documento della 2nd call) potrebbe essere pagata dopo la consegna dei deliverable finali. Il Q&A del 15 Gen 2025 specificava che l'85% veniva pagato all'inizio, il restante 15% dopo i deliverable finali.\n\n**1.6 Identificazione del Target e dei Beneficiari Attesi dal Bando:**\n\n*   **Target User Communities (come potenziali proponenti):** Research Infrastructures, Università, Istituti, PMI e grandi aziende, consorzi, ricercatori individuali.\n*   **Beneficiari Finali dei risultati dei progetti:**\n    *   Le comunità scientifiche in generale (inclusa la \"long tail of science\").\n    *   Ricercatori che adottano pratiche di Open Science e FAIR data.\n    *   L'ecosistema EOSC, attraverso l'integrazione di nuovi dati, servizi e utenti.\n    *   I Science Clusters, attraverso il consolidamento e l'espansione dei loro servizi.\n    *   La società europea e l'Area Europea della Ricerca (ERA), attraverso una scienza più aperta, efficiente e collaborativa.\n    *   Industria, University Associations/Learned Societies, citizen scientists.\n\n*   **Impatto Atteso:**\n    *   Aumento dell'adozione di pratiche Open Science e FAIR.\n    *   Sviluppo di soluzioni innovative (progetti/servizi) in ambito Open Science.\n    *   Maggiore utilizzo e arricchimento di EOSC e dei servizi dei Science Cluster.\n    *   Nuovi risultati scientifici derivanti dall'uso di FAIR data.\n    *   Maggiore collaborazione e cross-fertilizzazione tra domini scientifici e tra diversi tipi di attori.\n    *   Miglioramento della riproducibilità, accessibilità e sostenibilità della ricerca.\n\n**1.7 Definizione delle Scadenze Chiave:**\n\n*   **Apertura della Call (2nd OSCARS Open Call):** 14 Marzo 2025, 10:00 CET.\n*   **Chiusura della Call (Deadline per la sottomissione):** 14 Maggio 2025, 17:00 CEST (la call resta aperta per 60 giorni).\n*   **Notifica dei risultati ai proponenti:** Metà Luglio 2025.\n*   **Conclusione del TPPA (Third-Party Project Agreement):** Entro 1 mese dalla notifica.\n*   **Inizio dei progetti:** Entro 4 mesi dalla firma del TPPA. Non prima del 1 Ottobre 2025 e non oltre il 1 Febbraio 2026.\n\n---\n\nQuesta analisi dettagliata della Fase 1 dovrebbe fornirci una comprensione profonda del bando.\nIl prossimo passo sarà la **Fase 2: Auto-Analisi Critica della Tua Idea Progettuale**, dove confronteremo la tua idea con questi requisiti e obiettivi.\n\nSei pronto a descrivermi la tua idea progettuale affinché possiamo iniziare la Fase 2? Oppure hai domande su questa prima fase di analisi del bando?",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 51863,
    "datetime_utc": "2025-05-11T08:31:39.729455+00:00",
    "input_tokens": 19001,
    "output_tokens": 3553,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 4295}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 14706}], \"thoughtsTokenCount\": 522}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  },
  {
    "id": "01jtz7ws6rdt7r3h01a1fakedp",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "La mia idea progettuale è contenuta nell'allegato",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Perfetto. Ho analizzato il documento che descrive la tua idea progettuale, \"Open Data Explorer\". Si tratta di una proposta sottomessa (o da sottomettere) al \"6th Open Call of the NGI Zero Commons Fund\".\n\nOra possiamo procedere con la **Fase 2: Auto-Analisi Critica della Tua Idea Progettuale** mettendola a confronto con i requisiti del bando **OSCARS** che abbiamo analizzato in dettaglio.\n\n**2.1 Descrizione Chiara e Concisa dell'Idea (basata sul tuo documento):**\n\n*   **Nome Progetto:** Open Data Explorer\n*   **Obiettivo Principale:** Migliorare l'accessibilità di dati tabellari complessi (open data) sviluppando un chatbot innovativo capace di rispondere a query in linguaggio naturale.\n*   **Tecnologia Chiave:** Utilizzo di Small Language Models (SLM) per la generazione di codice (probabilmente Python con Pandas, come specificato più avanti), operanti indipendentemente da servizi cloud di terze parti. Questo per ridurre costi operativi, garantire privacy e allinearsi con valori open-source.\n*   **Utenti Target:** Policymakers, ricercatori, giornalisti, pubblico generale.\n*   **Risultati Attesi:**\n    1.  Un prototipo di chatbot per question-answering su open datasets.\n    2.  Metodologie open-source per interpretare dati strutturati tramite interfacce conversazionali.\n    3.  Un framework scalabile per l'integrazione con vari repository di dati pubblici.\n*   **Principi Guida:** Tutti i risultati (codice, modelli, documentazione) saranno apertamente accessibili.\n*   **Esperienza Pregressa Rilevante:**\n    *   Progetto \"Open Data Finder\" (candidato a NGI Zero Commons Fund 2024, non finanziato ma ha posto le basi).\n    *   Partecipazione a SemEval 2025 Task 8 (Q&A per dati tabellari), con ottimi risultati (2° posto) usando SLM e sviluppando un metodo di fine-tuning (ORPO).\n    *   Esperienza quindicennale del team in data journalism, NLP, AI, con progetti finanziati da EU, Google, Open Society Foundations.\n    *   Dataground SRLS (la tua organizzazione) è stata creata per guidare l'innovazione AI open-source.\n*   **Budget Richiesto (per NGI Zero):** €50.000 (Personal Costs €37k, Research Infrastructure €8k, Dissemination €5k).\n*   **Innovazione Dichiarata rispetto all'Esistente:**\n    1.  **Focus sull'accessibilità degli Open Data:** A differenza di servizi per database privati, è pensato per interagire con repository pubblici (CKAN, Eurostat).\n    2.  **Uso di SLM:** A differenza di LLM su cloud, gli SLM possono operare localmente/on-premise, riducendo costi e migliorando privacy.\n    3.  **Impegno Open-Source e Trasparenza:** Tutto sarà aperto.\n    4.  **Integrazione con Repository Pubblici:** Framework scalabile per accesso e analisi di un'ampia gamma di open datasets.\n*   **Sfide Tecniche e Soluzioni Proposte:**\n    *   Accuratezza/Robustezza: Uso di SLM multipli (majority voting), espansione dataset di fine-tuning.\n    *   Comprensione Linguaggio Naturale per Dati Tabellari: Uso di tecniche di preference-optimization (ORPO, GRPO), traduzione in codice Python (Pandas).\n    *   Scalabilità/Performance: Architettura \"agentic graph\".\n    *   Integrazione con Repository Diversi: Connettori API standardizzati, mapping semantico.\n    *   Privacy/Sicurezza: SLM leggeri per elaborazione on-device, misure di privacy-preserving (es. differential privacy).\n*   **Timeline (12 mesi):** Dettagliata per fasi bimestrali.\n*   **Ecosistema e Stakeholder:** Open Data Providers (Eurostat, CKAN), Ricerca & Accademia, Comunità Open-Source & AI, Civic Tech & Giornalisti, Settore Pubblico & Policymakers.\n*   **Sostenibilità (oltre NGI):** Contributi open-source, partnership con Eurostat/CKAN, ricerca di fondi EU (Horizon Europe, European Data Space), community GitHub, possibile modello di licenza API.\n\nOra passiamo al confronto con OSCARS.\n\n**2.2 Mappatura Idea vs. Obiettivi del Bando OSCARS:**\n\nCreiamo la tabella di confronto:\n\n| Obiettivi Specifici del Bando OSCARS                                                                                                                               | Come \"Open Data Explorer\" (ODE) risponde                                                                                                                                                                                                                                                                                          | Allineamento (Alto, Medio, Basso) | Note / Potenziali Aree di Miglioramento per OSCARS                                                                                                                                                              |\n| :----------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| Sviluppare o impegnarsi in **Open Data Research**.                                                                                                                 | ODE è intrinsecamente focalizzato sulla ricerca e l'utilizzo di Open Data. L'obiettivo è facilitarne l'accesso e l'analisi.                                                                                                                                                                                                       | Alto                              |                                                                                                                                                                                                                 |\n| Condurre progetti scientifici che facilitino e promuovano la **condivisione di dati e risultati di ricerca basati sui principi FAIR**.                              | ODE non \"condivide\" nuovi dati, ma facilita l'accesso a dati esistenti. I risultati del progetto (software, modelli) saranno FAIR (open source, documentati). La natura di ODE promuove l'uso di dati FAIR.                                                                                                                            | Medio-Alto                        | Potrebbe essere necessario enfatizzare come ODE stesso, come servizio, aderirà ai principi FAIR e come promuoverà l'uso di dati che sono già FAIR. L'output di ODE sono risposte, non dataset \"grezzi\".                |\n| Sviluppare e/o rendere disponibili **servizi o strumenti** (...) che abilitino l'Open Research (...) e incoraggino la pratica della scienza aperta basata su FAIR. | ODE è chiaramente uno **strumento/servizio** (chatbot + framework) che abilita l'Open Research (accesso facilitato a open data) e si basa su principi FAIR (output del progetto open source, uso di open data).                                                                                                                     | Alto                              |                                                                                                                                                                                                                 |\n| Sfruttare e/o **sperimentare i servizi dei Science Clusters** mirati a dimostrare e pilotare l'uso delle risorse EOSC.                                             | Non menzionato esplicitamente nella proposta NGI. Questo è un **GAP CRUCIALE** per OSCARS. Bisognerebbe identificare quali servizi dei Science Clusters (ENVRI-FAIR, ESCAPE, EOSC-Life, PaNOSC, SSHOC) potrebbero essere rilevanti e come ODE potrebbe integrarli o sfruttarli.                                                      | Basso                             | **Azione necessaria:** Investigare i servizi offerti dai 5 Science Clusters e vedere se ODE può integrarsi (es. accedere a dati specifici di un cluster, usare un servizio di autenticazione/calcolo di un cluster). |\n| Essere **cross-dominio** e/o avere un'**alta rilevanza sociale** (potenzialmente multidisciplinari e/o cross-cluster).                                             | ODE è intrinsecamente cross-dominio perché mira a integrare vari repository di dati pubblici (CKAN, Eurostat). Ha alta rilevanza sociale (policy makers, giornalisti, pubblico).                                                                                                                                                    | Alto                              | L'aspetto cross-cluster dipenderebbe dal punto precedente (integrazione con Science Clusters).                                                                                                                  |\n| **Estendersi oltre l'attuale gruppo di RI** interne ai Science Clusters, includendo nuove comunità di RI (...).                                                     | Non esplicitamente menzionato. L'integrazione con Eurostat e portali CKAN governativi potrebbe già coprire questo, ma non è legato ai Science Clusters. Bisognerebbe vedere se i dati a cui ODE vuole accedere provengono anche da RI non (o recentemente) parte dei Cluster.                                                            | Medio                             | Potrebbe essere utile identificare esplicitamente RI specifiche (ESFRI o altre) i cui dati sarebbero resi più accessibili da ODE.                                                                             |\n| Sfruttare il contesto di lavoro di **cross-fertilizzazione dei Science Clusters** per migliorare il coinvolgimento dei ricercatori nell'Open Science (...).        | Collegato al GAP sull'uso dei servizi dei Science Clusters. Se ODE si integrasse con più Cluster, questo obiettivo sarebbe soddisfatto.                                                                                                                                                                                                   | Basso                             | Come sopra, l'integrazione con i Cluster è chiave.                                                                                                                                                             |\n| Prevedere l'**uso di dati e servizi già onboardati sulla piattaforma EOSC**, sulle risorse dei Science Cluster e/o **portare nuovi oggetti di ricerca digitali e servizi RI sulla piattaforma EOSC Exchange**. | Non esplicitamente menzionato. ODE potrebbe accedere a dati già in EOSC e, come servizio, ODE stesso potrebbe essere un candidato per l'EOSC Exchange.                                                                                                                                                                        | Medio                             | **Azione necessaria:** Verificare se i dati target di ODE sono in EOSC e dichiarare l'intenzione di rendere ODE un servizio su EOSC Exchange.                                                                  |\n| Supportare i **principi FAIR** e adottare metriche FAIR rilevanti e policy EOSC.                                                                                 | L'impegno open-source e la trasparenza di ODE sono allineati. L'adozione esplicita di metriche FAIR e policy EOSC per il servizio ODE andrebbe dettagliata.                                                                                                                                                                         | Medio                             | Specificare come ODE (come servizio) sarà FAIR e quali policy EOSC adotterà.                                                                                                                                    |\n| **Depositare i dati di ricerca digitali** generati nei cataloghi fidati dei Science Clusters o in altri repository fidati federati in EOSC.                       | ODE genera \"risposte\" e \"metodologie\", non primariamente \"dati di ricerca digitali\" nel senso classico. Il codice, i modelli, la documentazione saranno su GitHub/HuggingFace. Questi potrebbero essere referenziati in Zenodo (federato EOSC).                                                                                             | Medio                             | Chiarire come gli output del progetto (software, modelli) saranno resi disponibili e rintracciabili tramite EOSC.                                                                                              |\n| Aumentare il coinvolgimento dei ricercatori nell'Open Science.                                                                                                     | Facilitando l'accesso agli open data, ODE mira chiaramente a questo.                                                                                                                                                                                                                                                               | Alto                              |                                                                                                                                                                                                                 |\n| **Priorità 2nd Call:** Uso di **esistenti FAIR data objects o servizi** (...) per rispondere a specifiche questioni scientifiche, producendo **nuovi risultati scientifici**. | ODE usa esistenti open data (presumibilmente FAIR). L'enfasi sui \"nuovi risultati scientifici\" prodotti *dagli utenti* di ODE è implicita. ODE stesso è un servizio che produce risposte. La proposta potrebbe enfatizzare use case dove ODE aiuta a generare insight scientifici.                                           | Alto                              | Enfatizzare come ODE *abilita* la produzione di nuovi risultati scientifici.                                                                                                                                    |\n\n**2.3 Valutazione Preliminare sui Criteri del Bando OSCARS (Qualitativa):**\n\n*   **Sezione 1: Project Description:**\n    *   Obiettivi Open Science: Alto\n    *   Obiettivi Scientifici e Impatti: Medio-Alto (l'impatto scientifico diretto di ODE come *strumento* è chiaro; l'impatto degli *utenti* di ODE è indiretto)\n    *   Engagement Comunità / Open Data Research: Alto\n    *   Leverage Science Cluster: Basso (GAP principale)\n    *   Rilevanza ESFRI/RI: Medio (potrebbe essere più esplicito)\n    *   **Punteggio Potenziale Stimato (su 40):** Senza colmare il gap sui Science Cluster, potrebbe essere intorno a 20-25. Colmando il gap, potrebbe salire significativamente.\n\n*   **Sezione 2: Scientific impacts:**\n    *   Oltre Stato dell'Arte: Alto (uso di SLM, focus open data, architettura agentic graph)\n    *   Potenziale Grande Impatto Scientifico: Alto (democratizzazione accesso ai dati)\n    *   Impatto su ESFRI/RI / Engagement tramite Science Cluster: Medio-Basso (dipende dai gap sopra)\n    *   **Punteggio Potenziale Stimato (su 30):** Attualmente 15-20. Potrebbe aumentare con una migliore connessione a RI/Cluster.\n\n*   **Sezione 3: Digital resources:**\n    *   Riferimento Piattaforme Science Cluster: Basso (GAP)\n    *   Risorse Digitali / Connessione EOSC: Medio (ODE come servizio per EOSC è una buona idea, ma va esplicitata. Le risorse che ODE usa devono essere meglio collegate a EOSC/Cluster)\n    *   **Punteggio Potenziale Stimato (su 10):** Attualmente 3-5. Facilmente migliorabile.\n\n*   **Sezione 4: Project Implementation, Budget Breakdown and Final Deliverables:**\n    *   Piani Implementazione Concreti: Alto (timeline dettagliata)\n    *   Risultati Realizzabili: Alto\n    *   Budget Ragionevole: Il budget NGI (€50k) è **molto inferiore** al minimo OSCARS (€100k). La struttura dei costi andrebbe rivista e scalata. Il team proposto è piccolo.\n    *   Commitment EOSC Compliance / Openness: Alto per openness; Medio per EOSC compliance specifica (va dettagliata).\n    *   **Punteggio Potenziale Stimato (su 20):** Attualmente 10-12 (a causa del budget e della necessità di dettagliare compliance EOSC).\n\n**2.4 Identificazione dell'Innovazione e del Valore Aggiunto (per OSCARS):**\n\n*   **Innovazione:**\n    *   Uso di SLM customizzati e fine-tuned (ORPO) per Q&A su dati tabellari, operanti on-premise/localmente.\n    *   Focus specifico sull'accesso conversazionale a **Open Data pubblici e diversificati**.\n    *   Architettura \"agentic graph\" per scalabilità.\n    *   Approccio interamente open-source.\n*   **Valore Aggiunto (se adattato a OSCARS):**\n    *   Potrebbe rendere i dati dei Science Clusters e di EOSC più accessibili a un pubblico più ampio (ricercatori non tecnici, policy maker).\n    *   Potrebbe fornire un servizio innovativo per l'EOSC Exchange.\n    *   Potrebbe dimostrare un uso pratico di SLM in contesti Open Science, promuovendo la sovranità digitale.\n\n**2.5 Analisi di Fattibilità Preliminare (per OSCARS):**\n\n*   **Tecnica/Operativa:** Il team ha dimostrato forte competenza tecnica (SemEval). La sfida sarà l'integrazione con i servizi/dati specifici di OSCARS/EOSC/Science Clusters.\n*   **Gestionale:** Il team ha esperienza in progetti EU. Per OSCARS, il progetto dovrebbe probabilmente essere più grande (budget €100k-€250k), il che potrebbe richiedere un team più strutturato o più partner. Dataground SRLS è una PMI (srls), il che è ammissibile.\n*   **Finanziaria:** Il budget NGI di €50k è troppo basso. Per OSCARS, la proposta andrebbe **significativamente scalata** in termini di attività e risorse per giustificare un budget di €100k o più. Questo potrebbe implicare:\n    *   Più lavoro sull'integrazione con vari Science Cluster.\n    *   Sviluppo di più connettori per specifici repository EOSC.\n    *   Attività di dissemination e community building più ampie e mirate all'ecosistema OSCARS.\n    *   Coinvolgimento di partner se necessario per coprire competenze specifiche sui Science Cluster o per la dissemination.\n\n**2.6 Verifica della Coerenza con i Requisiti di Ammissibilità OSCARS:**\n\n*   **Proponente:** Dataground SRLS (PMI italiana) è ammissibile. Andrea Nelson Mauro come PI è ammissibile.\n*   **Progetto:**\n    *   **Durata:** La timeline di 12 mesi è ammissibile (range 12-24).\n    *   **Budget:** €50k è **NON AMMISSIBILE** (min €100k). Va scalato.\n    *   **Tematiche:** Si adatta bene a \"Open Science / Data FAIRNESS challenge\" e al dominio \"Other (specify)\" o potrebbe toccare \"Social sciences and humanities\" (Eurostat) o altri a seconda dei dati integrati.\n    *   **Lingua:** Inglese (OK).\n    *   **Lunghezza Proposta:** Il formato NGI è diverso, ma il contenuto può essere adattato al webform OSCARS (max 10 pagine equivalenti).\n    *   **Obbligo Deposito Dati/Risultati:** L'approccio open source è coerente. Va specificato come i deliverable saranno rintracciabili in EOSC.\n    *   **Priorità 2nd Call (nuova organizzazione/tema):** Se Dataground SRLS non ha ricevuto fondi nella 1st call OSCARS, questo è un **vantaggio**. Il tema dell'accesso conversazionale a open data tramite SLM potrebbe essere sufficientemente distinto.\n\n**Sintesi Preliminare dei Punti di Forza e Debolezza per OSCARS:**\n\n**Punti di Forza:**\n*   Idea innovativa e tecnicamente solida (SLM, Q&A su tabelle).\n*   Team con comprovata esperienza (SemEval, progetti EU).\n*   Forte allineamento con i principi Open Science e FAIR.\n*   Potenziale di grande impatto sulla democratizzazione dell'accesso ai dati.\n*   Se Dataground è \"nuova\" per OSCARS, ha un vantaggio di priorità.\n\n**Debolezze / GAP da Colmare per OSCARS:**\n1.  **Budget:** La proposta attuale (€50k) è troppo piccola. Deve essere scalata ad almeno €100k, giustificando attività aggiuntive.\n2.  **Integrazione con Science Clusters/EOSC:** Questo è il **GAP più critico**. La proposta deve dimostrare chiaramente come sfrutterà i servizi dei Science Cluster e/o le risorse EOSC, e come vi contribuirà. È un requisito esplicito e pesantemente valutato.\n3.  **Dettaglio su Conformità EOSC:** Bisogna specificare meglio come il servizio ODE e i suoi output saranno conformi alle policy e metriche EOSC.\n4.  **Rilevanza ESFRI/RI:** Può essere ulteriormente rafforzata identificando specifiche RI i cui dati sarebbero valorizzati.\n5.  **Partner (Eventuale):** Per scalare il progetto e coprire le integrazioni con i Science Cluster, potrebbe essere necessario considerare partner strategici.\n\nIl prossimo passo sarà la **Fase 3: Analisi Comparativa (Studio dei Progetti Già Finanziati da OSCARS)**.\nPer questo, avrei bisogno che tu mi fornissi informazioni sui progetti finanziati nella prima call di OSCARS. Il documento \"OSCARS 2nd Open Call for Open Science Projects and Services\" a pagina 4 menziona: *\"Examples from the 1st Open Call can be found on the OSCARS webpage under 'projects'.\"*\nIl link dovrebbe essere qualcosa come `https://oscars-project.eu/projects`.\n\nSe puoi accedere a quella pagina e fornirmi un elenco dei progetti finanziati (titolo, breve descrizione, eventuale coordinatore/partner, budget se noto, Science Cluster di riferimento se evidente), sarebbe utilissimo.\n\nIn alternativa, se non hai accesso immediato, possiamo fare delle ipotesi basate sui tipi di progetti che tipicamente vengono finanziati in call simili (focalizzate su EOSC e Open Science). Ma avere i dati reali è decisamente meglio.\n\nCosa ne pensi di questa prima auto-analisi? Ti sembra che i GAPs identificati siano corretti?",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 61231,
    "datetime_utc": "2025-05-11T08:36:55.335234+00:00",
    "input_tokens": 25669,
    "output_tokens": 4583,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 7867}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 17802}]}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": [
      {
        "id": "0a64e0fd70ccfd359d262b934f5157e9ac3681eacfc1d8f68a563a8fdd96a278",
        "type": "application/pdf",
        "path": "/home/nelson/Documents/Dataground srls/Bandi EU e non/20250514 - Bando Oscar/opendataexplorer-ngi.pdf",
        "url": null,
        "content_length": null
      }
    ]
  },
  {
    "id": "01jtz86m0hq74796essbqa67pr",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "[\n  {\n    \"Project_Title\": \"AI-SCOPE: AI-Driven Enhancement of Surface Scattering Data for Open Science Platforms Across Europe\",\n    \"Project_url\": \"https://oscars-project.eu/projects/ai-scope-ai-driven-enhancement-surface-scattering-data-open-science-platforms-across\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"The AI-SCOPE project engages the European photon and neutron (PaN) science community to enhance the frequency and quality of FAIR-compliant data submissions to EOSC-indexed databases, such as the ESRF data portal or Zenodo.\\nThe project stands to dramatically enhance data FAIRness and beamtime efficiency, by introducing a live AI analysis tool for surface scattering experiments. By rigorously applying it to a diverse array of pre-existing scattering data and enabling live analysis of data from a multitude of sample types we will create a well-curated dataset with detailed annotations in EOSC indexed databases, and lay the groundwork for more uploads with metadata in the future. Both the AI-tool and the curated database will unlock new possibilities for data reuse and collaborative scientific exploration of X-ray and neutron data with machine learning helping also AI-guided materials discovery.\",\n      \"Challenge\": \"Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nThe main challenge of the AI-SCOPE project is to increase the frequency and quality of FAIR-compliant data submissions to EOSC-indexed databases, such as ESRF data portal or Zenodo, by addressing the current lack of well-annotated surface scattering data. Researchers often neglect valuable data due to time constraints, selective attention, and lack of incentives for annotating and sharing datasets. The project seeks to address these issues by introducing an AI tool that enhances real-time data analysis during beamtime and automates the generation of rich metadata, encouraging the reuse and FAIR submission of data in open databases. It must overcome the difficulties of reusing raw data by integrating standardised metadata and facilitating searches for specific materials and structures.\",\n      \"Solution\": \"The project tri-national team will introduce a sophisticated AI analysis tool for surface scattering experiments that performs an automated initial analysis, and simultaneously generates rich metadata annotations. This dual capability incentivises researchers to utilise the tool, as it streamlines their workflow and gives “live” feedback during experimental sessions at large scale facilities (“beamtimes”).\\nThe AI-tool will be designed to use AI to estimate structural sample parameters (such as film thickness, material densities, contact planes and unit cell dimensions) and feed these into the database using standardised metadata formats. This feature ensures the database can be searched for specific materials and structures from the automated AI analysis, thereby avoiding the pitfalls of a hard to re-use raw data dump and increasing reusability within the scientific ecosystem of EOSC / PaNOSC.\",\n      \"Scientific Impact\": \"The integration of AI-driven analysis tools to EOSC services and the creation of a curated FAIR database represent pivotal advancements in the field of surface scattering. Building on the state-of-the-art, the project will develop individual solutions and make them available, both at large-scale facilities and online, through EOSC services.\\nBy automating the analysis processes and enriching datasets with preliminary insights, these innovations enable a more comprehensive exploration of experimental data. This approach not only accelerates the discovery of novel materials but also facilitates meta-studies and machine learning applications in materials discovery. Moreover, providing a curated database as a nucleus and incentivising community contributions through accessible software tools, promises to result in a significantly higher user uptake of EOSC services and open databases, such as Zenodo. Similar to classical machine learning datasets that get used as benchmarks in the respective communities (e.g., ImageNet competition), the project strives to provide such a benchmark dataset to the European community (and beyond) to start also the comparison of different machine learning algorithms on real-world datasets.\\nThe project thereby serves as a catalyst for filling Zenodo and EOSC nodes (e.g., ESRF data portal) with valuable, actionable data, thus propelling the surface scattering field toward unprecedented collaborative and scientific achievements.\",\n      \"Keywords\": \"surface scattering data, photon and neutron science, PaN science, PaN data\",\n      \"Principal investigator\": {\n        \"Name\": \"Stefan Kowarik\",\n        \"Organisation\": \"University of Graz\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"AMBCAT - Digital Amber Catalogue\",\n    \"Project_url\": \"https://oscars-project.eu/projects/ambcat-digital-amber-catalogue\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, LS RI - Life Sciences, PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"Amber fossils provide a unique window into ancient ecosystems, offering insights into extinct organisms through remarkable preservation of both external and internal structures: fossilised specimens are preserved 3-dimensionally, they allow us to study surface structures of the animals such as hairs, scales or flower structures which are in the rarest case preserved in other forms of fossils, and they occasionally conserve internal soft tissues which allows the complete or partial reconstruction of the inner anatomy of the studied fossil organism.AMBCATaims to centralise and make accessible high-resolution digital scans of these rare fossils, fostering collaboration among researchers and advancing scientific research on a global scale.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nThe study of amber fossils is not excluded from difficulties. Conventional imaging tools often fail to capture critical details, especially when inclusions are obscured by other materials, or when the amber is non-transparent, presenting a milky appearance. Additionally, the scattered and redundant imaging efforts across various institutions can lead to wasted resources and missed opportunities for collaboration, hindering comprehensive scientific analysis.\",\n      \"Solution\": \"AMBCAT aims at making the physical amber fossils data centrally stored and archived at DESY FAIR and open available to the general community and public. A data catalogue will be offered, aggregating and providing all relevant metadata for the digitalised objects together with the imaging protocols. In conjunction with the actual image data, AMBCAT is also expected to provide access to the actual image data, as well as online visualisation tools in a browser-based application. This platform will streamline research effort allowing researchers worldwide to access and use the data, preventing duplication, and enabling comprehensive analysis using advanced μCT scanning technologies, which have shown to be an effective and valuable tool to visualise and create 3D models of amber fossils.\",\n      \"Scientific Impact\": \"By digitising and cataloguing over 1,500 amber fossils, AMBCAT promotes interdisciplinary research and public engagement, enabling the use of fossil data in fields ranging from climate, ecosystem and biodiversity change, to evolutionary and phylogenetic research. Since amber fossils have fascinated the public ever since, results from amber studies may be used to make concepts of paleoecosystem and climate research accessible to non-scientists. AMBCAT also serves as a bridge between scientific inquiry and public education, offering opportunities for exhibitions, educational resources, and media content that bring paleoscience to life.\\nAMBCAT will result in building and strengthening collaboration networks in Europe- and worldwide and lead to synergies, avoiding that a same sample is loaned or scanned/analysed several times by different research groups, and contributing to a usage of scan data for different purposes.\",\n      \"Keywords\": \"amber fossils, climate change, biodiversity, evolutionary research, phylogenetic research, data catalogue, data visualisation, μCT scanning technologies\",\n      \"Principal investigator\": {\n        \"Name\": \"Dr. Jörg U. Hammel\",\n        \"Organisation\": \"Helmholtz-Zentrum Hereon\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"AMIS - Advanced Metadata Intelligent System\",\n    \"Project_url\": \"https://oscars-project.eu/projects/amis-advanced-metadata-intelligent-system\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"TheAMISproject aims to develop an innovative web application specifically designed for humanities researchers, focusing on text analysis for metadata enrichment. It is expected to be a user-friendly tool for assessing metadata quality, enriching it with additional information, and facilitating text analysis. The web application seeks to streamline metadata creation by providing design assistance, improving academic discoverability, and offering personalised recommendations for scholarly data. By leveraging machine learning, AMIS enables a contextualised approach to metadata enrichment, enhancing research processes, particularly for heritage texts in the Social Sciences and Humanities (SSH).\",\n      \"Challenge\": \"Open Science Service\\nMetadata enrichment in digital humanities is often a manual, time-consuming task, with shared standards being defined but lacking open services that support the large-scale creation of quality metadata. As a result, due to differing interpretations of expectations, information extracted from the same field across a wide range of resources tends to display a high degree of inconsistency. The challenge, therefore, is to strike a balance between speed and precision while taking the researcher’s expertise into account.\",\n      \"Solution\": \"By developing a context-based web application, AMIS will provide an innovative environment for scholars to create and refine their own metadata in faster and more precise ways while also assisting them in identifying best practices and shared vocabularies for describing text content. The service uses machine learning techniques (text classification, named entity recognition - NER, named entity linking - NEL, sentiment analysis, topic modelling, text summarization, sequence labelling, and dependency parsing) to analyse the uploaded files, compare them with data from international repositories, and suggest enhancements in the form of metadata. The core module of AMIS will be trained on a wide variety of resources hosted in repositories such as Zenodo, Nakala, Rossio, Recolecta, Docta, and several others.\",\n      \"Scientific Impact\": \"By enhancing metadata quality and coherence, AMIS will contribute to the convergence of conceptual models in text description and analysis within cultural studies and other SSH disciplines. This will increase comparability among similar research conducted in different academic contexts and corpora. The project will also benefit researchers by improving their own metadata. Existing collections will be upgraded, while new digital corpora will be designed from the outset with enriched metadata perspectives. Heritage texts will not only be made available but will also be virtually linked through a multitude of information points to other existing resources, supporting further discoveries about the circulation of ideas and themes across cultural areas and over time. Finally, AMIS promotes better practices for metadata creation, improving the discoverability and reusability of scientific content.\",\n      \"Keywords\": \"metadata enrichment, nature language processing, text analysis, machine learning, (meta)data quality assessment, digital humanities\",\n      \"Principal investigator\": {\n        \"Name\": \"Fatiha Idmhand\",\n        \"Organisation\": \"University of Poitiers\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"AQUANAVI: Navigating Grand Challenges and their Mitigation using Aquatic Experimental RIs\",\n    \"Project_url\": \"https://oscars-project.eu/projects/aquanavi-navigating-grand-challenges-and-their-mitigation-using-aquatic-experimental-ris\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, LS RI - Life Sciences\",\n      \"Summary\": \"Critical environmental challenges affect aquatic ecosystems worldwide, and the need to coordinate research efforts is increasingly urgent. Mesocosms are key Research Infrastructures (RIs) allowing the study of highly complex environments comparable to natural ecosystems. Mesocosm experiments, which offer controlled yet realistic settings, are crucial for understanding and mitigating the impact of various stressors and their combinations on aquatic ecosystems. The AQUACOSM-RI consortium, comprising over 60 individual state-of-the-art mesocosm facilities at 28 host institutions across Europe, has therefore been instrumental in advancing aquatic environmental research across climate zones including marine, brackish and freshwater ecosystems.\\nTheAQUANAVIproject (Navigating Grand Challenges and their Mitigation using Aquatic Experimental RIs) aims to create an interactive atlas of aquatic mesocosm-based experimental research. Data, publications, reports and information generated by the AQUACOSM consortium and other mesocosm facilities worldwide will be integrated into a single, accessible platform incorporating Open Knowledge Maps' AI-driven visual discovery tools. The project will foster collaborations and, by enhancing data FAIRness, it will facilitate fast discovery and efficient use of aquatic RIs globally, accelerating the development of environmental mitigation strategies.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nAquatic ecosystems are under pressure from a variety of stressors, including pollution, habitat degradation, rising temperatures, and ocean acidification. These challenges interact in complex ways, making it difficult to predict their combined effects and develop mitigation measures. Experimental mesocosms, which offer controlled but realistic environments, are critical for studying the impact of these stressors. However, despite their significance, the extensive yet fragmented resources available in mesocosm-based research remain difficult to access and leverage.\",\n      \"Solution\": \"AQUANAVI addresses this by creating an interactive atlas that integrates data, publications, reports and facility information from theAQUACOSMconsortium and othermesocosm RIs worldwide. This platform enables quick discovery of available resources, facilitating new collaborations and expediting experimental setups. Using theHi Knowledgeplatform, AQUANAVI merges advanced semantic capabilities(e.g.,Wikidata), AI technology (e.g.,Open Knowledge Maps), and visualisation tools (e.g.,Open Knowledge MapsandScholia) to help researchers navigate complex information with ease, using cutting-edge visualisation techniques, artificial intelligence, and knowledge synthesis methods, and bridging gaps between disconnected research facilities and data sources.\",\n      \"Scientific Impact\": \"Open and collaborative by design, AQUANAVI’s architecture will engage a broad range of research communities. By consolidating data and information from diverse RIs, the platform will leverage and enhance the AQUACOSM and related RIs, securing the reusability and interoperability of existing data collections and better exploration of existing RIs in the future.Compliant with FAIR principles and EOSC requirements, AQUANAVI will ensure the long-term sustainability and openness of its resources, enriching both the ENVRI services portfolio and the broader scientific community. In summary, AQUANAVI will empower researchers and stakeholders to implement measures to mitigate the effects of climate change and other Grand Challenges facing aquatic environments, serving as a key resource within and beyond the European Research Area.\",\n      \"Keywords\": \"Aquatic environment, aquatic mesocosm-based experimental research, mesocosm, Hi Knowledge, interactive atlas, environmental mitigation strategies, climate change\",\n      \"Principal investigator\": {\n        \"Name\": \"Tina Heger\",\n        \"Organisation\": \"Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"ARTICYST - Devising Open Science practices to promote research and treatment in cystic kidney disease\",\n    \"Project_url\": \"https://oscars-project.eu/projects/articyst-devising-open-science-practices-promote-research-and-treatment-cystic-kidney\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"Prediction of clinically relevant outcomes is a key asset in chronic non-communicable diseases to guide both patient counselling and targeted therapeutic strategies, and can be a catalyst to demonstrate the impact of Open Science practices when appropriately deployed.TheARTICYSTproject seeks to enhance the prediction of clinically relevant outcomes in autosomal dominant polycystic kidney disease (ADPKD), the most common genetic kidney disease with more than 7 million people affected globally. Prediction of future kidney function loss at a young age is crucial to support patients in major life decisions. By leveraging Open Science practices, the project aims to develop a comprehensive infrastructure for data integration and predictive modelling, supporting patient decision-making and improving therapeutic strategies. The collaboration between two clinical partners and the integration of new data types will drive advancements in understanding and managing this prevalent condition.\",\n      \"Challenge\": \"Open Science project\\nADPKD patients face uncertainty regarding their future kidney function, impacting critical life decisions and treatment plans. Current prediction tools lack precision and do not allow for integration of different data types. This knowledge is crucial to guide therapeutic decision-making processes considering that the only available targeted therapy – the vasopressin-V2-receptor antagonist tolvaptan – comes with considerable side effects and needs to be taken life-long.Furthermore, including data across centres, which would be pivotal to improve such tools and allow for comparability of disease-courses, is non-trivial, hindering the ability to create reliable predictive models.\",\n      \"Solution\": \"ARTICYST will build an infrastructure to solve this problem based on existing multicentre data from large ADPKD cohorts and by making use of operational and existing services of the Science Clusters for the Life Sciences. The project will integrate all available clinical and laboratory data with genotype and MRI imaging results by optimised unbiased model building. Beyond using data contained in existing dedicated research databases, the ARTICYST framework will exploit the full power of all data contained in clinical information systems. Besides, an entirely new layer of predictive power is added by serum proteome data which has already been obtained from all cohorts using a dedicated mass spectrometry pipeline. This will be enabled through innovative database design to integrate omics and clinical data enhanced through the interaction with ELIXIR.In a second step, turning the proteome containing models into clinical diagnostics will require the establishment of targeted diagnostic assay panels. While this cannot be performed within the project at-hand, this process will be initiated in interaction with EATRIS.To prepare the extension of the ARTICYST platform towards additional centres and their biosamples in the future, ARTICYST will reach out to BBMRI-ERIC.\",\n      \"Scientific Impact\": \"The impact of ARTICYST extends beyond ADPKD. The developed platform has the potential to address similar challenges in other chronic non-communicable diseases (NCDs). By establishing targeted diagnostic assay panels and collaborating with patient advocacy groups, the project will increase the visibility of Open Science practices in clinical research. ARTICYST aims to create a sustainable infrastructure that can be adapted for future research and treatment advancements across various medical fields, promoting improved health outcomes for patients globally.\",\n      \"Keywords\": \"Autosomal dominant polycystic kidney disease, target therapy, data infrastructure, predictive modelling\",\n      \"Principal investigator\": {\n        \"Name\": \"Prof. Dr. Oya Beyan\",\n        \"Organisation\": \"Institute for Biomedical Informatics at the University of Cologne\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Astro-CC - Astronomy Open Science Competence Centre Pilot\",\n    \"Project_url\": \"https://oscars-project.eu/projects/astro-cc-astronomy-open-science-competence-centre-pilot\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The Virtual Observatory open standards are the basis for making astronomy data FAIR.\\nThe Astronomy Open Science Competence Centre Pilot project (Astro-CC Pilot) aims at expanding the use of Virtual Observatory standards by astronomy-focused ESFRIs, Research Infrastructures (RIs), and data-producing projects of all scales, enabling the astronomy research communities to accelerate their use of Open Science by supporting the implementation of FAIR principles. The project will run Community Competence Centre events engaging the European experts in astronomical data / service interoperability to prepare and define the scope of a Competence Centre that will serve an extended range of astronomical communities.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nThe Astro-CC Pilot project addresses the challenge of accelerating the adoption of Open Science principles within the astronomy research community. The complexity of managing astronomical data across various platforms and ensuring that it adheres to the FAIR principles, requires a community approach. The community also needs training and improved access to tools that support data interoperability and sharing across multiple research infrastructures (RIs).\",\n      \"Solution\": \"The project will run Community Competence Centre events, specifically targeted to engage different facets of the astronomy communities:\",\n      \"Scientific Impact\": \"The Astro-CC Pilot project will play a critical role in enhancing the astronomy community's adoption of Open Science by implementing FAIR principles. By improving data-sharing frameworks, it will facilitate new methodologies for managing diverse astronomical datasets within the EOSC.\\nBuilding on previous initiatives like ESCAPE and EOSC Future, the project will engage national RIs and expand into interdisciplinary fields, such as planetary science and heliophysics. These efforts aim to promote interoperability across diverse astronomical domains. The lessons learned from Astro-CC will inform the development of Competence Centres in other scientific disciplines, fostering broader collaboration and advancing the principles of Open Science.\",\n      \"Keywords\": \"Astronomy Open Science Competence Centre, data provider, technology forum, scientific training\",\n      \"Principal investigator\": {\n        \"Name\": \"Mark Allen\",\n        \"Organisation\": \"CNRS Strasbourg Astronomical Data Centre (CDS)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Astro-DM TSP - The Astronomy Dark Matter Test Science Project\",\n    \"Project_url\": \"https://oscars-project.eu/projects/astro-dm-tsp-astronomy-dark-matter-test-science-project\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The Astronomy Dark Matter Test Science Project (Astro-DM TSP) proposes to extend the work of theESCAPE Dark Matter Test Science Projectby creating common tools and services that facilitate joint experimental and observational constraints on dark matter properties. By leveraging the capabilities of major research infrastructures (RIs), such as the Vera Rubin Observatory and ESA's Euclid Space Telescope, the project will integrate thematic astronomical services into the EOSC, addressing the specific needs of the scientific community.\",\n      \"Challenge\": \"Open Science project, Citizen science, Main RI concerned\\nUnderstanding dark matter remains one of the most pressing challenges in contemporary astronomy, with all existing experimental evidence derived from observational astronomy. Despite significant advancements in high-energy and astroparticle physics, there are currently no shared tools or services available for observational astronomers to interpret the results of direct detection experiments, nor vice versa. This gap highlights the need for a collaborative framework to integrate these disciplines effectively.\",\n      \"Solution\": \"Astro-DM TSP will develop essential tools for analysing strong gravitational lensing, which provides unique insights into dark matter. The project will utilise existing services from the ESCAPE Science Cluster and ESA, including the ESCAPE data lake and Virtual Research Environment, while producing documentation on dark matter constraints, modelling Euclid's spectroscopic data on strong gravitational lenses, and developing advanced imaging tools for both Rubin and Euclid. These efforts will ensure that all generated data remains FAIR-compliant, fostering sustainable re-use and reproducibility across the astronomical community.\\nBy leveraging the capabilities of major RIs, such as the Vera Rubin Observatory and ESA's Euclid Space Telescope, the project will integrate thematic astronomical services into the EOSC, addressing the specific needs of the scientific community.\",\n      \"Scientific Impact\": \"Astro-DM TSP significantly advances dark matter research by integrating observational and experimental approaches. It enhances collaboration among astronomy, astroparticle, and particle physics communities. The project aims to develop essential tools for analysing strong gravitational lensing, enabling the discovery of rare lensing events and high-redshift galaxies. By producing joint derived data products from the Euclid Space Telescope and Vera Rubin Observatory, it aims to maximise scientific output and efficiency. The tools developed will support future research across multiple major RIs, including HST, JWST, and NASA's Roman space telescope. The project will also promote FAIR data principles, ensuring sustainable reuse and reproducibility. Ultimately, Astro-DM TSP contributes to a coherent understanding of dark matter, supporting the broader goals of the EOSC.\",\n      \"Keywords\": \"Dark Matter, astronomy, astronomical services, ESCAPE data lake, observational astronomy, experimental astronomy\",\n      \"Principal investigator\": {\n        \"Name\": \"Stephen Serjeant\",\n        \"Organisation\": \"The Open University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"BIO-CODES - Enhancing AI-Readiness of Bioimaging Data with Content-Based Identifiers\",\n    \"Project_url\": \"https://oscars-project.eu/projects/bio-codes-enhancing-ai-readiness-bioimaging-data-content-based-identifiers\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"(Standardised) content identification is becoming an essential requirement for bioimaging data reuse and the seamless integration of bioimaging data into AI models. Transparency regarding the origin of data, distinguishing between conventionally generated and AI-generated data, is essential to maintain data integrity and facilitate informed analysis. In this context, the International Standard Content Code - ISCC is a new, open-source identification system and globalISO standard, ensuring transparency, accessibility, and widespread adoption. TheBIO-CODES projectseeks to enhance the AI-readiness of bioimaging data by developing and implementing content-based identifiers, using ISCC. The project addresses the increasing complexity of bioimaging data, ensuring it adheres to FAIR principles, and prepares datasets for reliable use in AI-driven analyses. By integrating globally unique references into key platforms, such as OMERO, BIO-CODES aims to advance data integrity and streamline bioimage certification processes, ensuring transparency and reproducibility.\",\n      \"Challenge\": \"Open Science project, Industry cooperation, Main RI concerned, Cross-domain/Cross-RI\\nAs bioimaging data grows in complexity, much of it remains non-compliant with FAIR principles, which complicates data reuse and AI integration. Current methods for identification and certification of bioimaging data lack the robustness needed for generative AI models, risking data integrity, data reliability, thus scientific reproducibility. Addressing these gaps is essential to ensure bioimaging data’s value in Life Sciences research and mitigating biases in AI-driven analyses.\",\n      \"Solution\": \"BIO-CODES will evaluate the ISO 24138 International Standard Content Code (ISCC) and apply it to bioimaging data in Life Science research. Generated from digital content using cryptographic and similarity hash algorithms, ISCC ensures data integrity and supports use cases like deduplication, database synchronisation, and data provenance. The project will test routine proprietary formats from imaging core facilities and engage vendors. If ISCC proves useful, recommendations will be made for integrating it into existing workflows and software applications used for authentication and certification of bioimages. The main goal is to assess ISCC's applicability and create a proof-of-concept, with integration into platforms like OMERO to ensure FAIR compliance.\\n*OMERO is one of the most versatile and widely used platforms for managing bioimaging data, offering tools for storage, organisation, and analysis. By seamlessly integrating globally unique identifiers into OMERO it will streamline AI-driven bioimaging data preparation while ensuring data integrity and FAIR compliance. Standardised authentication processes will enable researchers to share datasets confidently, enhancing transparency and reproducibility. This approach also addresses ethical concerns in AI, mitigating biases from unverified data and promoting reliable AI models in biomedical research.\",\n      \"Scientific Impact\": \"BIO-CODES will enhance transparency and collaboration among researchers by introducing standardised content identifiers in bioimaging workflows. This enhances data reuse, mitigates ethical concerns in AI applications, and improves the reliability of AI models in Life Sciences. The ISCC’s ability to generate unique identifiers directly from digital files ensures ease of use and eliminates the need for manual management like DOIs. Its open-source nature supports decentralised digital content identification and encourages continuous improvement through collaboration among researchers, developers, and institutions.\",\n      \"Keywords\": \"bioimaging, bioimaging data, International Standard Content Code, ISCC, ISO 24138 ISCC standard, AI applications, AI models, big data, data integrity, standardised content identifier\",\n      \"Principal investigator\": {\n        \"Name\": \"Sylvia Le Dévédec\",\n        \"Organisation\": \"Leiden University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"CASTAL - Case Studies Atlas of Local Sustainability Transition\",\n    \"Project_url\": \"https://oscars-project.eu/projects/castal-case-studies-atlas-local-sustainability-transition\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"Over the last 25 years, the European Union has been solidly committed to making its cities and local communities more sustainable. Indeed, European communities, cities and regions have engaged in Local Sustainability Transition (LST), making the EU a laboratory for sustainable solutions, including the socio-technical aspects of transition. In this context, the CASTAL project consolidates over 500 European case studies on LST into a centralised digital repository: theCase Studies Atlas of Local Sustainability Transition - CASTAL, a new digital research object collecting FAIR-compliant case studies (mainly licensed in CC by). CASTAL contributes to Open Science and EOSC, by enabling future research and the exploitation of research results by researchers from SSH and other disciplines, and from local authorities, businesses and citizens.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nThe CASTAL project addresses several key challenges in leveraging existing research on LST in Europe. These include the fragmentation of knowledge, as case studies are dispersed across multiple repositories and projects; the lack of centralised archives for storing and preserving data; and insufficient efforts to organise data using indexes or metadata. Additionally, difficulties in aggregating data from multiple studies on the same cases hinder comprehensive analysis. As a result, valuable insights from two decades of LST research remain underutilised for future research and practical applications, such as cross-case comparisons or diachronic studies.\",\n      \"Solution\": \"The CASTAL project proposes several solutions for leveraging research on LST in Europe. It will create an archive collecting over 500 case studies, ensuring preservation and accessibility; a metadata dataset, describing each and all of the case studies (data are curated and deposited in DASSI repositories); and a web portal as a companion tool for increasing the accessibility and findability of the Atlas, and allowing users to update the Atlas as new case studies are produced.\\nThe project aims to engage with the scientific community of reference and with other actors (e.g., local authorities, European stakeholders), promote ten secondary studies using CASTAL as a data source, and develop web services and guidelines for updating the Atlas.\",\n      \"Scientific Impact\": \"By adhering to FAIR data principles, CASTAL will improve data sharing and reuse among researchers, local authorities, and businesses, maximising the impact of LST research and fostering evidence-based sustainability solutions in local communities and municipalities across Europe. The project will also integrate with CESSDA ERIC for long-term curation, creating a multi-purpose platform that supports cross-case analysis and knowledge brokerage for various stakeholders: SSH researchers, local authorities, policymakers and stakeholders with information and knowledge about the state of LST around Europe.\",\n      \"Keywords\": \"Local Sustainability Transition, Responsible Research and Innovation, Open Science, Societal Engagement and Research Project Management, data sharing, data curation\",\n      \"Principal investigator\": {\n        \"Name\": \"Fabio Feudo\",\n        \"Organisation\": \"Knowledge and Innovation - K&I\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"CDIF-4-XAS: Describing X-Ray Spectroscopy Data for Cross-Domain Use\",\n    \"Project_url\": \"https://oscars-project.eu/projects/cdif-4-xas-describing-x-ray-spectroscopy-data-cross-domain-use\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"TheCDIF-4-XASproject- Describing X-Ray Spectroscopy Data for Cross-Domain Use, will enable new science by making it easier to access, combine and reuse XAS data across research infrastructures (RIs) and disciplines.\\nCDIF-4-XAS will enhance the interoperability and reusability of XAS data by applying theCross-Domain Interoperability Framework (CDIF), a set of guidelines and practices for using domain-agnostic standards to support the interoperability and reusability of FAIR data, especially across domain and institutional boundaries. By embracing FAIR principles, the project aims to streamline the sharing of XAS data, thus enabling more efficient data integration across RIs and scientific domains, including life sciences, chemistry, and environmental sciences.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nX-ray Absorption Spectroscopy (XAS) data is vital for many fields, but its specialised formats and metadata conventions hinder cross-domain use. Data sharing between applications, databases, and facilities is inefficient, leading to the loss of essential experimental information. With increasing data volumes and interdisciplinary collaborations, the need for a more interoperable solution becomes urgent.\",\n      \"Solution\": \"The Cross-Domain Interoperability Framework (CDIF) addresses these challenges by establishing domain-agnostic standards to support data sharing across institutional and disciplinary boundaries. Through CDIF-4-XAS, metadata and provenance for XAS data are standardised, enhancing the integration of XAS datasets into EOSC infrastructures. This promotes data reuse across different research domains, making XAS data more accessible and valuable.\\nThe main outputs of the project will be:\",\n      \"Scientific Impact\": \"Coordinated by CODATA, and involving a number of PaNOSC partners and key infrastructures, CDIF-4-XAS will unlock new possibilities for research by enabling seamless data combination and analysis across RIs. Researchers in energy, chemistry, and environmental sciences, among others, will benefit from easier access to XAS data. By implementing CDIF for XAS data (XAS-CDIF), the project tackles the interoperability and reusability challenges described above, and can better integrate the XAS data into the emerging EOSC infrastructures for cataloguing, integration and analysis. The benefits will be considerable: existing resources will become more available, and it will become easier and more efficient to use data across RIs and in other domains. Finally, XAS data is a key research product in the domain of energy research, thereby representing key strategic developments towards a sustainable future for the global society. XAS is an important characterization method for materials used in these fields and essential for developing new materials.\",\n      \"Keywords\": \"X-ray spectroscopy, XAS, XAS data, Cross-Domain Interoperability Framework, CDIF\",\n      \"Principal investigator\": {\n        \"Name\": \"Simon Hodson\",\n        \"Organisation\": \"CODATA\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"CODEMETASOFT\",\n    \"Project_url\": \"https://oscars-project.eu/projects/codemetasoft\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, ESCAPE - Astronomy, Nuclear and Particle Physics, LS RI - Life Sciences, PANOSC - Photon and Neutron Science, SSHOC - Social Sciences and Humanities, Other\",\n      \"Summary\": \"Research Software (RS) has become a key asset to support, reuse and reproduce research outputs described in scientific publications. The growing acknowledgment of RS’s importance is reflected in initiatives, such as the Software Citation Principles, which advocate for the recognition of software contributions in academic curricula. However, the current state of Research Software metadata remains fragmented and inconsistent, hindering effective usage and compliance with FAIR principles. TheCODEMETASOFTproject aims to develop a comprehensive framework to streamline the management, enrichment, and propagation of RS metadata, thereby enhancing interoperability and supporting the scientific community's efforts to leverage software in research effectively.\",\n      \"Challenge\": \"RS metadata is crucial for assessing RS FAIRness, and its significance in academia is increasingly recognised, but navigating the existing metadata landscape presents multiple challenges: 1) software metadata is currently disseminated in heterogeneous files and documentation, and there is currently no framework integrating them. As a result, developers usually have to manually duplicate efforts when creating metadata records. 2) Software metadata consistency and curation are performed by hand. 3) There is a lack of automated tools to suggest enhancements and fill gaps in RS metadata.\",\n      \"Solution\": \"CODEMETASOFT will develop an innovative framework based on the CodeMeta standard. This framework will feature an Autocomplete CodeMeta Wizard adopting CodeMeta as a common metadata interchange format, to simplify the creation of metadata records and automate the management, enrichment and propagation of RS metadata across RS project files. The project will also introduce methodologies to compare similar (or complementary) metadata records, while detecting RS metadata gaps and enrichment suggestions to improve the quality and completeness of metadata records and README files.\",\n      \"Scientific Impact\": \"The implementation of CODEMETASOFT is poised to enhance the quality and consistency of Research Software metadata across Europe, promoting better compliance with FAIR principles. By reducing the manual workload and providing automated solutions for metadata management, the project aims to foster a more interoperable and reliable ecosystem for research outputs. While the resources developed will be made publicly available under an Open Source licence, the developed tools will be integrated with both the ESCAPE Open Source Software and Service Repository - OSSR (which uses CodeMeta as a source of metadata) and the EOSC. With the increasing adoption of CodeMeta by European research infrastructures, the project has the potential to make a critical impact by improving the quality of metadata at its core without disrupting the workflows followed by RSEs.\",\n      \"Keywords\": \"Research Software, Software Citation Principles, Research Software metadata, CodeMeta\",\n      \"Principal investigator\": {\n        \"Name\": \"Daniel Garijo and Thomas Villaume\",\n        \"Organisation\": \"Universidad Politécnica de Madrid and LAPP\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"COPLI - Creating a FAIR Open Science Pipeline for high-resolution LOFAR Imaging\",\n    \"Project_url\": \"https://oscars-project.eu/projects/copli-creating-fair-open-science-pipeline-high-resolution-lofar-imaging\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"COPLIis a project focused onCreating a FAIR Open Science Pipeline for high-resolution imagingusing the LOFAR radio telescope. By processing extensive datasets, COPLI aims to produce high-quality radio images that enhance our understanding of galaxy formation and evolution. The initiative emphasises Open Science principles, ensuring that valuable astronomical data is accessible for research and collaboration within the scientific community.\",\n      \"Challenge\": \"Open Science project, Main RI concerned, Cross-domain/Cross-RI\\nUnderstanding how galaxies formed after the Big Bang and evolved over time is a fundamental question in astronomy. This type of investigation requires deep radio images with sub-arcsecond resolution, which the pan-European LOFAR radio telescope can provide. However, the lack of automated pipelines for processing the vast data volume has hindered the utilisation of all available LOFAR stations, resulting in images with significantly lower resolution and sensitivity.\",\n      \"Solution\": \"COPLI will develop an interoperable and sustainable processing pipeline for widefield high-resolution LOFAR imaging. The aim is to create a pipeline capable of running in a (largely) automated fashion on large-scale computing infrastructures while ensuring that the pipeline and science products adhere to FAIR principles. This includes improving data lineage by enhancing the data processing framework, software, and derived science products with provenance information, resulting in higher traceability and reproducibility. All science-ready data products will be ingested into a FAIR and trusted repository that is federated with EOSC to make them visible and accessible to the global astronomical community.\",\n      \"Scientific Impact\": \"The COPLI project will deliver an ultra-deep radio view of our Universe, with images and catalogue products accompanied by comprehensive processing history and metadata, made available to the global astronomical community, other research domains, and citizen scientists. The availability of the images to the entire community will facilitate numerous other radio and multi-wavelength astronomical studies, such as the evolution of black hole accretion and time-variable radio emission from stars and exoplanets. COPLI will serve as an inspiring example of how Open Science can contribute to advancing research and knowledge in the field of astronomy. In this respect, the project's state-of-the-art pipeline will be crucially important for the LOFAR ERIC LOFAR2.0 project and the LOFAR community. A primary objective of LOFAR2.0 is to map the entire northern sky at sub-arcsecond resolution and the project pipeline framework directly contributes to the LOFAR2.0 project goals.\",\n      \"Keywords\": \"LOFAR radio telescope, high-resolution LOFAR imaging, automated pipelines\",\n      \"Principal investigator\": {\n        \"Name\": \"Reinout van Weeren\",\n        \"Organisation\": \"Leiden University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"DraCorOS. Fostering Open Science in Digital Humanities by connecting the “DraCor” Ecosystem to EOSC\",\n    \"Project_url\": \"https://oscars-project.eu/projects/dracoros-fostering-open-science-digital-humanities-connecting-dracor-ecosystem-eosc\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"The community-driven drama corpora platformDraCor, currently hosting over 3,300 theatre plays in 18 corpora from more than 15 languages, offers a digital ecosystem of tools, research-driven APIs, programming libraries and Jupyter notebook-based training material.\\nThe DraCorOS project will further develop this platform by aligning its services with EOSC, promoting a culture of Open Science, and evolving its open software architecture for broader reuse in other Open Science projects. It aims at boosting accessibility, reproducibility and engagement with data-driven research, with particular attention to small, under-resourced European languages.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nWith the digital transformation, a large joint virtual space of cultural data is being created, which supports the public perception of a common European heritage. Humanities research has the opportunity to develop into truly FAIR-data-oriented research based on large amounts of data. However, this transformation demands not only high-level e-infrastructures, but also active participation from small disciplinary communities. These communities, foundational to Humanities research, require support in establishing their own Open Science cultures to ensure a sustainable shift towards data-driven, open scholarship.\\nSpecifically, the DraCor drama corpora platform, which can be regarded as the main hub for data-driven research on European drama and as a community-based competence centre for Computational Literary Studies, still faces some challenges to be considered a community-based ecosystem of services fully committed to Open Science principles.\",\n      \"Solution\": \"Enhanced Discoverability and Access:The project will transform theDraCorplatform into a semantically enriched, linked open data environment, integrating its datasets, tools, and services with EOSC platforms like SSH Open Marketplace and CLARIN Virtual Language Observatory. This will improve the findability and accessibility ofDraCorresources, fostering easier access to data for researchers.\\nPromoting Open Science Culture:By offering Open Science training and workflows, the project will empower researchers in Computational Literary Studies and related fields to adopt FAIR-data practices. This will increase research productivity, transparency, and reproducibility, helping scholars actively engage with EOSC services.\\nOpen Source Software Development:TheDraCorplatform’s underlying software architecture, based on Programmable Corpora, will be further developed into an independent Open Source solution. This will enable other Open Science projects to easily host and share their own (programmable) textual research corpora, promoting FAIR data-intensive research.\",\n      \"Scientific Impact\": \"The project will facilitate the transfer ofDraCor’sprogrammable corpora software to other disciplines, and will contribute to the cross-domain Linked Open Data cloud, while providing high-quality, human-curated training data for AI-driven workflows.\\nThe integration with the EOSC services will lead to an increased visibility and findability of the resources from the DraCor ecosystem, enabling the geographically and linguistically diverse academic communities of (computational) literary scholars to conduct research on a broader and more diverse basis. The dissemination to these communities (e.g., DARIAH and CLARIN) of the technical solutions developed to enhance reproducible research, and their publication as workflows, can lead to a higher quality of research.\",\n      \"Keywords\": \"Computational Literary Studies, drama corpora platform, DraCor, Open Science training, Linked Open Data\",\n      \"Principal investigator\": {\n        \"Name\": \"Peer Trilcke\",\n        \"Organisation\": \"University of Potsdam\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"EHRI-KG - European Holocaust Research Infrastructure: Knowledge Graph\",\n    \"Project_url\": \"https://oscars-project.eu/projects/ehri-kg-european-holocaust-research-infrastructure-knowledge-graph\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"The European Holocaust Research Infrastructure (EHRI) aims to enhance access to Holocaust-related historical sources and promote transnational scholarship. TheEHRI Portalhosts over 370,000 archival descriptions from 2,000 institutions worldwide. The EHRI-KG project will develop a comprehensive Linked Open Data (LOD) Knowledge Graph (KG) to complement the EHRI Portal and further advance data accessibility, aligning with the evolving semantic web and LOD Standards, and supporting the integration of Holocaust data into the EOSC.\",\n      \"Challenge\": \"Open Science Service\\nThe European Holocaust Research Infrastructure (EHRI) project, initiated in 2010, aims to enhance access to Holocaust-related historical sources, fostering transnational scholarship and promoting education and remembrance. The project has developed the EHRI Portal, which hosts over 370,000 archival descriptions from over 2,000 institutions globally. However, the complex, multilingual nature of these sources presents significant challenges for researchers. To address this, the proposed project seeks to create a comprehensive Linked Open Data (LOD) Knowledge Graph (KG) that aligns with evolving semantic web standards.\",\n      \"Solution\": \"The EHRI Knowledge Graph (EHRI-KG) will complement the existing EHRI infrastructure, providing a scalable template for institutions to adopt LOD with minimal technical investment. As one of the first large-scale applications of the Records in Contexts (RiC) standard — a new standard sponsored by the International Council on Archives (ICA) and designed specifically for archival data — EHRI-KG will capture and disseminate valuable insights through open-source deliverables, public documentation, and training materials. The project will also integrate outputs into the EOSC via the SSHOC Marketplace, broadening its impact beyond the Holocaust research and archival communities.\",\n      \"Scientific Impact\": \"The EHRI-KG project will significantly enhance cohesion and interoperability within Holocaust research by making a unified dataset of Holocaust archival material available in Linked Open Data (LOD) form, promoting new patterns of exploration and discovery. Its impact extends beyond Holocaust research, enhancing data reuse, interoperability, and accessibility in educational, collaborative, and archival settings. The project will also provide a template for other archival institutions to adopt LOD, helping smaller organisations navigate legacy systems and enhance data accessibility. The project will contribute to shaping future archival semantic web standards, promoting broader adoption of the Records in Contexts (RiC) ontology and conceptual model.\",\n      \"Keywords\": \"Data accessibility, Linked Open Data - LOD, Records in Contexts (RiC) standard, Records in Contexts (RiC) ontology\",\n      \"Principal investigator\": {\n        \"Name\": \"Mike Bryant and Herminio García González\",\n        \"Organisation\": \"NIOD Institute for War, Holocaust & Genocide Studies, and Kazerne Dossin Museum, Memorial & Documentation Centre\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"ETAP - Einstein Telescope Analysis Portal\",\n    \"Project_url\": \"https://oscars-project.eu/projects/etap-einstein-telescope-analysis-portal\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"TheEinstein Telescope Analysis Portal (ETAP)is a cutting-edge platform developed to address the growing demand for open access to gravitational wave (GW) data, following the historic discovery of GWs in 2015 from the collision of black holes. This groundbreaking event marked the beginning of a new era in astronomy, driving the establishment of Open Science frameworks, such as the Gravitational Wave Open Science Center (GWOSC). As Europe prepares for the next-generation Einstein Telescope (ET), the ETAP project will provide researchers and citizen scientists with an innovative platform and advanced tools to access and analyse the complex data expected from future GW detections, fostering collaborative multi-messenger research and promoting sustainability in scientific computing.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Citizen science, Main RI concerned, Cross-domain/Cross-RI\\nThe ET will increase the detection rate of GWs by a factor of a thousand compared to current detectors, producing data of unprecedented complexity. This surge in data complexity, combined with the need for cross-disciplinary collaboration in multi-messenger astronomy, presents a significant computational and data integration challenge. Furthermore, ensuring that these complex datasets fully adhere to FAIR principles will require the development of new platforms capable of managing resources from multiple Research Infrastructures (RIs) while addressing sustainability concerns in high-performance computing.\",\n      \"Solution\": \"The project will build on the ESCAPE Virtual Research Environment (VRE) to produce the Einstein Telescope Analysis Platform, ETAP, a common computing platform for both ET and citizen scientists alike. The portal will integrate data from multiple RIs and utilise advanced tools, supporting both simple jupyter notebooks and complex, reproducible analysis workflows in REANA. A key focus is on extending the ESCAPE VRE to work with multiple Data Lakes, enabling seamless access to data and metadata from multiple RIs to support multi-messenger and GW science. In this context, a high-level tool written in python will be developed to provide a more coherent interaction with multiple Data Lakes, together with a common interface to ensure FAIR-compliant metadata integration. Finally, the ETAP project will also design a monitoring service to record computing metrics of the ETAP, allowing analysis of GW science pipelines computing costs.\",\n      \"Scientific Impact\": \"ETAP will empower researchers and citizen scientists to conduct cutting-edge research in GW and multi-messenger science, leveraging the vast potential of the Einstein Telescope. The platform’s ability to handle and integrate data from multiple Research Infrastructures (RIs) will enhance the scientific impact of GW data analysis and ensure that metadata adheres to FAIR principles. Moreover, by integrating a new service to monitor the environmental and financial costs of computational workflows, ETAP not only meets the future computational demands but also addresses sustainability challenges, contributing to the long-term success of Open Science in GW research.\",\n      \"Keywords\": \"gravitational wave, gravitational wave data, multi-messenger science, Einstein Telescope\",\n      \"Principal investigator\": {\n        \"Name\": \"Paul Laycock\",\n        \"Organisation\": \"University of Geneva\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"EVA-FAIR - Implementing FAIR principles in the European Vegetation Archive data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/eva-fair-implementing-fair-principles-european-vegetation-archive-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"TheEVA-FAIRproject aims to implement FAIR principles in European databases of vegetation plots. These databases store plant occurrence and abundance records sampled across the continent for over a century. Such plots are the primary source of information on plant diversity in natural and anthropogenic habitats and their changes over time. EVA, the European Vegetation Archive currently contains more than 2 million vegetation plots with approx. 50 million plant occurrence records.\\nThe goal of the project is to move EVA data towards open access, publishing them in trusted and FAIR-compliant repositories, such as Zenodo.\\nBy advancing open science practices, the project supports researchers, conservationists, and citizen scientists across Europe, enabling comprehensive analyses of biodiversity changes over time.\",\n      \"Challenge\": \"Open Science project\\nThe European Vegetation Archive (EVA) has historically faced data ownership restrictions that hindered full data openness. While the archive has contributions from 108 data providers and contains extensive records essential for biodiversity research, its varying access levels - Open Access, Semi-restricted Access, and Restricted Access - have limited the potential utility of this wealth of information. As more data providers recognise the advantages of adopting FAIR principles, there is a pressing need to facilitate a transition toward more open access, which can greatly enhance collaboration and data sharing across the continent.\",\n      \"Solution\": \"The EVA-FAIR project will implement a two-phase approach to facilitate this transition. Firstly, it plans to use and develop the Czech National Phytosociological Database (CNPD) as a model for best practices in data sharing. This pilot will produce open datasets and develop R codes for taxonomic harmonisation and data export to the Global Biodiversity Information Facility (GBIF), which is a global open-access online database of data on plant occurrences used for numerous projects of basic and applied research and for citizen science. An open version of the database (CNPD-Open), and a taxonomically harmonised and geographically and environmentally stratified subset of this database (CNPD-Open-Strat) will be released. All the data and scripts will be published on Zenodo, GitHub linked with Zenodo, and the EOSC-compliant Czech National Repository Platform.\\nThe second phase will engage EVA data providers through workshops and training sessions, showcasing the benefits of open data, and equipping them with the necessary skills to contribute to the initiative.\",\n      \"Scientific Impact\": \"By advancing the public availability of European vegetation-plot data, EVA-FAIR will significantly enhance the resources accessible to researchers and conservation practitioners alike. This project aims to break down barriers that have restricted access to valuable biodiversity data. It will also foster interdisciplinary research and applied studies in nature conservation and ecosystem restoration. The project not only aligns with the goals of the Environmental Research Infrastructures (ENVRI) Community and the International Association for Vegetation Science but also aims to improve citizen science initiatives in regions where high-quality biodiversity data remain scarce, ultimately enriching our understanding of Europe's plant diversity.\",\n      \"Keywords\": \"European Vegetation Archive, vegetation-plot data, trusted repositories\",\n      \"Principal investigator\": {\n        \"Name\": \"Milan Chytrý\",\n        \"Organisation\": \"Masaryk University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIR-EO - FAIR, Open and AI-Ready Earth Observation Resources\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fair-eo-fair-open-and-ai-ready-earth-observation-resources\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"Earth Observation (EO) data is crucial for monitoring environmental changes, but its potential is often underutilised due to challenges in integrating advanced AI techniques. TheFAIR-EO projectaims to bridge this gap by making EO data more accessible and AI-ready, fostering innovation across both fields. Through the creation of AI4EOhub, a comprehensive and accessible repository of annotated datasets, AI models, and analysis pipelines, the project will enhance data discovery, preparation, analysis, and reproducibility. By aligning with FAIR principles, FAIR-EO supports global initiatives for sustainable and collaborative research, driving forward the application of AI in environmental and geospatial sciences.\",\n      \"Challenge\": \"Open Science project\\nThe AI and EO communities often struggle with data fragmentation, lack of standardisation, and barriers to collaboration. Data discovery, preparation, and the reproducibility of experiments are particularly difficult due to the absence of accessible, well-documented repositories. Additionally, EO data lacks comprehensive FAIR adherence, limiting its potential for AI applications and hindering cross-domain innovation.\",\n      \"Solution\": \"FAIR-EO aims to establish AI4EOhub, an integrated repository providing preprocessed, standardised, and annotated EO datasets optimised for AI applications, alongside standard AI methods and pre-trained models. A key innovation is the ontology-based schema for the semantic annotation of EO data, deep learning models, pipelines and results, enhancing their description, organisation, and interoperability for broader applications. AI4EOhub will house several key components designed to streamline the use of EO data in AI applications: a DataHub with annotated EO datasets, a ModelHub with AI methods and pre-trained models and their performance metrics, a PipelinesHub for ready-to-use analysis pipelines, and a ResultsHub for performance metrics and results from various models applied to a wide range of EO datasets. Moreover, the knowledge base will be populated with the semantically annotated experimental datasets, models, pipelines and results. Finally, AI4EOExplorer, a web-based interface, will make it easy for users to interact with AI4EOhub resources.\",\n      \"Scientific Impact\": \"By promoting resource reuse and avoiding repetitive experiments, FAIR-EO supports sustainable research practices, aligning with the 'Green AI' initiative. It encourages interdisciplinary research and develops an interoperable semantic annotation schema to enhance data, AI model, and pipeline description and organisation. Emphasising interoperability with existing platforms and tools, it fosters cohesive and efficient data (re-)use. AI4EOhub will provide FAIR and AI-ready EO resources, setting a new standard for machine learning in EO. By bridging the gap between the EO and AI communities, it will significantly enhance the application of these technologies across scientific disciplines.\",\n      \"Keywords\": \"Earth Observation resources, Earth Observation data, AI4EOhub, DataHub, ModelHub, PipelinesHub, ResultsHub, AI4EOExplorer, AI applications, environmental sciences, geospatial sciences\",\n      \"Principal investigator\": {\n        \"Name\": \"Dragi Kocev\",\n        \"Organisation\": \"Bias Variance Labs d.o.o.\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIRFUN4Biodiversity - FAIR AI models for functional annotation of biodiversity genomics resources\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fairfun4biodiversity-fair-ai-models-functional-annotation-biodiversity-genomics-resources\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"FAIRFUN4Biodiversityaims to enhance functional annotation of genomic resources generated by the biodiversity genomics community, particularly for non-model organisms. By leveraging AI-driven methodologies, this project seeks to generate publicly accessible functional data, promote Open Science practices, and improve cross-domain interoperability. The project will ensure compliance with FAIR principles, and will expand the portfolio of open access tools readily available for the biodiversity genomics community, ultimately contributing to a deeper understanding of the functional landscape of non-model organisms.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nUnderstanding the evolution of coding genes and their functions is crucial in evolutionary biology, yet many protein-coding genes remain poorly characterised, particularly in non-model organisms. This lack of functional annotation, especially in what is termed the 'dark proteome' - genes  within a proteome without functional annotation - leads to incomplete models of evolutionary change and limits the identification of conserved or lineage-specific features. Traditional homology-based methods often fail to adequately transfer functional annotations. With the advent of initiatives such as the European Reference Genome Atlas or ATLASEA, where new genomes from non-model organisms are being sequenced and released daily, we need to leverage faster and scalable sequence-based functional prediction methods. In this line, embracing concepts from orthogonal disciplines, such as computer science and Artificial Intelligence (AI), could alleviate the problem.\",\n      \"Solution\": \"FAIRFUN4Biodiversity addresses these challenges with FANTASIA (Functional ANnotation based on embedding space SImilArity), a novel pipeline leveraging AI models from natural language processing, which overcomes the current limitations of homology-based methods and recovers functional annotation with great informativeness for virtually all genes in a proteome. The tool is currently available as an open access Singularity container.\\nTo make it fully FAIR-compliant,  the project will i) generate functional annotation data of the genomic resources generated by the biodiversity genomics community, and make all data publicly available to the research community; (ii) provide publicly-available functional annotation of genomes generated by the current biodiversity genomics consortia, such as the European Reference Genome Atlas (ERGA) or ATLASEA; (iii) Engage with the biodiversity genomics community for knowledge transfer and connect to RIs within the Science Clusters such as ELIXIR or LifeWatch; (iv) Improve the tool by leveraging bilingual models that take into account both natural language processing algorithms and protein structure inference ones, such as ProstT51.\",\n      \"Scientific Impact\": \"This project significantly advances biodiversity genomics by addressing functional annotation for non-model organisms. By expanding publicly available functional annotation of the generated genomic resources, FAIRFUN4Biodiversity enhances downstream biological analyses and fosters collaboration among European RIs, such as ELIXIR and LifeWatch. Ultimately, it aims to deepen our understanding of the functional landscape of non-model organisms, stimulate Open Science practices in functional annotation of genomes of non-model organisms, improve cross-domain interoperability, and strengthen long-term coordination within the biodiversity genomics community at the European level.\",\n      \"Keywords\": \"biodiversity genomics, biodiversity information, technology, data science, European Reference Genome Atlas, functional annotation, FANTASIA\",\n      \"Principal investigator\": {\n        \"Name\": \"Rosa Fernández\",\n        \"Organisation\": \"Institute of Evolutionary Biology, Spanish National Research Council (CSIC)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIRGamma: Contribution for the FAIRisation of Gammapy\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fairgamma-contribution-fairisation-gammapy\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"Very-high-energy (VHE) astrophysics is transitioning towards Open Science with the construction of new large Research Infrastructures (RIs), such as the Cherenkov Telescope Array (CTA) and KM3NeT. These observatories will provide open data with a dedicated open data format, and open research software usable by the users. For this software to be fully FAIR-compliant, theFAIRGammaproject aims at integrating the FAIR principles into Gammapy, the leading Python library for VHE gamma-ray astrophysics. By enhancing Gammapy’s compliance with FAIR standards, the project will facilitate the generation of interoperable and reusable data, supporting the broader astrophysics community in advancing open and collaborative research across multi-wavelength and multi-messenger platforms.\",\n      \"Challenge\": \"Open Science project, Main RI concerned, Cross-domain/Cross-RI\\nThe data and the open research software deployed at CTA and KM3NeT do not fully adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles yet, particularly for research software (FAIR4RS). Gammapy, the leading Python library in this field, is at the forefront of this challenge, but it still requires improvements to achieve full FAIR compliance.\",\n      \"Solution\": \"Handling FAIR data and generating FAIR data are the core objectives of the project. The FAIRGamma project seeks to make Gammapy FAIR by ensuring that it fully implements the FAIR4RS principles. This implies supporting different versions of the common open data formats of the VHE (GADF/VODF), propagating and generating metadata (including the IVOA provenance) describing the data and the software, and finally finding and accessing FAIR data into the Virtual Observatory.\",\n      \"Scientific Impact\": \"The FAIRisation of Gammapy will be a major milestone for VHE astrophysics, making it the first open scientific library in this domain to generate FAIR astrophysical data. The FAIRGamma project will significantly benefit major European VHE observatories, such as CTA and KM3NeT, as well as their running pathfinders. It will also open new research avenues for physicists studying Extreme Environments, gamma/neutrino counterparts of gravitational waves and fundamental physics, such as dark matter, and multi-messenger astronomy, ensuring that data is reusable and comprehensible both by humans and machines.\",\n      \"Keywords\": \"Open research software, science analysis tool, Extreme environments, very-high-energy gamma rays, gravitational waves, fundamental physics, dark matter, multi-messenger astronomy, very-high-energy astrophysics, Cherenkov Telescope Array Observatory (CTAO)\",\n      \"Principal investigator\": {\n        \"Name\": \"Bruno  Khélifi\",\n        \"Organisation\": \"APC, CNRS/IN2P3\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIRification of IsoSeq Evidence-driven annotation of the biodiversity\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fairification-isoseq-evidence-driven-annotation-biodiversity\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"The ERGA (European Reference Genome Atlas) and ENA (European Nucleotide Archive) play vital roles in preserving genomic diversity, contributing to the Earth BioGenome Project (EBP), which aims to sequence all eukaryotic species. As the need to understand species' resilience to climate change intensifies, effective genome annotation becomes crucial. Our research group has developed SQANTI3, a quality control tool for long-read transcriptomic data, which filters inaccurate models to enhance genome annotations. The project team proposes leveraging SQANTI3 alongside the FAIR principles to create an open-source, standardised IsoSeq evidence-driven annotation pipeline. This initiative will refine genome annotation practices, support broader European and global genomic projects, and facilitate the characterisation of genomes for all species on Earth, addressing existing challenges in data integration and accuracy.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nThe project tackles the urgent need to understand species' resilience to climate change, impacting ecosystems and conservation efforts. A major challenge lies in improving genome annotation, essential for organising and preserving genomic data. While advancements in long-read sequencing (lrRNA-seq) support large-scale projects like the Earth BioGenome Project, integrating this data into annotation pipelines remains problematic due to noise and inaccuracies. Current guidelines lack specific instructions for utilising full-length RNA sequences, highlighting the need for better data integration and reuse in future annotation efforts.\",\n      \"Solution\": \"The project addresses a critical gap in the current genome annotation landscape for biodiversity by developing a thoroughly benchmarked pipeline for long-read-driven genome annotation within the ERGA pilot project. This pipeline will be designed for seamless integration with existing genome annotation efforts, significantly enhancing the quality of annotations. Additionally, it will effectively utilise existing IsoSeq data and promote the generation of new data to further improve annotation accuracy.\",\n      \"Scientific Impact\": \"This project is expected to significantly enhance the quality of both current and future genome annotations through the development of an improved annotation pipeline and the effective utilisation of existing sequencing data in open-access databases.\",\n      \"Keywords\": \"biodiversity, Earth BioGenome Project, genome annotation, genome characterisation, IsoSeq, IsoSeq evidence-driven annotation pipeline\",\n      \"Principal investigator\": {\n        \"Name\": \"Ana Conesa\",\n        \"Organisation\": \"Spanish National Research Council-Institute for Integrative Systems Biology\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIRMD - Disorder to Order: Streamlining Biomolecule Simulation Re-Use with FAIR NMRlipids database\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fairmd-disorder-order-streamlining-biomolecule-simulation-re-use-fair-nmrlipids-database\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"Disordered biomolecules, such as lipids and intrinsically disordered proteins (IDPs), are crucial to understanding cellular membranes and protein behaviours, yet standardised and accessible training data for these molecules is lacking. TheFAIRMD projectbuilds on the success of the NMRlipids Databank, which provides open access to molecular dynamics (MD) simulations of lipid bilayers. By expanding the database to include MD simulations of disordered proteins, and by improving its interoperability with existing public databases, FAIRMD aims to fill this gap, offering high-quality, FAIR data to support AI model development. This will enhance research across biophysics, drug design, and materials sciences.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned\\nThe rapid advancement of artificial intelligence (AI) in scientific research is significantly hampered by the lack of standardised, accessible training data. While databases like the Protein Data Bank (PDB) have revolutionised biological research, similar resources for disordered biomolecules, such as lipids and IDPs, are lacking. This creates a barrier for AI-driven research and limits advancements in areas, such as drug development and biomaterial design.\",\n      \"Solution\": \"FAIRMD aims to expand the NMRlipids Databank, a collaborative resource for MD simulations of lipid bilayers, to include disordered proteins, providing researchers with programmatic access to quality evaluated MD simulation models of disordered proteins. This advancement will enable the development of AI models to predict structural and dynamical properties of disordered biomolecular complexes from atomistic details. Bridging quality-evaluated MD simulations to other databases containing biological functions and biophysical properties of molecules will benefit a wide range of fields with high societal impact, such as design of sustainable materials, novel foods, and drugs.\",\n      \"Scientific Impact\": \"By addressing practical challenges in data distribution with open collaboration and overlay-databank approaches, the project promotes FAIR data sharing and supports the development of AI-based tools. The application of the open collaboration model, with shared authorship in the resulting publications, and the overlay-data bank concept promoted by FAIRMD are not limited to biophysical chemistry but bear potential advances across scientific domains.\",\n      \"Keywords\": \"NMRlipids database, disordered biomolecules, intrinsically disordered proteins, molecular dynamics, MD simulations, biophysical chemistry, molecular biology, biophysics, drug design, materials sciences\",\n      \"Principal investigator\": {\n        \"Name\": \"Markus Miettinen\",\n        \"Organisation\": \"University of Bergen\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FAIRY - FAIRification of YEASTRACT+ to Support Yeast-based Bioeconomy and Health\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fairy-fairification-yeastract-support-yeast-based-bioeconomy-and-health\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"TheFAIRYproject focuses on enhancing YEASTRACT+, a widely used open-access FAIRsharing Knowledge-base and repository developed by ELIXIR, that supports research on yeast transcriptional regulation. Yeasts are vital organisms in both the bioeconomy, e.g. through the industrial production of food, energy and commodities, and in health research, due to their role as pathogens and model organisms for studying genetics and disease. FAIRY aims to significantly advance scientific understanding of yeast regulatory networks and improve the platform by integrating advanced data analysis tools, ensuring compliance with FAIR principles. By relying on the Galaxy Europe platform for raw data curation, the project will link with the EOSC, increasing the database’s scientific utility and promoting data sharing across disciplines.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nDespite YEASTRACT+'s role as a leading resource for yeast transcriptional regulation, it currently lacks quantitative gene expression data. Moreover, many yeast species in the database remain poorly characterised, impeding the understanding of their regulatory mechanisms and broader bioeconomic applications. Additionally, many yeast species remain understudied, hindering our understanding of their regulatory mechanisms. The FAIRY project aims to tackle this challenge by incorporating machine learning models trained on cross-species regulatory data, while ensuring that the data remains FAIR.\",\n      \"Solution\": \"FAIRY will develop and integrate tools for quantitative transcriptomics analysis within YEASTRACT+, allowing researchers to identify regulatory interactions and assess their quantitative impact on gene expression levels. The project will rely on the Galaxy Europe platform for raw data curation, in particular large-scale RNA-seq data processing, enhancing the efficiency and scalability of quantitative transcriptomics analysis. Additionally, machine learning models trained on cross-species regulatory data will be employed to predict new transcriptional regulatory rules for transcription factor activity and evolution in poorly-characterised yeasts. To align with the FAIR principles, the project will develop web services, or APIs, enabling seamless data exchange between YEASTRACT+ and major biological databases (such as GenBank, KEGG, MetaCyc, SGD/CGD, and FungiDB), thus enhancing data accessibility, interoperability, and reusability. Researchers will be able to effortlessly access and combine YEASTRACT+ data with information from these complementary resources, fostering a more comprehensive understanding of yeast regulatory networks.\",\n      \"Scientific Impact\": \"By combining data on how genes are regulated and how yeast cells process nutrients, the FAIRY project will improve our understanding of how transcription factors control gene expression and the consequent phenotypic outcome. It will also allow scientists to predict how these regulatory events affect the cell’s metabolism, which is important for optimising yeast-based production processes towards a circular bioeconomy. The project will help researchers identify which genes are essential for the cell's survival, not just in metabolism but also in how genes are regulated. This will expand current tools used for identifying potential drug targets. Typically, each user has to perform these calculations themselves. The FAIRY project will pre-calculate these outcomes for a range of yeast species (currently limited to S. cerevisiae and C. albicans), storing them in a central database. By offering ready-made predictions, the project will make it easier for scientists to reuse the data, speeding up research in designing better yeast strains for industry and finding new antifungal drug targets.\",\n      \"Keywords\": \"yeast regulatory networks, transcriptional regulatory mechanism, cell factory design, drug development.\",\n      \"Principal investigator\": {\n        \"Name\": \"Pedro Tiago Monteiro\",\n        \"Organisation\": \"CS Department of IST- Universidade de Lisboa and INESC- ID Lisboa\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FASCA - Facilitating Scholarly Communication Analysis through GoTriple Pipeline\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fasca-facilitating-scholarly-communication-analysis-through-gotriple-pipeline\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"FASCA builds on theGoTripleplatform, a multilingual discovery service developed as an EOSC service by the TRIPLE project led by OPERAS* Research Infrastructure (RI) and catering specifically to the needs of researchers in the domains of the Social Sciences and Humanities (SSH). GoTriple provides SSH researchers with access to over 15 million publications and datasets, promoting international collaboration and Open Science. TheFASCAproject enhances this platform by integrating data science tools to support comprehensive analysis of scholarly communication, with a specific focus on linking research to the Sustainable Development Goals (SDGs). Through two pilot studies, the project aims to deliver FAIR-compliant research outputs, including publications, data, and code, while establishing a sustainable support GoTriple-based service for data-driven SSH research.\\n*OPERAS is the RI supporting open scholarly communication in SSH across the European Research Area. Its mission is to coordinate and federate both technical and non-technical resources in Europe. OPERAS is an ESFRI on the path to become an ERIC in 2027. It is part of the SSH Open Cluster with its representative in the SSHOC Governance Board.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nThe need for accessible and comprehensive research tools in the SSH has become increasingly evident as scholars struggle to navigate a vast array of multilingual resources. Traditional research methods often fall short of leveraging the full potential of large datasets and diverse research outputs. Moreover, there is a growing demand for making scholarly communication more aligned with global challenges such as the SDGs. Without efficient tools for data-driven research, the SSH community faces obstacles in fostering cross-disciplinary collaborations and ensuring Open Science practices.\",\n      \"Solution\": \"FASCA aims to establish a data science support service for multilingual data-driven research with data science assistance, by implementing a pipeline for comprehensive research support through tailored, personalised services. It will run two pilot research projects dedicated to investigating the relationship between scholarly communication and SDGs. Selected teams will get access to support from the Data Science team, publish their manuscript/preprint, data and code in a FAIR-compliant way. The pilots will be used to test and develop a GoTriple Data Science support service that will be offered on top of the GoTriple discovery platform.\\nIt will build a Data Science GoTriple pipeline that facilitates the use of GoTriple data (publications, projects, profiles, datasets) with the Data Science Universal Toolbox (JupyterLab, Neo4j) and improved GoTriple functionalities (GoTriple API, Chatbot, Pundit). It will enable users to publish research results (on GitHub, Zenodo, BinderHub) in a FAIR-compliant way by linking the results with datasets, as well as to use  software and run research pilots. Their results will be presented through the dedicated GoTriple’s Project page, as a manuscript/preprint, research data and code to form a FAIRified research object which connects publication, research data and research code.\",\n      \"Scientific Impact\": \"The project is expected to enhance the EOSC service GoTriple by developing and providing a Data Science support service, which will allow the SSH community to build cross-disciplinary collaborations between the SSH Open Cluster and other EOSC Thematic Clusters thanks to shared background of data-driven methodologies. The results of the 2 data-driven research pilots from SSH researchers eager to obtain data science support to investigate relationships between scholarly communication and SDGs, will be published in a FAIR-compliant way (manuscript/preprint, code and data). The project’s FAIR-compliant outputs will serve as models for future studies, promoting best practices in Open Science.\",\n      \"Keywords\": \"research query support, data retrieval, metadata, semantic annotation, data analysis, data visualisation, GoTriple platform\",\n      \"Principal investigator\": {\n        \"Name\": \"Sy Holsinger\",\n        \"Organisation\": \"OPERAS\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FIESTA - FAIR image analysis across sciences\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fair-image-analysis-across-sciences\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, ESCAPE - Astronomy, Nuclear and Particle Physics, LS RI - Life Sciences\",\n      \"Summary\": \"Unlocking the potential of imaging across scientific fields, theFAIR Image Analysis Across Sciencesproject breaks down barriers between disciplines by developing robust, reusable, and interoperable workflows. It will utilise public data archives, such as the BioImage Archive and Copernicus data collections, along with advanced AI models, ensuring compliance with FAIR principles throughout the research process, and enhancing knowledge transfer across disciplines. The project empowers researchers to enhance data accessibility, foster interdisciplinary collaboration, and drive innovative solutions for complex challenges in bioimaging, astrophysics, and environmental sciences.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation, Main RI concerned, Crossdomain/Cross-RI\\nImaging techniques have become increasingly popular across scientific fields due to their ability to provide rich and comprehensive information. These images, image series, and videos are invaluable resources, offering researchers a wealth of information to explore and analyse. Despite the widespread use of imaging, the practice of extracting quantitative information from image data remains fragmented across scientific communities. Scientists often work in isolated silos, resulting in limited cross-talk and collaboration. This isolation can lead to redundant work and inefficient resource utilisation, as experts in different domains may duplicate efforts unknowingly. The rise of AI and machine learning further complicates this landscape, with access to the necessary tools and computational resources becoming a significant bottleneck for researchers.\\nThe project aims to tackle these challenges by creating image analysis workflows that can be shared and reused across multiple scientific domains, such as bioimaging, environmental sciences, and astrophysics.\",\n      \"Solution\": \"The \\\"FAIR Image Analysis Across Sciences\\\" project addresses these challenges by developing reusable image analysis workflows that can be shared across disciplines such as bioimaging, environmental sciences, and astrophysics. By leveraging FAIR-enabling resources like Galaxy and WorkflowHub, the project aims to democratise access to advanced tools and AI models, ensuring that both established researchers and citizen scientists can benefit from these resources globally, including in the Global South.\\nThe project will also engage with various RIs and Science Clusters to facilitate the long-term sustainability of its outcomes.\",\n      \"Scientific Impact\": \"By promoting domain-agnostic workflows, the project aims to boost scientific research and innovation, increase the adoption of workflow management systems, and enhance data compatibility across domains. The emphasis on interdisciplinary collaboration will empower researchers to share methodologies and foster partnerships between academia and industry, ultimately driving the commercialization of research outcomes and enhancing societal impact.\",\n      \"Keywords\": \"FAIR image analysis, imaging, imaging techniques, reusable image analysis workflows\",\n      \"Principal investigator\": {\n        \"Name\": \"Antje Keppler and Beatriz Serrano-Solano\",\n        \"Organisation\": \"Euro-BioImaging ERIC Bio-Hub\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Findable Big Data from Various Material Characterisation Techniques\",\n    \"Project_url\": \"https://oscars-project.eu/projects/findable-big-data-various-material-characterisation-techniques-0\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"Unlocking the potential of experimental data, the projectFindable Big Data from Various Material Characterisation Techniquesenhances the findability of experimental datasets across photon and neutron (PaN) facilities and materials science laboratories through a foundational classification of experimental techniques. The integration of the PaNET ontology with the NeXus Ontology enables the establishment of a foundational classification of PaN experiment techniques, fostering semantic coherence across data catalogues and records within major European facilities and world-wide.\\nBy bridging semantic gaps and establishing coherent classification systems towards ontology harmonisation, the project paves the way for seamless data discovery and analysis, empowering researchers to advance their Open Science initiatives across diverse materials characterization techniques.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation, Cross-domain/Cross-RI\\nIn the realm of Open Science, the findability of experimental datasets presents significant challenges, particularly at PaN facilities, where data is abundant yet often dispersed across diverse platforms and repositories. The complexity of finding relevant data highlights the need for improved ontology harmonisation to optimise the data discovery process.\",\n      \"Solution\": \"To improve the semantic clarity of dataset discovery, the project aims to extend and integrate the PaNET ontology into the research data management platformNOMAD, and to the metadata ecosystem of institutions, such asESRF, orHZB. This will enhance FAIRification, by ensuring comprehensive descriptions of experimental endeavours and facilitating precise identification of the techniques utilised in each instance. This integration will improve the discoverability of datasets by providing comprehensive descriptions of experimental techniques, facilitating semantic interoperability, and establishing best practices for data management.\\nOur goals will be achieved by:\",\n      \"Scientific Impact\": \"The integration of the enhanced PaNET ontology into EOSC services, such as the PaN data portals and the NOMAD service developed by FAIRmat, represents a significant and exemplary milestone in advancing the accessibility and discoverability of experimental data. It enables researchers to easily find relevant experiments across different platforms and facilities.\\nThe institutions involved in the project, from the German NFDI projectsFAIRmatandDAPHNE4NFDIto  the EFRI landmark ESRF, will contribute to the integration of PaNET into their respective search platforms, thus augmenting search capabilities and facilitating streamlined data discovery processes.\\nThis enhanced data findability lays the groundwork for comprehensive analysis of experimental big data, also leveraging tools, such as theNOMAD's Artificial Intelligence Toolkit.\",\n      \"Keywords\": \"big data, material characterisation, spectroscopy, PaNET ontology, data discovery, data findability, data analysis\",\n      \"Principal investigator\": {\n        \"Name\": \"Claudia Draxl\",\n        \"Organisation\": \"Institut für Physik - Humboldt-Universität zu Berlin\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"FSD - Federation of Solar Data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/fsd-federation-solar-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The FSD project aims to enhance FAIR data archiving, curation and sharing, of solar data of the SOLARNET Virtual Observatory (SVO), other astrophysical databases, and the ESCAPE-ESAP science platform. It will do so by leveraging standards established by the SOLARNET project and the International Virtual Observatory Alliance (IVOA). This work will not only improve seamless access to existing data through Virtual-Observatory (VO) tools, but will also support the future operations of the European Solar Telescope (EST), Europe’s largest solar telescope.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nThe sheer volume and diversity of solar data, generated by a wide array of observational instruments and computational models, present significant challenges in terms of accessibility, interoperability, and reuse. Without adherence to widely accepted data standards, such as those of SOLARNET and IVOA, much of this data remains siloed and difficult to integrate into broader astrophysical and heliophysics research, limiting its potential impact.\",\n      \"Solution\": \"FSD will ensure that existing solar datasets and archives are made FAIR-compliant using advanced tools for data archiving, curation and sharing. SOLARNET and IVOA standards will be applied, and suitable network solutions (such as EPN-TAP) will be adopted, facilitating the integration of solar data into a shared framework accessible to the global astrophysical community.\\nThe SVO web client will allow users to easily search for and find data across datasets, and in the Heliophysics Event Knowledge Base (HEK). In addition, web services and clients will be provided for searching the resulting metadata, reviewing results, and efficiently accessing the data.\",\n      \"Scientific Impact\": \"The project will improve seamless access to solar observations and simulations data, benefiting not only the astrophysical community, but also scientists in related fields, such as geophysics, space physics, and meteorology, as well as in applied research, such as solar-energy technologies, space technology, and more. By making data management processes FAIR compliant, FSD will ensure that solar datasets are available for complex searches, script-based analyses, and cross-disciplinary research, enabling scientific breakthroughs in understanding solar phenomena and their broader implications.\\nThe way metadata is collected will be optimised, achieving a milestone for the development of future EST instrumentation, while testing  technical aspects of the data-lakes that will be used for archiving and sharing scientific data.\",\n      \"Keywords\": \"solar data, SOLARNET Virtual Observatory, IVOA standards, astrophysics, geophysics, space physics, meteorology, solar-energy technologies, space technology\",\n      \"Principal investigator\": {\n        \"Name\": \"Pavol Schwartz\",\n        \"Organisation\": \"Astronomical Institute of Slovak Academy of Sciences - AI SAS\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"GASPS - Gaia All-Sky Parameters for Stars Service\",\n    \"Project_url\": \"https://oscars-project.eu/projects/gasps-gaia-all-sky-parameters-stars-service\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"Understanding the properties of stars, such as their temperature and brightness, is crucial for studying stellar evolution, but it requires analysing and comparing data from measurements of brightness at all wavelengths, as well as measuring the distance to the star, correcting all such measurements, and finally comparing data to the best models of stars. TheGASPSproject aims to do this for the entire sky. Leveraging the PySSED tool -  the Python Stellar Spectral Energy Distribution package - GASPS integrates and analyses data from multiple catalogues - including that of the European Space Agency’s Gaia satellite, with datasets of over a billion stars - ensuring the data is FAIR. The final aim is to make the data easily accessible to researchers, educators, and the general public, and the software freely available and compatible with other astronomy software for further data reuse.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation\\nThe measurement of fundamental stellar parameters, such as temperature, brightness, and interaction with the environment is vital for understanding stellar life cycles. However, integrating and cross-referencing star data across vast catalogues, such as Gaia, which includes over a billion stars, poses a significant challenge, leading to a lack of availability and FAIRness in these key data. Complex corrections are required to account for distance, dust, and gas interference, demanding advanced tools to streamline these analyses across such massive datasets.\",\n      \"Solution\": \"GASPS aims to use the PySSED package to automatically combine and analyse stellar data from multiple sources. PySSED automatically brings together measurements of stars from different catalogues and estimates the star's temperature and inherent brightness, accounting for both the distance to the star and how its light is affected by dust and gas between the star and the Earth. The project will make PySSED compliant with the open-data standards of the Virtual Observatory (VO), publishing the updated version available on the EXPLORE platform, with open access to both software and data, and will apply these advanced calculations to the entire Gaia DR3 all-sky survey of 1,467,744,818 stars, offering public access to stellar parameters, such as temperature and UV/IR excess. Additionally, it will provide visualisation tools to explore and use this immense dataset.\",\n      \"Scientific Impact\": \"GASPS is expected to be an Open Science service, available as a custom web-based tool deployed on a science platform or via API for integration into Python scripts. Leveraging results of previous Horizon 2020 projects, notably ESCAPE and EXPLORE, GASPS will deliver the largest, internally coherent high-fidelity catalogue of SED derived stellar parameters, and will provide the full supporting code behind it, as well as new tools. It will make existing catalogues Findable and Accessible in a transparent way, and will support use cases across ESCAPE’s RIs and other facilities.\\nGASPS will boost the impact of PySSED by aligning it with VO standards to improve interoperability. This will allow easier integration into VO workflows, including linking PySSED with other VO tools within the EOSC umbrella (e.g., TopCat). The advanced tools and data products foreseen in the frame of GASPS will significantly extend the Gaia DR3 catalogue with self-consistent, reliable data on physical parameters of stars, cross-referencing data from several ESFRI facilities, including ESO and other ESA satellite missions, and ongoing/future large-scale surveys, such as those of Euclid and the Vera Rubin Observatory. The Open Science nature of GASPS ensures that the astronomical community, educators, and the public can fully engage with this rich dataset, fostering new discoveries and collaborations.\",\n      \"Keywords\": \"PySSED, Python Stellar Spectral Energy Distribution package, Gaia catalogue, star properties, SED - Spectral Energy Distribution\",\n      \"Principal investigator\": {\n        \"Name\": \"Iain McDonald\",\n        \"Organisation\": \"University of Manchester\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"GOYAS - Geospatial Open-science Yielding ApplicationS: innovative and FAIR remote-sensing data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/goyas-geospatial-open-science-yielding-applications-innovative-and-fair-remote-sensing\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"The use of data generated through remote sensing (satellites, drones, etc.) has progressively increased since the 1970s. Its usefulness has been demonstrated in the global monitoring of Earth Systems, which is essential in a climate change scenario. This wealth of data has proven invaluable for environmental assessments, resource management, and scientific research. However, simply providing access to these datasets is not enough. To ensure reproducibility and reusability, data must be accompanied by comprehensive metadata and documentation, and has to adhere to the FAIR principles. TheGOYASproject aims to establish a robust data management system that supports the lifecycle of experimental and discontinuous remote-sensing products, facilitating their integration into the EOSC to enhance their accessibility and reproducibility across various research domains.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nRemote sensing data must be accompanied by adequate metadata and documentation to make it effectively FAIR throughout its lifecycle. It is therefore necessary to establish mechanisms that facilitate, support, or automate the production of FAIR data. Research in Earth and Environmental Sciences using remote sensing often requires the definition of new data products based on new methods, or algorithms or when different sources are combined. These data products, generally experimental but with high added value, provide relevant and novel information for academic, economic, and management applications. Initiatives like Copernicus, despite their extensive data distribution capabilities, demand rigorous standards for data production and often fall short in accommodating novel methods, or alternative preprocessing techniques. This gap in the landscape of data management inhibits the potential for broader application across various domains, including academic research, environmental monitoring, and resource management.\",\n      \"Solution\": \"GOYAS will establish a FAIR data system to manage the entire lifecycle of experimental or discontinuous remote-sensing products. This includes preprocessing original data sources to generate derived products, publishing the data and metadata in accordance with FAIR principles, and connecting these outputs to EOSC resources to facilitate analysis, interoperability, and reuse. The initial data to be published, due to the relevance and novelty of the methods used, include:\",\n      \"Scientific Impact\": \"The scientific impact of the GOYAS project lies in its ability to fill critical gaps in the accessibility and reusability of high-value remote sensing data, addressing the needs of diverse stakeholders across academia, industry, and government. Many remote sensing products, such as inundation maps and water quality data, are valuable but lack visibility, access, or integration into existing data-sharing frameworks, and are thus unavailable to users despite their potential importance for research and environmental management. GOYAS will publish these products in a FAIR-compliant manner, enhancing ecosystem monitoring and research. This will benefit diverse scientific communities, from oceanography and hydrology, to biodiversity, ecosystem research, or marine biology. In addition, it will support the  efforts of research infrastructures in the Earth and environmental sciences, such as those in the ENVRI Science Cluster, as well as environment monitoring initiatives, such as LTER.\",\n      \"Keywords\": \"remote-sensing data, remote-sensing products, Inundation maps, hydroperiod maps, water turbidity maps, AI-driven water quality assessments, oceanographic data products, Satellite-derived bathymetry\",\n      \"Principal investigator\": {\n        \"Name\": \"Fernando Aguilar\",\n        \"Organisation\": \"IFCA-CSIC - Spanish National Research Council\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"HarDIS - Harmony in the Democratic Ideological Space\",\n    \"Project_url\": \"https://oscars-project.eu/projects/hardis-harmony-democratic-ideological-space\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"In today’s democracies, the relationship between political parties and voters plays a critical role in shaping political outcomes. However, research on this interaction, including party responsiveness to voter preferences, is often hampered by fragmented and non-FAIR datasets, which are not maintained and harmonised in the long-term. TheHarDIS project– Harmony in the Democratic Ideological Space – aims to create a community-based, sustainable, scalable, and FAIR linkage key database. HarDIS will enhance the integration of new party and voter data into theParty Factsplatform. It will also harmonise data, streamlining access for researchers and ensuring long-term collaboration in democracy studies. The project also contributes to the long-term sustainability of such databases by aligning with the European Research Infrastructure (RI) Monitoring Electoral Democracy (MEDem).\",\n      \"Challenge\": \"Open Science service\\nIn contemporary democracies, understanding the relationship between political parties and voters is paramount, but long-term research on this interaction is hindered by the fragmented and short-lived nature of many data harmonisation projects. While initiatives such as Party Facts, True European Voter, OnBound, QuestionLink, or the Survey Data Recycling Project have made significant strides, their sustainability is often compromised by the lack of ongoing funding and institutional support, leading to the deterioration of valuable datasets over time.\",\n      \"Solution\": \"HarDIS main objective is to make linking and harmonisation efforts FAIR and sustainable for the research community as a whole. To do so, HarDIS will create a scalable and community-driven framework for sustainable linkage key databases. The project will reach out to providers of other community-based data projects and to Party Facts’ contributors  to systematically analyse their experiences and needs, improve Party Facts' collaboration tool and provide a general framework for community-based linkage key databases. The project will also integrate new data into Party Facts, extending its scope and depth substantially. Data on the position of parties included in Party Facts and their voters will be harmonised.\",\n      \"Scientific Impact\": \"HarDIS will significantly reduce the burden of data preparation for democracy researchers, allowing them to focus on their core analyses. Its scalable and sustainable model for linkage key databases will enhance long-term collaboration and data reuse in the Social Sciences. Additionally, HarDIS’ harmonisation of political party and voter data will create a blueprint for similar efforts in other research fields, promoting broader harmonisation and advancing the study of political representation, party competition, and public opinion. The project also plans to integrate HarDIS' framework for collaborative linkage key databases into the MEDem RI, and in the SSH Open Marketplace.\",\n      \"Keywords\": \"Democracy research, data harmonisation, Party Facts, MEDem\",\n      \"Principal investigator\": {\n        \"Name\": \"Lea Kaftan\",\n        \"Organisation\": \"GESIS - Leibniz-Institute for the Social Sciences\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"HEFTIE - Handling Enormous Files from Tomographic Imaging Experiments\",\n    \"Project_url\": \"https://oscars-project.eu/projects/heftie-handling-enormous-files-tomographic-imaging-experiments\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences, PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"X-ray tomography is a technique used across diverse scientific fields, such as material sciences, geosciences, palaeontology, and life sciences. With current experiments producing enormous datasets, the development of ‘chunked’ file formats - which break down massive files into smaller parts - has become essential for data processing. TheHEFTIE projectaims to significantly improve tools and educational resources for handling such datasets by creating a comprehensive digital textbook and new visualisation software. These advancements will accelerate the analysis of 3D organ scans and unlock discoveries in the life sciences, benefiting also other fields, such as neuroscience, archaeology, and the earth sciences.\",\n      \"Challenge\": \"Open Science Service, Industry cooperation, Main RI concerned\\nX-ray tomography produces extremely large datasets, often exceeding 1 TB, which are too large to fit into standard computer memory. This has led to the adoption of 'chunked' file formats, enabling scientists to process these datasets in manageable portions. However, many researchers lack the tools and training to work with such massive data effectively, which slows down analysis and limits the scientific discoveries that could be made from this data.\",\n      \"Solution\": \"HEFTIE aims to develop a comprehensive digital textbook and new software tools for working with chunked 3D imaging datasets. The textbook will provide clear guidance on setting up and running data analysis and visualisation pipelines, while the new tools and a visualisation software will make it easier to use these datasets. Specifically, and in connection with theHuman Organ Atlas project, the project team will use the textbook and the new tools to analyse scans of human organs to understand how the human body works in health and disease, using  a technique allowing the scan of 3D images of whole human organs down to the resolution of individual cells. By leveraging open-source technologies, such as the Zarr data format, Python, and the WEBKNOSSOS software, HEFTIE ensures long-term accessibility and community engagement.\",\n      \"Scientific Impact\": \"By adhering to FAIR data principles and integrating with the EOSC, HEFTIE will expand access to terabyte-scale data and foster open science collaboration across Europe. The digital textbook and the tools will be designed from the start to run on theVirtual Infrastructure for Scientific Analysis - VISA, the Virtual Research Environment (VRE) cloud computing platform developed as part of PaNOSC, to ensure accessibility to a wide range of users across the PaNOSC cluster.\\nImproved training and tools for working with the data will allow the project team and other researchers around the world, to speed up data analysis and make new discoveries from unused data. Beyond its direct applications in life sciences, the tools and training materials developed will be applicable to other research fields, such as neuroscience, archaeology, and the earth sciences.\",\n      \"Keywords\": \"X-ray tomography, Human Organ Atlas, VISA - Virtual Infrastructure for Scientific Analysis, 3D organ scans, 3D imaging datasets, chunked data\",\n      \"Principal investigator\": {\n        \"Name\": \"David Stansby\",\n        \"Organisation\": \"University College London\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"HiMAGNETOS: High Magnetic Field Open Science\",\n    \"Project_url\": \"https://oscars-project.eu/projects/himagnetos-high-magnetic-field-open-science\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, ESCAPE - Astronomy, Nuclear and Particle Physics, LS RI - Life Sciences, PANOSC - Photon and Neutron Science, Other\",\n      \"Summary\": \"The HiMAGNETOS project aims to advance the development of standards and tools to reduce the obstacles for researchers to make their experimental data FAIR. It will do so by enhancing open access to high magnetic field research data generated at the Laboratoire National des Champs Magnétiques Intenses (LNCMI) within the EOSC framework. The project strives to establish comprehensive standards, develop specialised tools, and create a centralised data repository that will facilitate the production of FAIR data. By streamlining these processes, the project aims to reduce barriers for researchers, ensuring that experimental data can be easily shared, accessed, and reused, thus promoting a more collaborative research environment.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nHigh magnetic field research plays a vital role in advancing various scientific disciplines, yet access to related data is often restricted. While some open data initiatives, such as the NHMFL MagLab in the USA, exist, they primarily promote generic frameworks that do not adequately address the specific requirements of high magnetic field experiments. This lack of targeted support creates significant barriers for researchers who wish to share their findings or access critical experimental data. Consequently, the potential for collaboration and innovation in this field is stifled, highlighting the urgent need for a structured approach to improve data accessibility.\",\n      \"Solution\": \"HiMAGNETOS will implement a common data management plan derived from insights gained in the ISABEL H2020 project, which focuses on improving the sustainability of the EMFL. The project will adapt existing frameworks, such as the DataCite Metadata Schema and Nexus Format, to meet the unique challenges posed by high magnetic field conditions. A central data repository will be established for both in-house and external users, complete with defined curation policies and uploading procedures. Additionally, the project will link research data to the original experimental proposals, enhancing data quality and usability by providing context, such as magnetic field homogeneity and other facility-related parameters. This comprehensive approach aims to create a robust data ecosystem that supports diverse research needs.\\nMoreover, the LNCMI has developed, over the years, many data reduction software for each kind of high field experimental techniques. These software packages allow reading the raw data from the acquisition systems, adjust reduction parameters and save the reduced data and some metadata in a user-friendly format. To ensure data reuse and interoperability, these software packages will be made open source or available as an online notebook.\",\n      \"Scientific Impact\": \"The advancements made through the HiMAGNETOS project will significantly enhance collaboration and reproducibility in high magnetic field research. By making data and tools openly available, the project will not only improve the accessibility of high-quality experimental data but also set a precedent for future initiatives within the European Magnetic Field Laboratory (EMFL) community. HiMAGNETOS is envisioned as a pilot phase toward the broader integration of EMFL services into the EOSC, fostering a culture of Open Science in the field. Ultimately, HiMAGNETOS will empower researchers to build upon existing data, leading to innovative discoveries and a deeper understanding of high magnetic field phenomena.\",\n      \"Keywords\": \"High magnetic field research, high magnetic field research data, centralised data repository, DataCite Metadata Schema, Nexus Format, open-source data reduction software\",\n      \"Principal investigator\": {\n        \"Name\": \"Christiane Warth-Martin and Nicolas Bruyant\",\n        \"Organisation\": \"CNRS\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"HOMEROS - Harmonising Observations from Multi-hazard Environments in Research for Open Science\",\n    \"Project_url\": \"https://oscars-project.eu/projects/homeros-harmonising-observations-multi-hazard-environments-research-open-science\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"In today's interconnected world, communities face a diverse range of hazards, including natural disasters like earthquakes, floods, and landslides, as well as human-induced events. To address these challenges, theHOMEROSproject aims to strengthen multi-hazard assessment methodologies in seismology, geodesy, and geology. Leveraging technical solutions suited to the big data era, HOMEROS represents a significant advancement in standardising research processes, promoting collaboration, and enhancing understanding and prediction capabilities for natural hazards through Open Science practices. Focusing on high-risk areas in Greece with significant seismic activity, strong earthquakes, floods, and landslides, HOMEROS will compile earthquake catalogues and assess ground deformation products to provide a comprehensive evaluation of seismic hazards. Additionally, it will enhance the understanding of flood and landslide risks, vital for effective mitigation strategies.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nCommunities worldwide face a wide range of natural and human-induced hazards, which require more robust, data-driven approaches to assessment and mitigation. In regions like Greece, where seismic activity, floods, and landslides pose significant threats, existing observation systems often lack standardisation when combining interoperable services from diverse resources related to essential geohazard variables, hindering effective multi-hazard assessments. The HOMEROS project addresses these issues by developing methodologies that harmonise data and services, aiming for a more unified and comprehensive hazard assessment strategy.\",\n      \"Solution\": \"The project integrates interoperable services from various sources, including pan-European data services (within the ENVRI Science Cluster), local resources (at the national level), and the EOSC, to standardise research processes. By comparing two geographical areas in Greece as case studies, each representing multi-hazard environments, HOMEROS aims to identify network and data needs crucial for improving earth observation and hazard assessment capabilities. Integration with the ENVRI Community and the EOSC will enable HOMEROS to utilise Open Science environments effectively, facilitating multidisciplinary data exchangeability and showcasing the value of integrating e-science tools in research. Moreover, the project will address data challenges, develop scientific workflows, and provide open access datasets and products essential for comprehensive multi-hazard assessments, including seismic data, ground deformation data, and information on landslides and floods.\",\n      \"Scientific Impact\": \"HOMEROS will establish links between institutions, research infrastructures (RIs) and projects to disseminate results and promote the use of RI services for scientific purposes. Through the promotion of Open Science practices, it aims to democratise access to knowledge, encourage multi-disciplinary collaboration, and enhance community resilience against interconnected hazards. By offering accessible ecosystems for data exchange following the FAIR principles, HOMEROS will advance scientific knowledge in multi-hazard assessment. Its approach, characterised by transparent data sharing, collaborative modelling, and active community engagement, aims to catalyse significant changes in disaster preparedness, response strategies, and recovery processes. Ultimately, the project will empower communities worldwide to build resilience and adapt to the ever-changing risk landscape. Such open and collaborative efforts can lead a global shift towards more resilient societies.\",\n      \"Keywords\": \"data sharing, collaborative modelling, disaster preparedness, hazard assessment, earthquakes, ground movement, landslides, floods\",\n      \"Principal investigator\": {\n        \"Name\": \"Eleftheria  Papadimitriou\",\n        \"Organisation\": \"Aristotle University of Thessaloniki\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Implementing FAIRness in structure-based drug design through Fragalysis Cloud\",\n    \"Project_url\": \"https://oscars-project.eu/projects/implementing-fairness-structure-based-drug-design-through-fragalysis-cloud\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"The project Implementing FAIRness in structure-based drug design through Fragalysis Cloud aims to enhance the transparency and accessibility of structure-based drug design (SBDD) data by making it fully open and compliant with FAIR principles. Leveraging Diamond’s XChem service and the Fragalysis Cloud platform, the project will streamline data deposition and establish a first-of-its-kind tool for sharing, exploring, and evolving SBDD experiments. This will significantly lower the barrier to open data sharing, advancing drug discovery methodologies.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nThere is a strong consensus in the field of medicinal chemistry, that existing datasets for exploring and improving structure-based drug design (SBDD) are highly inadequate. The state-of-the-art of data availability has evolved into the separate repositories of the Protein Data Bank (accessed through PDBe in ELIXIR), and the ChEMBL database. These are individually well-developed and highly FAIR, but only for very specialised informaticians, and with necessarily strictly constrained ontologies. Moreover, data is often fragmented across various tools and hidden in unstructured formats, making it difficult for researchers to access and build upon existing datasets\",\n      \"Solution\": \"The project aims to enhance the Fragalysis Cloud platform to facilitate open and FAIR-compliant sharing of SBDD data. Fragalysis Cloud is a collaboration platform developed at Diamond’s XChem facility, used for curating, sharing and disseminating views of 3D data, and implementing best-practice medicinal chemistry algorithms and workflows for progressing results from fragment screens through early DMTA cycles. The project will map out clearly how Fragalysis can be a platform for collectively annotating and depositing data to PDBe and ChEMBL. Furthermore, it will make details of annotation workflows and computations accessible from those repositories, and generalised workflows will be made available to WorkflowHub.\\nBy integrating data from 50–150 experiments, the project will create an accessible mechanism for depositing and querying large-scale datasets. Collaborating with key INSTRUCT (XChem) and ELIXIR services (PDBe and ChEMBL), which host the key data categories of the field (3D structures of protein-compound complexes and assay results of the compounds, including biochemical activity, biophysical affinity and ADME measurements), the project will enable researchers to explore SBDD projects with comprehensive data on context, provenance, design rationales and inspection of computational analyses.\",\n      \"Scientific Impact\": \"The project will revolutionise the accessibility and reuse of SBDD data, providing an innovative tool for examining collective datasets in a FAIR manner. By making previously isolated data (and metadata) available for querying across projects, the project will improve transparency and reproducibility of experiments, while allowing  to advance methodology and algorithms in the field. Researchers and developers will benefit from the insights gained from shared workflows, while contributing significantly to the FAIR data landscape in the life sciences.\\nThe scientific relevance of the tools will be stress-tested by applying the new tools to real-world XChem data, both historic and new, and training the user community in their use, thereby greatly expanding the quantity and quality of SBDD data in the public domain.\",\n      \"Keywords\": \"Structure-based drug design data, FAIR data,  Fragalysis cloud, medicinal chemistry, biomedical sciences\",\n      \"Principal investigator\": {\n        \"Name\": \"Warren Thompson\",\n        \"Organisation\": \"Diamond Light Source\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"ioChem-BD: Open-Data for Computational Chemistry and Materials Science\",\n    \"Project_url\": \"https://oscars-project.eu/projects/iochem-bd-open-data-computational-chemistry-and-materials-science\",\n    \"Project_content\": {\n      \"Science cluster\": \"Other\",\n      \"Summary\": \"TheOpen-Data for Computational Chemistry and Materials Science: ioChem-BDproject emerges from a growing recognition of the importance of Open Science in accelerating research and innovation in the fields of computational chemistry and materials science. With the rapid advancement of computational methods generating vast amounts of data, there is a critical need for effective data management and sharing practices. ioChem-BD aims to address this gap by fostering good practices in research data management and promoting the generation of FAIR data. By transforming digital repositories and introducing innovative services, the project seeks to create an engaging platform that not only facilitates data sharing but also cultivates a culture of transparency and reproducibility in research, ultimately accelerating chemical discoveries and enhancing collaboration across the scientific community.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned\\nIn the realms of computational chemistry and materials science, there is a pressing need to address the fragmented landscape of research data. Current practices in research data management often lead to inefficiencies in data sharing, limiting researchers’ ability to access and utilise valuable datasets. Moreover, the lack of adherence to FAIR data principles hinders the reproducibility of scientific findings and inhibits collaborative efforts. All this makes the need for a more integrated and adaptable data management solution increasingly evident.\",\n      \"Solution\": \"The ioChem-BD project provides a comprehensive approach to overcoming these challenges by developing a robust platform that promotes FAIR data practices. Key objectives include assessing researchers’ real needs for FAIR data, developing software tools fostering user proficiency and data quality control features, and creating software tools to contribute to the adoption of FAIR data principles in the computational chemistry and materials science community. By integrating Semantics Data Models (SDM) into the repository, ioChem-BD ensures that data from various origins can be analysed collectively, enhancing the potential for insightful research outcomes. Additionally, the project will host workshops, talks and meetings with the community, to further improve data FAIRness and the platforms.\",\n      \"Scientific Impact\": \"The impact of ioChem-BD extends across the scientific community, enhancing researchers' capabilities in data-driven research. By providing tools that support the analysis, transformation, and comparison of data, the platform aims to empower users to generate high-quality, representative datasets that can confirm both new and established hypotheses. Furthermore, the project envisions transforming the ioChem-BD platform into a virtual research environment (VRE) that promotes collaboration and innovation. Through this project, researchers will be equipped to produce impactful research that advances knowledge in computational chemistry and materials science, thereby contributing to the broader goals of Open Science and collaboration in the scientific community.\",\n      \"Keywords\": \"computational chemistry, materials sciences, Semantics Data Models, virtual research environment, research data management\",\n      \"Principal investigator\": {\n        \"Name\": \"Carles Bo\",\n        \"Organisation\": \"Institut Català d'Investigació Química\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"LabID-PROV: Tracking and Sharing Data Provenance with RO-Crate in Lab Integrated Data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/labid-prov-tracking-and-sharing-data-provenance-ro-crate-lab-integrated-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"FAIR data management and the ability to track (meta)data from sample collection to the final computed (or derived) datasets have become increasingly crucial due to the increasing size and diversity of data generated across various scientific disciplines. However, to ensure reproducibility, comprehensive provenance metadata also needs to be available. Lab Integrated Data (LabID) is a web-based integrated platform designed to help individual scientists, research groups and core facilities to better manage, annotate and share their experiments, assays, samples and datasets actively, in compliance with FAIR principles. While in LabID processed data can already be stored and connected to its primary data, associated assays and original samples, accurate modelling of both workflow (WF) and WF runs is currently lacking. TheLabID-PROV projectaims to extend the LabID data model to include these concepts, offering a unified application to manage derived data provenance independently from analysis procedure and platform, and providing a concrete solution to ensure the traceability of derived data.\",\n      \"Challenge\": \"Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nManaging the increasing volume and variety of (meta)data - from sample collection to derived datasets - poses significant challenges in scientific research. While primary data is regularly shared in repositories, the sharing of final derived datasets remains inadequate. This is often due to the lack of comprehensive provenance metadata that ensures reproducibility and trustworthiness. Furthermore, the diverse languages, tools, and computing environments used by various stakeholders complicate the tracking of workflows and their corresponding metadata, limiting the reusability of shared data and hindering Open Science efforts.\",\n      \"Solution\": \"LabID-PROV proposes to enhance the LabID platform by allowing accurate modelling of both WFs and WF runs. To this end, the project will use several resources in the LS RI Science Cluster, namely, WorkflowHub, a resource for WF indexing, discovery and re-use; RO-Crate, which can be used to package and share research objects including WF and WF runs using the WF and WF Run RO-Crate profiles; Galaxy, a web-based analysis platform; and Zenodo, an open platform for preserving and sharing research output. The main goal is to streamline the import of datasets (and their metadata) described using Workflow Run RO-Crate profiles into LabID. Also, the project will implement use cases using both omics and imaging data, reflecting real-world scenarios, to demonstrate the integration of the new LabID capabilities into the existing Open Science landscape. These use cases will be used to generate online LabID tutorials demonstrating best practices in WF development and the FAIR dissemination of associated derived data with their provenance. Finally, wider dissemination of this work will be achieved by registering the training in the Training eSupport System and through a future LabIDPROV workshop.\",\n      \"Scientific Impact\": \"The LabID-PROV project aims to bridge critical gaps in data traceability to facilitate FAIR sharing of derived data together with their provenance metadata, through the integration of LS RI Science Cluster tools and standards. LabID-PROV not only will enhance the reproducibility of scientific findings but also encourages researchers across various disciplines to adopt robust data management practices. The project will ultimately support a wider community of scientists, enabling them to efficiently share valuable research outputs and contribute to a more sustainable Open Science ecosystem.\",\n      \"Keywords\": \"LabID platform, RO-Crate, data provenance, data traceability, data reproducibility\",\n      \"Principal investigator\": {\n        \"Name\": \"Prof. Eileen Furlong\",\n        \"Organisation\": \"EMBL\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"MADDEN - Multi-RI Access and Discovery of Data for Experiment Networking\",\n    \"Project_url\": \"https://oscars-project.eu/projects/madden-multi-ri-access-and-discovery-data-experiment-networking\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The MADDEN project is designed to tackle a key challenge in modern scientific research: facilitating seamless data access and sharing across multiple Research Infrastructures (RIs) in international collaborations. Scientific experiments, especially in fields such as gravitational wave (GW) research, often produce vast amounts of data stored in isolated Data Lakes. This fragmentation creates barriers to collaboration and data access. TheMADDENproject aims to build a multi-RI Data Lake managed with Rucio, a robust open-source framework for data management, distribution and access, initially developed to meet the requirements of the ATLAS experiment in High Energy Physics (HEP), and now widely adopted across various scientific communities. The project enhances Rucio to create a unified Multi-RI Data Lake, supporting international efforts to share and analyse experimental data more effectively, all within the EOSC framework.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Citizen science, Main RI concerned, Cross-domain/Cross-RI\\nModern scientific research, especially in fields such as GW research, requires international collaboration. Each RI typically manages its own data in isolated Data Lakes, hindering easy access and shared analysis. The emerging need is to establish a common data infrastructure that allows scientists to securely access data across multiple RIs for more efficient collaboration and discovery, especially as projects such as the Einstein Telescope (ET) and Cosmic Explorer (CE) prepare for the future of GW research.\",\n      \"Solution\": \"The MADDEN project aims to build a multi-RI Data Lake managed with Rucio. It will deploy the relevant services for the ET Data Lake based on Rucio, and build a multi-RI Data Lake by deploying a mock-up of a CE Data Lake, managed by an independent Rucio instance. Rucio authentication features will be extended to allow ET users to seamlessly access data from both ET and CE Rucio instances.The project will also advance and test RucioFS, a tool originally developed by INFN-Torino to provide a POSIX-like view of the Rucio catalogue in a multi-RI environment, implementing the required missing functionalities to be used in a production environment.Additionally, it is proposed to leverage Rucio metadata capabilities to incorporate the possibility of making rich metadata queries in RucioFS for filtering search results according to metadata conditions.Finally, the project aims to set up a demonstrator for the Virgo collaboration to test the technology.\",\n      \"Scientific Impact\": \"All developments proposed in this project (a multi-RI capable Rucio, a production-ready RucioFS and scalable metadata queries in Rucio) are part of the ESCAPE distributed data management (DDM) roadmap, and are applicable also in contexts beyond the GW community.As Rucio is the DDM solution adopted by the ESCAPE Science Cluster within EOSC, this project is the first necessary architectural step towards a consistent support of Open Science and the FAIR data principles in the GW physics domain, beyond the current existing GW Open Science Center (GWOSC). Common frameworks such as Rucio and RucioFS would allow less experienced data analysts to easily access and browse experimental data, and bridge the gap for multi-messenger analysis typically performed by scientists belonging to different astrophysics domains and astronomers.\",\n      \"Keywords\": \"Distributed data management (DDM),  gravitational waves, gravitational wave research, data lake, multi-RI data lake, Einstein Telescope, Cosmic Explorer, Rucio, RucioFS\",\n      \"Principal investigator\": {\n        \"Name\": \"Federica Legger\",\n        \"Organisation\": \"INFN (National Institute for Nuclear Physics)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"MatScatNet - A Cloud Database for Sharing X-ray & Neutron Scattering Data for Materials Research\",\n    \"Project_url\": \"https://oscars-project.eu/projects/matscatnet-cloud-database-sharing-x-ray-neutron-scattering-data-materials-research\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"MatScatNetis an innovative project aimed at enhancing materials research by creating a high-quality, open-access database for sharing X-ray and neutron scattering data. These powerful characterisation methods are essential for understanding the atomic structure of materials, which directly influences their properties and performance. Despite their significance, a comprehensive, openly accessible repository of experimental data is currently lacking, limiting researchers' ability to drive materials discovery and technological advancements. MatScatNet seeks to bridge this gap by providing a centralised platform that facilitates collaboration among researchers, industries, and research infrastructures (RIs), thereby promoting an Open Science approach and supporting innovation in fields such as nanotechnology, energy, and pharmaceuticals.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation\\nThe characterisation of materials through powder diffraction, total scattering techniques and pair distribution function analysis is essential for understanding their atomic structures and how these structures influence material properties. However, the lack of an openly accessible, structured repository for experimental data hinders research and collaboration. Currently, the only structured database of this sort is that of the International Centre for Diffraction Data (ICDD), which contains about 19,000 powder diffraction dataset. This gap limits researchers' access to crucial data, slowing down the materials discovery process.\",\n      \"Solution\": \"MatScatNet aims to develop an Open Science database populated with high-quality experimental data generated through a dedicated high-throughput setup at the ESRF. The project aims to engage diverse communities and RIs and enhance data sharing through user-friendly tools that improve the searchability and reusability of powder diffraction patterns and pair distribution function data. To achieve this goal, the MatScatNet team will develop and integrate new algorithms that will improve the speed, identification and findability of data.\",\n      \"Scientific Impact\": \"By establishing the first high-quality open materials scattering network (MatScatNet), complete with a growing database with advanced retrieval and identification interface and analysis tools for material scientists, MatScatNet will significantly advance materials research by fostering collaboration between academia and industry, ultimately enhancing the pace of materials discovery.\\nThe project also expects to empower researchers working in the many fields of material science to learn and perform initial data appraisals without prior expert knowledge. This will be accomplished by allowing users to identify and match their data with those from the MatScatNet database, the PaNOSC repository, and other open data sources, such as the Crystallography Open Database (COD) and Materials Project.\",\n      \"Keywords\": \"Materials science, material characterisation, neutron scattering, x-ray scattering, scattering data, materials research\",\n      \"Principal investigator\": {\n        \"Name\": \"Dr. Maxwell W. Terban\",\n        \"Organisation\": \"Momentum Transfer\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"MC-ReDD - Metadata Capture and validation for Re-use of raw Diffraction Data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/mc-redd-metadata-capture-and-validation-re-use-raw-diffraction-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"Diffraction experiments generate vast amounts of raw data in diverse formats, complicating their reuse and interoperability across research domains. TheMC-ReDDproject responds to this challenge by focusing on the expanded imgCIF format, which allows easy transfer and processing of the metadata associated with image data, without needing to transfer the actual data images. The project will create a publicly-available, easy-to-use tool for semi, or fully automatic construction of imgCIF files from raw data sets. This will facilitate the interoperability and reusability benefits offered by the imgCIF scheme so as to ensure trust in raw data re-use by specialists from other domains.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nRaw diffraction data are captured using numerous binary data formats and varying metadata structures, making it difficult to achieve interoperability or reusability. The large size of these datasets, often over 100 GB, further complicates data sharing and validation. Current methods for handling raw data lack standardisation, hindering transparent communication between researchers, and making it harder to trust and reuse data across disciplines.\",\n      \"Solution\": \"MC-ReDD aims to build on the text-based imgCIF format to create a publicly-available, easy-to-use tool for semi or fully automatic construction of imgCIF files from raw data sets allowing easy transfer and processing of the metadata associated with image data, without necessarily needing to transfer the entire dataset. This scheme offers, for the first time, a way to transparently communicate rich information about raw data in a standardised, robust, machine-readable fashion, allowing third-party raw data services to be provided on the open web.\",\n      \"Scientific Impact\": \"While enhancing interoperability and reusability of raw diffraction data across scientific domains, the project aims to make the tools available as an Open Science Service within the EOSC Web of FAIR data and Services, or PaNdata software catalogue, as well as in the form of an open service hosted by the IUCr journals website. The imgCIF files can become part of open data collections hosted at Metadata Catalogues, enabling cross searching with other EOSC components. The imgCIF file thus represents a FAIR Digital object.\",\n      \"Keywords\": \"diffraction data, powder diffraction, nano electron diffraction, imgCIF\",\n      \"Principal investigator\": {\n        \"Name\": \"Dr. Loes M.J. Kroon-Batenburg\",\n        \"Organisation\": \"IUCr CommDat working group\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"MORCDA - Making Open Research Data Suitable for Comparative Discourse Analysis\",\n    \"Project_url\": \"https://oscars-project.eu/projects/morcda-making-open-research-data-suitable-comparative-discourse-analysis\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"The promotion of a European public sphere is critical for the cultivation of democratic discourse and opinion formation. In this context, discourse analysis offers profound insights into transnational media discourses, supported by multilingual and comparative perspectives. Recent advancements in discourse analysis leverage machine learning and corpus analysis, expanding the scope for cross-border linguistic research. However, the full potential of Open Research Data (ORD) for comparative, multilingual discourse analysis remains underutilised. The MORCDA project seeks to bridge gaps between comparative discourse researchers and open research data communities, facilitating the adoption of ORD practices and optimising research infrastructures (RIs) for enhanced comparative analysis across European languages and contexts.\",\n      \"Challenge\": \"Open Science project, Main RI concerned\\nThe promotion of a European public sphere is critical for the cultivation of democratic discourse and opinion formation. In this context, discourse analysis offers profound insights into transnational media discourses, supported by multilingual and comparative perspectives. Recent advancements in discourse analysis leverage machine learning and corpus analysis, expanding the scope for cross-border linguistic research. However, the full potential of ORD for comparative, multilingual discourse analysis remains underutilised.\",\n      \"Solution\": \"MORCDA will bridge gaps between comparative discourse researchers and open research data communities, facilitating the adoption of ORD practices and optimising RIs for enhanced comparative analysis across European languages and contexts. It will do so through workshops with academic partners in Europe. In particular, it will focus on:\",\n      \"Scientific Impact\": \"The project will favour enhanced data utilisation by making extensive language data corpora more accessible and interoperable, facilitating deeper and more nuanced comparative studies across European regions. Moreover, it will contribute to community building by establishing a robust network linking ORD with comparative discourse analysts, supported by a platform offering ongoing training and resource sharing. Finally, RIs - particularly within the CLARIN community - will benefit from the project, and will adapt to support the needs of comparative research, influencing broader Open Science initiatives.\",\n      \"Keywords\": \"discourse analysis, transnational media discourse, comparative discourse analysis, cross-border linguistic research\",\n      \"Principal investigator\": {\n        \"Name\": \"Philipp Dreesen\",\n        \"Organisation\": \"Zurich University of Applied Sciences\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"mTeSS-X: Scaling training portal federation for RIs through Multi-tenanting and Exchange\",\n    \"Project_url\": \"https://oscars-project.eu/projects/mtess-x-scaling-training-portal-federation-ris-through-multi-tenanting-and-exchange\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences, PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"With the goal of overcoming the fragmentation of training resources across Research Infrastructures (RIs) and the Science Clusters, themTeSS-Xproject will enhance the existing open-source TeSS platform  - developed by ELIXIR specifically for building aggregator training portals, and currently used in ELIXIR and PaNOSC training portals - to natively support federation. Such a fully-featured open-source multi-tenanted training platform is expected to be an innovation for building a federation of portals to help break down barriers between thematic communities, promote a more cohesive European research community, and promote FAIR and open training.\\nThe project strives to support the federation of training catalogues using a multi-tenancy approach, and enabling cross-instance content exchange. This will allow RIs and their communities to maintain tailored catalogues with distinct identities, while simultaneously benefiting from a shared global pool of resources.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nMany RIs and EOSC projects have limited training catalogue capability beyond sparse, outdated lists on web pages, while cross-portal exchange has been discussed but not practised. Fragmentation of resources hampers the reuse of valuable training materials and limits the dissemination of events and learning pathways. Additionally, the need to customise and maintain separate portals for each RI results in duplicated efforts and inefficiencies, reducing the overall sustainability of training resources in the European research ecosystem. The main challenge of the project is thus to enable the federation of catalogues, support cross-discipline training resource exchange and celebrate and maintain catalogue unique identity.\",\n      \"Solution\": \"mTeSS-X will extend the TeSS platform by introducing multi-tenancy, allowing multiple communities to maintain distinct training catalogues within a shared instance, while preserving their individual identities. Each community will have a tailored view of the global catalogue (with custom look, features, content and integrations with other services), giving the appearance of a standalone portal, while benefiting from shared content and automated harvesting. This means that, for instance, in a Science Cluster portal, RIs would retain their own catalogue identity while promoting cross-RI, and/or cross-cluster sharing. In this respect, it will be demonstrated that merging the ELIXIR and PaN-Training portals into a single managed instance can present them as independent portals while still benefiting from shared content.\\nThe project will also implement TeSS-X plugins to facilitate seamless content exchange between catalogues implemented as separate instances, enabling communities that run their own portals to participate in cross-catalogue resource sharing. This innovative approach will optimise both portal operation and training material dissemination across disciplines.\\nMixing and matching these setups will enable cross-disciplinary training knowledge exchange, and will optimise portal operation, while serving training providers and consumers.\",\n      \"Scientific Impact\": \"By fostering the federation of interoperable training catalogues, mTeSS-X will significantly enhance the findability, accessibility, and reuse of high-quality training resources. The project will improve the sustainability of training portals by reducing operational duplication and enabling resource-sharing across portals. The adoption of FAIR principles and open training practices will ultimately lead to a more efficient and impactful training ecosystem, benefiting both training providers and consumers: training providers will have their training resources propagated across catalogues and disciplines. Training consumers will have the availability of a much higher amount of FAIR training resources across scientific disciplines.\",\n      \"Keywords\": \"training portal, content exchange, knowledge-exchange, federation of portals, federation of catalogues, training catalogue\",\n      \"Principal investigator\": {\n        \"Name\": \"Carole Goble\",\n        \"Organisation\": \"University of Manchester\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"NAPMIX: Nuclear, Astro, and Particle Metadata Integration for eXperiments\",\n    \"Project_url\": \"https://oscars-project.eu/projects/napmix-nuclear-astro-and-particle-metadata-integration-experiments\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The NAPMIX project aims to create a cross-domain metadata schema tailored for the Nuclear, Astro, and Particle Physics communities. By leveraging local and international connections, including Open Science initiatives like EOSC, EURO-LABS, and the ESCAPE Cluster, NAPMIX will foster synergies with related fields, enhancing cross-domain interoperability. The schema will facilitate the sharing and long-term findability of datasets, representing a breakthrough in Open Science within experimental physics. Community feedback will be integral to refining the schema, ensuring its relevance and sustainability.\",\n      \"Challenge\": \"Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nThe NAPMIX project emerged to address a significant gap in nuclear physics: the lack of a unified metadata schema necessary for achieving FAIR datasets. This challenge extends to the fields of particle and astro-particle physics, highlighting the need for a collaborative European effort to create a common metadata schema with user-friendly infrastructure. By integrating expertise across these domains, NAPMIX aims to enhance data management practices.\",\n      \"Solution\": \"NAPMIX will establish a collaborative community in experimental nuclear, astro-particle, and particle physics to develop a comprehensive metadata schema and user interface. The project will include:\\nFinally, the project will publish the schema and broaden connections within the ESCAPE community, while striving to connect to the EOSC.\",\n      \"Scientific Impact\": \"NAPMIX is positioned to significantly advance research data management by promoting discipline-specific metadata and fostering collaboration across fields. The project will enhance the uptake of Open Science practices and contribute to scientific progress through its integration with existing infrastructures, such as ESCAPE and PUNCH4NFDI. By facilitating cross-domain adoption and interoperability, NAPMIX will serve as a scalable service deployable across multiple research infrastructures. Upon completion, the community will continue to support and expand the schema, furthering the goals of FAIR data and Open Science.\",\n      \"Keywords\": \"cross-domain metadata schema, data management, cross-domain interoperability\",\n      \"Principal investigator\": {\n        \"Name\": \"Andrew  Kishor Mistry\",\n        \"Organisation\": \"GSI Helmholtzzentrum für Schwerionenforschung GmbH\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"OASIS - Open Audiovisual Science Innovation Scheme\",\n    \"Project_url\": \"https://oscars-project.eu/projects/oasis-open-audiovisual-science-innovation-scheme\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"The exploration of audiovisual materials represents an expanding area of scientific research. Film analysis has been prevalent and institutionalised within film studies and history for numerous decades; they now engage a diverse array of academic disciplines, including sociology, media studies, educational sciences, political science, or theatre studies. TheOASISproject -Open Audiovisual Science Innovation Scheme- focuses on the analysis, enhancement, archiving, and dissemination of video corpora for research within the Humanities and Social Sciences. The project promotes Open Science methodologies and facilitates the development of innovative tools tailored for video research, ultimately enriching the research landscape.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nLeading research in Humanities and Social Sciences on audiovisual corpora entails confronting significant challenges related to FAIR principles and openness. These challenges include devising methods for hosting and archiving audiovisual materials, mastering tools for qualitative and quantitative data mining and analysis, including AI technologies, and formulating strategies for the dissemination and valorisation of video data, both within and beyond the academic sphere, all while addressing the particular format and copyright concerns associated with images and videos.\",\n      \"Solution\": \"To meet these challenges, OASIS will develop an active network that brings together researchers from different European countries who are managing and working on online video corpora. This network, gathering over 200 researchers, will build upon the established experience of theHuma-Num CANEVAS consortium. In partnership with CANEVAS and other entities across Europe, OASIS broadens these collaborative efforts, with the aim of developing open-source and interoperable tools tailored for audiovisual corpora, and fully integrating the Science Cluster model and the EOSC ecosystem.\\nSpecifically, the OASIS project will facilitate the translation and customisation of these free and open-source tools to suit the needs of new European communities.\\nBuilding upon these initial steps, OASIS will develop educational resources in four languages - French, English, Croatian, and Romanian. Training courses and events will be organised, addressing researchers and doctoral candidates who aspire to undertake research involving audiovisual corpora. They will be designed to empower new research communities and to help them assimilate open tools, standards, and best practices for conducting research on audiovisual corpora in SSH domains.\",\n      \"Scientific Impact\": \"The development of a European research community on audiovisual corpora in the Humanities will be undertaken in collaboration with several key partners, including the IHECS school of media, communication, and journalism in Belgium, the Institute of Ethnology and Folklore Research in Croatia, the University of Craiova in Romania, and the Catholic University of Paris in France. Each of these partners will be working to develop the use of audiovisual corpora in a local community, but also in a given discipline, such as Media Studies, Performing Arts Studies, Political Sciences and Education. Additionally, OASIS will benefit from the expertise of DARIAH Working Groups, enhancing the dissemination of project results to already active transnational communities, thereby significantly contributing to the advancement of Open Science practices.\",\n      \"Keywords\": \"video research, video sharing platform, video annotation tool, media studies, sociology, media studies, educational sciences, political science, theatre studies\",\n      \"Principal investigator\": {\n        \"Name\": \"Laurent  Tessier\",\n        \"Organisation\": \"Institut Catholique de Paris\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"ONTOLISST - Thematic ontologies in social science research data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/ontolisst-thematic-ontologies-social-science-research-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"ONTOLISSTcombines existing social science ontologies with new algorithms to optimise topic assignment in multilingual contexts. It explores how standard background variables, such as gender or age, can be included in non-thematic metadata. The ultimate goal is to create cost-effective discovery tools in multiple EU languages, improving access to and visibility of social science research for a diverse group of users, from expert audiences to policymakers.\",\n      \"Challenge\": \"Open Science Service\\nThe ONTOLISST project addresses the challenge of how digital European and international social scientific archives construct realities through thematic assignments in their metadata ontologies. Currently, there is a lack of streamlined, multilingual tools to assign topics and concepts across social scientific data from various archives, which impedes data discoverability and accessibility for diverse audiences. The project team's analyses of the ontologies provide a basis to interpret thematic metadata systems that are often taken for granted, even though they are a product of specific languages, are historically formed and discipline-based practices.\",\n      \"Solution\": \"ONTOLISST proposes to develop the Light Social Science Thesaurus (LiSST), a controlled vocabulary of approximately 100 terms organised into two hierarchical layers. This thesaurus will be used to create a gold standard corpus for automated or semi-automated topic assignment in research data repositories, leveraging annotated samples of social scientific (meta)data in Finnish, English, French, and Hungarian. Additionally, the project investigates how Natural Language Processing (NLP) tools can be employed for efficient topic annotation across languages.\",\n      \"Scientific Impact\": \"Improving Understanding of Metadata CreationONTOLISST explores how scientific traditions and languages shape social science metadata, offering insights into how thematic metadata systems influence research interpretations.Enhancing Data Curation PracticesThe project simplifies thematic metadata assignment with a multilingual thesaurus and NLP tools, addressing current complexities and improving both manual and automated curation.Supporting Semi-Automated Metadata AnnotationBy creating a cross-language annotated dataset, ONTOLISST enhances NLP-driven topic assignment, reducing language barriers and improving data discoverability of social science data for both human and machine users.Fostering Interoperability across RIsONTOLISST supports cross-domain data curation standardisation, enabling smaller RIs and non-experts to access and reuse data more efficiently.Advancing AI-Driven CurationThe project tests AI tools for metadata curation, promoting cost-effective, open science practices that enhance data accessibility across disciplines and regions.\",\n      \"Keywords\": \"social science ontologies, metadata ontologies, thematic metadata systems, Light Social Science Thesaurus (LiSST), Natural Language Processing\",\n      \"Principal investigator\": {\n        \"Name\": \"Judit Gárdos\",\n        \"Organisation\": \"HUN-REN Centre for Social Sciences\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"OPAL - Ontology Portal for Astronomy Linked-data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/opal-ontology-portal-astronomy-linked-data\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"The OPAL project aims to advance the semantic infrastructure for astronomy by creating an ontology portal, as a centralised repository for vocabularies and ontologies. Astronomy has long benefited from theInternational Virtual Observatory Alliance - IVOA, which promotes data interoperability within the field. However, these vocabularies remain confined to the IVOA ecosystem, limiting their application in related disciplines, such as heliophysics and planetary sciences. The project, in collaboration with theOntoPortal Alliance, will expand the use of IVOA vocabularies across astronomy and neighbouring fields, enhancing data sharing, reuse, and interdisciplinary collaboration.\",\n      \"Challenge\": \"Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nThe astronomy community is recognised for itsVirtual Observatory infrastructure - IVOA, which enables interoperability and, in turn, smooth data sharing and reuse within this ecosystem. A key piece of this interoperability framework is the development and maintenance of vocabularies, as managed by the Semantics Working Group of the IVOA. Thanks to, among others, the EU funded Europlanet and ESCAPE programs, IVOA vocabularies spread to neighbouring disciplines (such as planetary sciences, heliophysics and particle physics). However, these vocabularies are still siloed in the IVOA framework.\",\n      \"Solution\": \"The OPAL project aims to design and set up an ontology portal, as a centralised Semantic Artefact Catalogue (SAC) for astronomy. The project will partner with the OntoPortal Alliance to re-use and customise the OntoPortal SAC technology. An OntoPortal instance offers a rich description of the metadata of the artefacts, and various advanced ontology-based services, such as browsing, searching, visualising, computing metrics, annotating, recommending, assessing FAIRness, as well as receiving community feedback and change requests. OPAL will establish the OntoPortal instance for astronomy, which will collect vocabularies and other \\\"semantic artefacts\\\" (SA) from the areas in the ESCAPE Science Cluster, including the Earth’s near-space environment, the early Universe, space physics, planetary sciences, stars and exoplanets, the interstellar and intergalactic medium, galaxies, compact objects (e.g., pulsars), cosmic rays, cosmology, and particle physics.\",\n      \"Scientific Impact\": \"By collating all the ESCAPE fields vocabularies and ontologies into a single portal, OPAL will enable semantic interoperability across various astronomy databases and applications. It will use common and coordinated FAIR assessment methodologies, and will improve data discoverability through consistent annotations. By enhancing the semantic infrastructure of astronomy, the ontology portal will thus enable researchers to discover and use data more efficiently through standardised vocabulary and ontology resources. The portal’s FAIR-compliant services will improve data discoverability and interoperability across various sub-disciplines within astronomy and related fields, supporting more comprehensive, interdisciplinary research.\",\n      \"Keywords\": \"Astronomy vocabularies and ontologies, Semantic Artefact Catalogue (SAC), IVOA (International Virtual Observatory Alliance), Semantic interoperability, FAIR principles, OntoPortal Alliance, Semantic Web technologies, Planetary sciences, Heliophysics\",\n      \"Principal investigator\": {\n        \"Name\": \"Clement Jonquet and Baptiste Cecconi\",\n        \"Organisation\": \"INRAE and OBS-PARIS\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Open and FAIR Integrated Phenology Monitoring System\",\n    \"Project_url\": \"https://oscars-project.eu/projects/open-and-fair-integrated-phenology-monitoring-system\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"Plant phenology is the seasonal progression of plant activity through stages of dormancy, active growth, senescence, and back to dormancy (Richardson et al. 2018). It is a key ecological parameter directly related to ecosystem processes and ecosystems-climate relations (Penuelas et al, 2009) and impacted by climate change and consequently linked to carbon and water cycles at ecosystem scale (Linderholm 2006). The projectOpen and FAIR Integrated Phenology Monitoring Systemaims to revolutionise the monitoring of plant phenology of terrestrial ecosystems through the design, development, and testing of an innovative phenological camera, coupled with a comprehensive post-processing software platform using open science and FAIR data practices. The project seeks to overcome traditional monitoring limitations, thereby enhancing data interoperability and accessibility for researchers, journalists, and the general public alike.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation, Citizen science, Main RI concerned, Cross-domain/Cross-RI\\nDespite the recognised significance of plant phenology - underscored by its classification as an Essential Biodiversity Variable - traditional monitoring methods often rely on infrequent visual observations, leading to challenges in data sharing and interoperability. Although advancements in satellite and remote sensing have facilitated large-scale monitoring, the current European PhenoCam network relies on a specific commercial camera (StarDot NetCam SC5) that is not updated to the new standard protocols, and is no longer produced. This hinders long-term data collection and compromises the quality of digital data flow. There is a strong need for a clear standard protocol, the optimisation of the processing routines for a Real Time phenology calculation, and the FAIR principle implementation to link images to the final product and all the metadata needed.\",\n      \"Solution\": \"The project aims to solve these main issues by supporting and promoting the implementation of the Phenocam measurements and an open science service to produce open access and FAIR phenology observations. It will do so by designing and developing a new phenological camera that operates on open protocols, thus providing detailed build-up instructions for potential industrial production. This camera will ensure the longevity and adaptability of monitoring efforts. Secondly, the project will implement a full data cycle, from the images quality evaluation to the final products, with special care about the FAIR principles and full metadata provision, to meet the standards required by Research Infrastructures (RIs). The processing procedures will be integrated into the ICOS Ecosystem Thematic Centre, connected to the ICOS Carbon Portal to serve both research RIs and interested citizens.\",\n      \"Scientific Impact\": \"The project will integrate the lessons learned in the phenology monitoring activities, as well as the knowledge of established RIs, such as ICOS, in data treatment and FAIRness, into an Open Science platform that will help to more easily deploy a network of phenocams across RIs, and offer a centralised and standardised service to create user ready products.The project results will help to facilitate science through a more easy data findability and accessibility, a continuous update of the results, a Near Real Time provision of the images and derived metrics, and a standardisation and interoperability among the different RIs in ESFRI and with the other international initiatives, while also engaging citizen scientists and educational institutions to foster broader community involvement in phenology research.\",\n      \"Keywords\": \"phenology, phonology monitoring system, plant phenology, phenocams\",\n      \"Principal investigator\": {\n        \"Name\": \"Dario Papale\",\n        \"Organisation\": \"University of Tuscia (Viterbo, Italy)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"OpenCorrAM - Open Corrosion Data for Additively Manufactured Alloys\",\n    \"Project_url\": \"https://oscars-project.eu/projects/opencorram-open-corrosion-data-additively-manufactured-alloys\",\n    \"Project_content\": {\n      \"Science cluster\": \"Other\",\n      \"Summary\": \"Metal additive manufacturing (MAM), central to Industry 4.0, has drawn significant interest in sectors such as medical, aerospace, energy, and automotive. However, widespread adoption remains limited due to concerns over inconsistent material properties and a lack of full understanding of how novel microstructures impact performance. To address these challenges, theOpenCorrAMproject will investigate the corrosion behaviour of additively manufactured materials, such as austenitic stainless steels, nickel-based superalloys, and titanium alloys. Using high-resolution microscopy, microcapillary electrochemical techniques, and surface analysis, the project will study passive layer formation and breakdown. By comparing AM materials with conventionally manufactured ones, it will provide new insights into corrosion science. Project findings will benefit industry, research, and policymakers by improving material performance, sustainability, and safety.\\nThe project team will develop a comprehensive, open-access database that interrelates processing, (micro)structure, and performance of AM alloys, with focus on corrosion aspects. This database will advance the state of knowledge, foster innovation in materials science and engineering, and help overcome barriers to the broader implementation of MAM technologies.\",\n      \"Challenge\": \"Open Science project, Industry cooperation, Main RI concerned, Cross-domain/Cross-RI\\nThe growing adoption of MAM faces significant hurdles, particularly the high variability in material properties and the unclear connections between novel microstructures and their performance. This inconsistency limits the full-scale implementation of MAM across industries requiring reliable corrosion resistance, such as aerospace and energy. The need for a systematic approach to understanding the corrosion behaviour of additively manufactured materials is crucial for fostering confidence in their use.\",\n      \"Solution\": \"Employing advanced high-resolution and microcapillary electrochemical techniques, OpenCorrAM strives to create an open and comprehensive database of intercorrelated results in the different fields of MAM performance analysis, that catalogues the corrosion characteristics of various additively manufactured materials. These investigations serve to provide exceptional insights into micro and submicron structural details, as well as the electrochemical and corrosion properties. Machine learning techniques will analyse the data, establishing relationships between microstructural features and corrosion behaviours, thereby enhancing predictive capabilities in materials performance.\",\n      \"Scientific Impact\": \"OpenCorrAM not only enhances the scientific understanding of corrosion in additively manufactured materials but also revolutionises the field through the establishment of an open-access knowledge base.\\nBy enabling access to a broad collection of data and findings, and by integrating machine learning techniques to analyse data and identify relationships between microstructural features and corrosion properties, OpenCorrAM helps standardise methods and approaches in corrosion studies related to AM materials, responding to the widely recognised strong need to open corrosion data to develop and feed predictive models in corrosion science. The project thus fosters collaboration and innovation, driving advancements in corrosion studies and supporting the MAM industry’s growth.\",\n      \"Keywords\": \"corrosion science, MAM industry, Metal additive manufacturing, high corrosion resistance, Additively Manufactured Alloys, metal 3D printing, open corrosion data\",\n      \"Principal investigator\": {\n        \"Name\": \"Reynier Revilla\",\n        \"Organisation\": \"Vrije Universiteit Brussel\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Opravidlo 2.0 – Public Online Proofreading Service\",\n    \"Project_url\": \"https://oscars-project.eu/projects/opravidlo-20-public-online-proofreading-service\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"Lexicon and grammar are essential to communication, spoken or written, which is more efficient if lexicon and grammar follow the orthography rules of the standardised language. Good quality writing is considered a social advantage since it reduces possible misunderstandings in communication. It is part of a person’s social capital.\\nOpravidlo 2.0aims to bring a beta-version online proofreading service,Opravidlo, into its operational phase. The project contributes to the evolution of Czech automatic proofreading by integrating AI with expert linguistic rules, making efforts to improve recall while maintaining precision. With over 161,000 monthly users, it corrects spelling, grammar, and typesetting errors with high precision, contributing to Open Science and aiding both native speakers and foreigners. Opravidlo 2.0 will enhance recall and precision, empowering  users from different linguistic backgrounds to communicate more effectively in Czech.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nMaintaining high precision in automatic proofreading while improving recall is a critical challenge. Current systems, including Beta Opravidlo, effectively identify spelling and grammatical issues but face limitations in covering all orthographic phenomena. Low recall can lead to user dissatisfaction, as incomplete error detection is problematic for a wide range of language users.  The risk is that users will lose trust in the tool and refuse to use it. Most current tools are outdated (over 15 years old) and may not account for shifts in formal Czech, such as influences from social media or the introduction of new words.\",\n      \"Solution\": \"Opravidlo 2.0 plans to combine the current rule-based system with artificial intelligence (AI) approaches, enhancing both recall and precision. New neural network models will be developed and trained for different proofreader modules, which will refine error detection across areas like punctuation, grammatical agreement, and sentence syntax, and neural models will also be applied to the explainability module.\\nIn the frame of the project, a set of formalised grammar rules will be published. This dataset can be adapted for languages with similar orthography rules, together with a corpus of corrected texts obtained with the users’ prior permission, and including Opravidlo 2.0 suggestions, applied rules, and information about whether the user accepted the correction.\\nThe tool will also expand its utility with an internationalised interface to support Czech language learners from diverse backgrounds, including speakers of Slovak, Ukrainian, Russian, and English.\\nAll relevant data and methods will be published mainly via CLARIN ERIC and DARIAH ERIC.\",\n      \"Scientific Impact\": \"Opravidlo 2.0 will contribute to Open Science by publishing its grammar rules and anonymised corrected texts via the CLARIN infrastructure. This will enable linguistic analysis, teaching, and tool development across languages with similar orthography systems. The project’s hybrid AI approach aims to set a new standard for proofreading technologies, benefiting both public services and future linguistic research. Also, a high-precision proofreader will serve the public good for all age groups of native Czechs and accelerate the integration of immigrants to Czechia.\\nAll materials will be released via the CLARIN infrastructure. The published material can, among others, serve for linguistic analysis, language teaching, and training and evaluation of automated proofreaders in different languages.\",\n      \"Keywords\": \"proofreading, Opravidlo, Czech language, artificial intelligence, AI tool\",\n      \"Principal investigator\": {\n        \"Name\": \"Ales Horak\",\n        \"Organisation\": \"Masaryk University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"OSPARK Bootcamp - Open Science Promotion and Advocacy for Research Knowledge Bootcamp\",\n    \"Project_url\": \"https://oscars-project.eu/projects/ospark-bootcamp-open-science-promotion-and-advocacy-research-knowledge-bootcamp\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, ESCAPE - Astronomy, Nuclear and Particle Physics, LS RI - Life Sciences, PANOSC - Photon and Neutron Science, SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"The OSPARK project aims to empower the Open Science community by developing a comprehensive  multi-week training bootcamp focused on marketing and communication. The programme specifically targets members of the OSCARS Science Clusters and the broader Open Science community, aiming to create “multipliers” who can effectively promote their initiatives. By equipping these advocates with evidence-based strategies, the project seeks to enhance the visibility of ESFRIs and other research infrastructures (RIs), enabling them to share their messages more effectively. This proactive approach not only addresses current gaps in awareness but also cultivates a robust network of Open Science advocates dedicated to fostering a collaborative research environment.\",\n      \"Challenge\": \"Open Science project, Cross-domain/Cross-RI\\nThe Open Science community grapples with a significant awareness gap regarding the valuable resources and services offered by various RIs. Many potential users remain uninformed about the wealth of knowledge and support available to them, limiting their engagement with these initiatives. This lack of visibility not only affects the effectiveness of individual projects but also hinders the overall growth of the EOSC. As the landscape of research continues to evolve, bridging this awareness gap is crucial for fostering a culture of openness, collaboration, and equitable access to scientific resources.\",\n      \"Solution\": \"The OSPARK curriculum encompasses foundational principles of evidence-based marketing, modern media utilisation, and innovative outreach techniques that extend beyond traditional scientific publications. Participants will engage in practical exercises, exploring various outreach formats such as videos, science slams, and social media campaigns tailored to specific target audiences.\\nThroughout the bootcamp, a community-based and agile methodology will be employed, integrating feedback and case studies from existing RIs and Open Science initiatives. This collaborative approach ensures that the training remains relevant, effective, and responsive to the needs of the participants, ultimately empowering them to implement these new skills in their day-to-day work.\",\n      \"Scientific Impact\": \"The OSPARK project holds the potential to dramatically increase the visibility and effectiveness of Open Science initiatives, including RIs, the Science Clusters, grassroots communities and more. By not only training participants in marketing and outreach but also co-developing full-fledged marketing strategies during the bootcamp, the project aims to transform lesser-known initiatives into thriving resources that attract researchers and research staff. Furthermore, all training materials and supporting data will be made openly accessible under FAIR principles, promoting broader reuse and adaptation of the knowledge generated. This commitment to openness ensures that the tools and strategies developed during the project are available for continued learning and improvement, ultimately contributing to the advancement of Open Science as a whole.\",\n      \"Keywords\": \"training, education, training bootcamp, marketing and communications, Open Science advocacy\",\n      \"Principal investigator\": {\n        \"Name\": \"Heidi Seibold\",\n        \"Organisation\": \"Digital Research Academy\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"PaN-Finder: Photon and Neutron federated knowledge finder\",\n    \"Project_url\": \"https://oscars-project.eu/projects/pan-finder-photon-and-neutron-federated-knowledge-finder\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"ThePaN-Finderproject aims to enhance the capabilities of the existing PaNOSC Data Portal, which provides researchers access to a wealth of data from Europe’s Photon and Neutron (PaN) Research Infrastructures (RIs). Originally developed as part of the EU Horizon 2020 project PaNOSC, the portal has already improved the visibility and accessibility of scientific data. PaN-Finder will introduce an AI-powered search tool that simplifies user interaction, enabling intelligent, prompt-based searches. By leveraging state-of-the-art AI technologies, such as Large Language Models and Natural Language Processing, this tool will make it easier for researchers, journalists, and the general public to navigate and utilise valuable PaN data effectively.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nAs the scientific data pool grows, finding relevant and high-quality data becomes increasingly difficult. The existing PaNOSC Data Portal has enabled access to PaN research outputs, but there remains a need for more intuitive search mechanisms. The challenge lies in improving user interaction and data retrieval efficiency, enabling a wider range of users to navigate this expansive knowledge base effectively.\",\n      \"Solution\": \"PaN-Finder is a prompt based search system, similar to ChatGPT, which will include a UI interface and underlying infrastructure. It will adopt state-of-the-art AI technologies (including Large Language Modelling - LLM, AutoEncoders, and Retrieval Augmented Generation - RAG), and pair them with Information Retrieval (IR) and Natural Language Processing (NLP) methodologies. As a result, the PaN-Finder tool will have the capability to understand both summative and exploratory questions, providing meaningful responses that summarise a vast knowledge pool and provide references to relevant resources. Embedding this innovative feature in the existing PaNOSC Data Portal, the project aims to stimulate usage within the PaN community and accelerate its adoption rate.\",\n      \"Scientific Impact\": \"PaN-Finder will significantly improve data findability and accessibility, and will incentivise RIs to provide higher-quality data with well defined metadata, resulting in higher reusability of data. Open Science research will thus be enhanced, by promoting the adoption of FAIR principles across the research community, and enabling a wider audience to perform AI assisted, intelligent, user-friendly searches on the PaN knowledge. PaN-Finder will generate invaluable intelligence on PaN data, which will be instrumental in developing comprehensive guidelines for data curation and enhancing existing metadata. By facilitating better data discovery and curation practices, the project will accelerate scientific progress and collaboration, ensuring that high-quality data with enriched metadata are easily accessible for future research. The long-term impact will extend beyond the PaN community, fostering the seamless adoption of AI innovations while enhancing the overall quality and accessibility of PaN data resources. As part of the project’s outreach activities, these insights will be shared in the community, fostering collaboration and strengthening data quality standards.\",\n      \"Keywords\": \"PaN Data Portal, photon and neutron data, photon and neutron science, AI technologies, PaN data\",\n      \"Principal investigator\": {\n        \"Name\": \"Massimiliano Novelli\",\n        \"Organisation\": \"European Spallation Source\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"ParlaCAP - Comparing agenda settings across parliaments via the ParlaMint dataset\",\n    \"Project_url\": \"https://oscars-project.eu/projects/parlacap-comparing-agenda-settings-across-parliaments-parlamint-dataset\",\n    \"Project_content\": {\n      \"Science cluster\": \"SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"TheParlaCAPproject leverages advanced natural language processing to analyse political agendas and sentiments in debates from 27 European national parliaments. The automatic coding of agendas throughout a wide dataset of more than 7 million speeches, given in more than 20 languages, has become possible recently with significant developments in natural language processing and artificial intelligence, allowing for multilingual transformer models to provide both highly consistent and accurate codings. By integrating the ParlaMint dataset and the Comparative Agendas Project's coding scheme, the project will create a comprehensive, FAIR dataset for comparative political research, enhancing transparency and accountability in legislative discourse across Europe.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI,\\nParliaments are the cornerstone of democracy in Europe, ensuring the political representation of citizens. Despite their empirical relevance, parliamentary studies have often limited their scope to a single parliamentary body or a small group of parliaments analysed in comparative perspective.\\nThe main challenge of the ParlaCAP project is to bridge the gap between existing parliamentary research data and how to utilise these data in political science research by integrating two key international and cross-disciplinary initiatives: the CLARIN ERIC ParlaMint project, which provides texts of parliamentary debates from 27 European national parliaments, and the Comparative Agendas Project (CAP), which offers a coding schema of 21 topics for tracking political agendas in parliamentary proceedings.\",\n      \"Solution\": \"The project will employ the Comparative Agendas Project's text-as-data methodology to analyse parliamentary debates of all the 27 parliaments, consisting of more than 7 million speeches, given in more than 20 languages, by automatically coding the agenda of each speech and transforming the ParlaMint corpora into a structured and tabular dataset, available for complete download through CESSDA ERIC.\\nThe project aims to further code each speech with the sentiment expressed, as well as cross-reference the data with the PartyFacts metadatabase on political party metadata and the V-DEM surveys on the state of democracies. With this enriched and fully-FAIR dataset, now suitable for quantitative research, it will be possible to acquire a comprehensive understanding of how political attention is distributed across policy areas by analysing topic and sentiment coding over an unprecedented number of parliaments for political science research. The dataset will be available through RIs, such as CESSDA, CLARIN, and DARIAH, along with a graphical user interface and API for broader accessibility.\",\n      \"Scientific Impact\": \"ParlaCAP will revolutionise comparative parliamentary studies by providing a robust dataset for tracking political agenda-setting across European parliaments. Its open, FAIR data management approach will support a wide range of RIs and projects in social sciences, while promoting transparency in political discourse and accountability of legislative bodies. The findings will have societal relevance, fostering collaboration in political science and beyond.\\nMoreover, the engagement activities foreseen in the frame of the project will provide services and accompanying tutorials to raise awareness of the political studies community and ensure that the project's results are FAIR for further application and research by scientists across various Social Sciences and Humanities (SSH) domains.\",\n      \"Keywords\": \"natural language processing, parliamentary research data, ParlaMint, political science, AI, artificial intelligence\",\n      \"Principal investigator\": {\n        \"Name\": \"Nikola Ljubešić\",\n        \"Organisation\": \"Jožef Stefan Institute\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"PNU-OpenLab - Advanced Materials Open Science Centre at the Precarpathian National University\",\n    \"Project_url\": \"https://oscars-project.eu/projects/pnu-openlab-advanced-materials-open-science-centre-precarpathian-national-university\",\n    \"Project_content\": {\n      \"Science cluster\": \"Other\",\n      \"Summary\": \"ThePNU-OpenLab project, hosted at Vasyl Stefanyk Precarpathian National University (PNU), aims to serve as a central hub for advanced research on materials for energy and life sciences. It seeks to expand access to cutting-edge research tools for Ukrainian scientists, with the possibility of future integration into the Central European Research Infrastructure Consortium (CERIC-ERIC). PNU-OpenLab includes existing facilities, such as PNU-NanoLab, which focuses on nanotechnology, and PNU-FabLab, a space designed to foster practical learning and prototyping. Together, these labs aim to enhance interdisciplinary collaboration and promote open access to advanced technology, tools, and expertise.\\nUltimately, the project’s goal is to establish PNU-OpenLab as a leading centre for Open Science in Ukraine, driving innovation and collaboration in advanced materials research.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nPNU-OpenLab addresses the need for consolidating Ukraine’s scientific research efforts in advanced materials and nanomaterials amidst ongoing challenges. The project aligns with the National Open Science Plan, which aims to foster open access to scientific data, encourage knowledge dissemination, and enhance citizen engagement in science. In the context of war, relocating key research and industrial capacities to Western Ukraine has also become crucial.\",\n      \"Solution\": \"By establishing the PNU-OpenLab as a leading centre for collaboration, open data sharing, and advanced materials and nanomaterials research, the project aims to enhance collaboration among Ukrainian researchers, while providing consultancy on FAIR data management, and offering standardised and FAIR-compliant data repositories. Training programmes and workshops will equip researchers with knowledge on data collection, storage, and sharing, fostering data accessibility and reuse in the global scientific community, and empowering them to work in Open Science frameworks.\\nThe certified PNU-NanoLab will offer a controlled environment for materials characterisation and experiments, crucial for industries, such as defence, energy, and precision engineering. While integration into CERIC is an aspirational goal, PNU-OpenLab will build partnerships with European RIs, aligning with international standards.\",\n      \"Scientific Impact\": \"PNU-OpenLab will facilitate the certification of the PNU-NanoLab, enabling it to deliver accredited materials and data analysis services. This certification will ensure rigorous quality control and reliable results, essential for industries relying on high-tech materials. Furthermore, PNU-OpenLab will promote interdisciplinary collaboration and contribute to initiatives like the EDUC-WIDE project, enhancing Ukraine's integration into the European Research Area and supporting science-industry collaborations, especially critical in the context of the ongoing challenges faced by the country.\",\n      \"Keywords\": \"advanced materials, materials science, nanomaterials, PNU-OpenLab, PNU-FabLab, PNU-NanoLab, FAIR data repository\",\n      \"Principal investigator\": {\n        \"Name\": \"Volodymyr Kotsiubynskyi\",\n        \"Organisation\": \"Vasyl Stefanyk Precarpathian National University\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"PRIVAGAMS - Services for Privacy Advancement through Generative AI and Model Sanitisation\",\n    \"Project_url\": \"https://oscars-project.eu/projects/privagams-services-privacy-advancement-through-generative-ai-and-model-sanitisation\",\n    \"Project_content\": {\n      \"Science cluster\": \"LS RI - Life Sciences\",\n      \"Summary\": \"PRIVAGAMScreates a cutting-edge platform for generating privacy-preserving simulated data using Generative Adversarial Networks (GANs). It ensures high-quality, realistic datasets while protecting sensitive information, with applications in clinical, tabular, and imaging data. The project also focuses on sanitising machine learning models, through advanced techniques, such as model distillation and watermarking, enabling secure research across various RIs.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Cross-domain/Cross-RI\\nEnsuring privacy while maintaining data utility is a growing concern, especially in sensitive fields like healthcare, where data includes personal medical histories and diagnostic images. Current anonymisation techniques struggle to preserve critical relationships within data, such as links between diagnoses with corresponding imaging data. Additionally, machine learning models can inadvertently retain sensitive information, potentially exposing private data during analysis, particularly in federated learning frameworks.\",\n      \"Solution\": \"PRIVAGAMS will create a platform enabling research institutions to produce high-quality simulated data customised for specific needs, thereby increasing data availability while ensuring privacy remains intact. The project will do so by utilising Generative Adversarial Networks (GANs) to create simulated datasets that closely mimic real data without exposing personal information. These simulated datasets retain the statistical and relational properties of the original data, making them highly valuable for research while safeguarding privacy.\\nThe platform supports multiple data types, including clinical, tabular and imaging data. It also introduces privacy-enhancing techniques for machine learning models, such as model distillation and watermarking, reducing the risk of sensitive data exposure.\",\n      \"Scientific Impact\": \"The innovations of PRIVAGAMS will significantly enhance research capabilities in fields requiring access to sensitive data, such as healthcare and genomics. By allowing the generation of privacy-preserving yet realistic datasets, the platform enables institutions to share data more freely without compromising privacy. Its model sanitisation techniques secure machine learning models against data leakage. With initial integration into BBMRI-ERIC and plans to expand to EuroBioImaging, ELIXIR and EUCAIM, the platform enhances FAIR data management and privacy practices for sensitive data across RIs.\",\n      \"Keywords\": \"Generative Adversarial Networks, sanitising machine models, model distillation, watermarking, sensitive data\",\n      \"Principal investigator\": {\n        \"Name\": \"Heimo Müller\",\n        \"Organisation\": \"Medical University Graz\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"RISKY - Wildlife Mortality from Energy and Transport Infrastructure\",\n    \"Project_url\": \"https://oscars-project.eu/projects/risky-wildlife-mortality-energy-and-transport-infrastructure\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, LS RI - Life Sciences\",\n      \"Summary\": \"The RISKY project addresses the significant threats to biodiversity caused by energy and transport infrastructures, including millions of wildlife fatalities annually from collisions and electrocutions. By developing an open-access web platform that extends the Atlas of Living Australia (ALA), RISKY aims to expand existing biodiversity data services by integrating mortality risk analysis for terrestrial vertebrates. The project seeks to guide sustainable infrastructure planning, minimise mortality, and inform conservation strategies to achieve zero net loss of biodiversity.\\nPhoto credits: Marcel Huijser\",\n      \"Challenge\": \"Open Science project, Open Science Service, Industry cooperation\\nAddressing the climate emergency requires a rapid transition to renewable energy technologies and a substantial expansion of the electricity grid. However, these infrastructures pose significant threats to biodiversity, primarily through mortality from collisions and electrocutions, with millions of birds and mammals affected annually. The loss of individuals can increase the risk of local extinction, undermining long-term conservation efforts and resulting in significant socioeconomic consequences, including human fatalities and costly infrastructure damage. To counter this, it’s crucial to prioritise environmentally sustainable planning that minimises impacts on wildlife and achieves zero net loss of biodiversity.\",\n      \"Solution\": \"RISKY will develop a web platform that extends the Atlas of Living Australia (ALA) by integrating global wildlife mortality data from collisions and electrocutions. Users will be able to access mortality locations and rates, along with four Virtual Research Environment (VRE) analytical tools, including spatially explicit population models, life traits models, and sensitivity mapping to predict species extinction risks under various infrastructure expansion scenarios. These tools will enable users to assess, visualise, and mitigate the impacts of infrastructure on wildlife populations globally.By providing critical insights into the impact of mortality from energy and transport infrastructures, RISKY will inform effective conservation strategies while minimising socioeconomic costs. The platform will be accessible to a broad audience, including NGOs, consultants, public administration officials, and infrastructure operators.\",\n      \"Scientific Impact\": \"RISKY will provide new open-access data and analytical tools that enhance scientific understanding of mortality risks associated with energy and transport infrastructures. This platform will facilitate better nature-friendly planning and more effective mortality mitigation methodologies. Additionally, it will significantly expand the user base, engaging scientists in wildlife conservation, applied ecology, civil engineering, and various stakeholders, thereby fostering collaborative efforts to protect biodiversity.\",\n      \"Keywords\": \"wildlife mortality data, biodiversity data, Atlas of Living Australia - ALA, mortality risk analysis, population models, life traits models, sensitivity mapping\",\n      \"Principal investigator\": {\n        \"Name\": \"Clara  Grilo\",\n        \"Organisation\": \"BIOPOLIS- CIBIO\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"RSOTC - The Regional State of the Climate dashboard\",\n    \"Project_url\": \"https://oscars-project.eu/projects/rsotc-regional-state-climate-dashboard\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences\",\n      \"Summary\": \"Human-made climate change is one of the main challenges that current societies are facing. As the impacts become increasingly relevant, near-real-time monitoring of the climate, namely the evolution of variables such as rainfall and temperature, is something essential. TheRegional State of the Climate dashboard - RSOTCproject aims to create a web-based dashboard providing near-real-time climate data (maps and diagrams) for European political regions (e.g., NUTS divisions), showing the evolution of climate indicators as those used in the “State of the climate” (SofC) reports. It will enable users to visualise climate trends such as rainfall and temperature, applying FAIR principles to ensure open access, reproducibility, and transparency. The project leverages existing climate data, integrates advanced statistical tools, and enhances access for scientific analysis, policy-making, and public awareness.\",\n      \"Challenge\": \"Open Science project, Open Science Service\\nWhile State of the Climate (SofC) reports (e.g., WMO, C3S, AEMET, NOAA, CSIRO) have been gathering increased public attention, they often lack regional detail (such as time series and histograms) and do not typically follow FAIR principles: although data are usually open, the code and algorithms used for downloading and processing it are not, thus hindering reproducibility. Additionally, current reports tend to omit statistical insights, such as trend analysis or advanced filtering, which limits the understanding of long-term climate patterns vs. short-term variability.\",\n      \"Solution\": \"The RSOTC project will provide a FAIR-by-design service that offers visual and statistical insights into climate trends at regional levels. In this respect, the RSOTC dashboard will allow users to explore regional climate data with unprecedented detail. All the assets used to build the whole workflow, from downloading the data to producing the figures, will undergo the FAIR principles. The dashboard integrates open-access data from ERA5 reanalysis and E-OBS gridded datasets, with open workflows published in open repositories, such as GitHub and Zenodo, for further analysis and reuse. The project also foresees the development of an API to be included in the service catalogue of ENVRI and integrated with the EOSC EU Node AAI and portal systems. This will allow project members to join the EOSC federation, and to bring data from the C3S Climate Data Store to the ENVRI ecosystem.\",\n      \"Scientific Impact\": \"By providing tools for statistical testing and trend identification, the project will allow users to explore climate data with unprecedented detail, such as precipitation time series with a regional detail, and to apply statistical tools to test scientific hypotheses, a feature which is not available in other similar dashboards. This will allow identifying new trends on the extremes and other important statistics, but also other important features of the data that may have been overlooked by previous analysis. The dashboard also promotes collaborative research by allowing the scientific community to share findings and insights, thereby advancing climate science and fostering informed decision-making on environmental policy.\",\n      \"Keywords\": \"Regional State of the Climate dashboard, climate change, State of the Climate, State of the Climate reports, climate data\",\n      \"Principal investigator\": {\n        \"Name\": \"Markel García\",\n        \"Organisation\": \"Predictia Intelligent Data Solutions SL and Consejo Superior de Investigaciones Científicas\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"SemantyFish: Advancing Visibility, Interoperability and Exploitability of FishBase\",\n    \"Project_url\": \"https://oscars-project.eu/projects/semantyfish-advancing-visibility-interoperability-and-exploitability-fishbase\",\n    \"Project_content\": {\n      \"Science cluster\": \"ENVRI - Environmental Sciences, LS RI - Life Sciences\",\n      \"Summary\": \"SemantyFishaims to enhance the visibility, interoperability, and exploitability of FishBase by transforming it into a semantic web knowledge base. By leveraging semantic web technologies, the project will enable seamless data exchange, complex queries, and integration with global research infrastructures, advancing its role in open science and interdisciplinary research.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned, Cross-domain/Cross-RI\\nFishBase, the world’s most accessed database on marine and freshwater fishes, provides comprehensive data on fish biology, such as life cycle, history, population dynamics, taxonomy, and more. It is continuously updated with new information contributed by experts in the field, making it an invaluable resource for the fisheries domain. Its resources are accessible through replicating websites allowing users to search and find the desired resources, using various search forms. However, despite FishBase being accessed over 23,000 times daily, its interoperability and integration across different systems and domains can be enhanced to exploit its full potential for fisheries management, aquatic biodiversity, and interdisciplinary research.\",\n      \"Solution\": \"SemantyFish aims to transform FishBase into a semantic web knowledge base using cutting-edge semantic web technologies. By structuring data with standard vocabularies and ontologies, the project will enhance data interoperability, making FishBase more discoverable and accessible for both humans and machines. This shift will allow seamless data exchange across domains and enable more complex and meaningful data queries. In addition, it will enable richer, more meaningful and complex data queries and analysis by relying on semantic relationships between entities.\\nAs a semantic web knowledge base, its contents can be queried using SPARQL. Furthermore, a dedicated API will be implemented as a versatile gateway for accessing the wealth of resources of the knowledge base in different formats and standards. Designed with REST principles, it will offer simplicity, scalability and interoperability, making it the ideal interface for developers, scientists and researchers.\",\n      \"Scientific Impact\": \"SemantyFish will significantly enhance FishBase’s visibility, interoperability, and integration with other RIs, such as the Global Record of Stocks and Fisheries (GRSF), the World Register of Marine Species (WoRMS), and the EOSC, also linking FishBase to broader domains, such as climate change, food security, and pollution research.\\nThe richness of FishBase contents, combined with the advanced expressivity offered by semantic web technologies, will allow for richer and more precise representation of concepts and their relationships. Through the provision of both generic and fine-grained APIs, the research community and the wider public will have access to this wealth of information. This will reinforce the placement of FishBase as a credible source of fish-related information.\",\n      \"Keywords\": \"Fisheries, Computer Science, FishBase, semantic web technologies, data interoperability\",\n      \"Principal investigator\": {\n        \"Name\": \"Prof. Yannis Tzitzikas\",\n        \"Organisation\": \"Institute of Computer Science (ICS) of the Foundation for Research and Technology - Hellas (FORTH)\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"SHARE: Synchrotron x-ray analysis of Heritage Accessible to and Reusable by Everyone\",\n    \"Project_url\": \"https://oscars-project.eu/projects/share-synchrotron-x-ray-analysis-heritage-accessible-and-reusable-everyone\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science, SSHOC - Social Sciences and Humanities\",\n      \"Summary\": \"Synchrotron X-ray analytical techniques are increasingly used in heritage science to characterise the composition of ancient and artistic objects, but access to the data collected during experiments remains very limited. TheSHAREproject will set up an easily searchable database that will provide information on art objects and model samples analysed throughthe Historical Materials Block Allocation Group (BAG), providing easy access to raw and processed data, and metadata. By preserving valuable data, and promoting the integration of FAIR principles and Open Science practices within the heritage and broader scientific communities, SHARE opens the door for researchers worldwide to explore, share, and build upon this vast collection.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Citizen science, Main RI concerned, Cross-domain/Cross-RI\\nCultural heritage is a vital record of past civilisations, and its preservation requires detailed analysis using advanced technologies. Synchrotron x-ray techniques have proven invaluable for characterising ancient and artistic objects. However, access to these resources remains limited to expert users through a competitive selection process. Furthermore, despite recent upgrades such as the installation of the ESRF’s Extremely Brilliant Source, which enhances synchrotron performance, the data generated from these experiments is still largely inaccessible and difficult to reuse in line with FAIR data principles. The SHARE project aims to tackle this challenge by making the Synchrotron Heritage BAG data easily accessible and reusable by everyone.\",\n      \"Solution\": \"The SHARE project aims to develop an easily searchable database of over 1,600 datasets from synchrotron analysis on art objects and model samples, collected and analysed since 2020 through the Historical Materials BAG. This database will provide easy access to raw and processed data, and metadata. The first beneficiaries of this database will be the BAG users themselves, who will be able not only to easily retrieve and share their data, produce and cite DOIs, but also query the database with a view to cross-fertilisation and transversal research.\\nThe project also aims to familiarise the heritage community with FAIR best practices and contribute to the implementation of Open Science tools that can benefit ESRF and other synchrotrons’ user communities.\",\n      \"Scientific Impact\": \"The creation of a database with advanced search tools will allow BAG users to track, share, and sustain their datasets. By preserving and making heritage data reusable, the project ensures that researchers from the global heritage science community can continue to study and safeguard cultural treasures. SHARE will also provide training to the BAG community in FAIR best practices, ensuring that Open Science becomes standard.\\nWith the ongoing development of better data processing tools, notably using artificial intelligence (AI), the project opens up new possibilities for data-driven discoveries. The adoption of Open Science practices and FAIR data principles will serve as a model for synchrotron facilities worldwide, encouraging the development of similar initiatives and enhancing collaborative research across scientific domains.\",\n      \"Keywords\": \"heritage science, synchrotron, synchrotron x-ray analysis, Historical Materials Block Allocation Group (BAG)\",\n      \"Principal investigator\": {\n        \"Name\": \"Marine Cotte\",\n        \"Organisation\": \"ESRF\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"STARLIGHT - Stereo Telescopes Analysis Real-time Leading-edge Innovation Gamma High-Throughput data\",\n    \"Project_url\": \"https://oscars-project.eu/projects/starlight-stereo-telescopes-analysis-real-time-leading-edge-innovation-gamma-high\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics, Cluster-funded project(s)\",\n      \"Summary\": \"STARLIGHT aims to revolutionise the Real-Time Analysis (RTA) of CTAO data by transitioning from a monoscopic to a stereoscopic approach. Integrating simultaneous observations from multiple telescopes will enhance event reconstruction, increase sensitivity, and enable rapid detection of transient high-energy phenomena.\\nPhoto credit: CTAO gGmbH\",\n      \"Challenge\": \"Detecting low-energy gamma events is challenged by the limited Cherenkov light yield and interference from night sky background. The current monoscopic methods lack the precision and speed needed to capture fleeting cosmic phenomena, such as gamma-ray bursts and AGN flares.\",\n      \"Solution\": \"STARLIGHT will implement a three-phase development: beginning with stacked monoscopic analysis, advancing to low-level stereoscopy, and culminating in high-level stereoscopic analysis. By leveraging HPC and machine learning techniques, the project will merge data from multiple telescopes in real time to improve accuracy and trigger timely alerts.\",\n      \"Scientific Impact\": \"The enhanced RTA system will significantly improve gamma-ray detection sensitivity and resolution, leading to breakthroughs in high-energy astrophysics. This advancement will enable precise studies of transient events and foster interdisciplinary collaboration within the open science framework, strengthening the European research infrastructure.\",\n      \"Keywords\": \"Gamma-ray Astronomy, Stereoscopic Analysis, Real-Time Analysis, High-Performance Computing, Open Science\",\n      \"Principal investigator\": {\n        \"Name\": \"Sami Caroff\",\n        \"Organisation\": \"CNRS-LAPP\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"Streamlining open data policies in Rucio data management platform\",\n    \"Project_url\": \"https://oscars-project.eu/projects/streamlining-open-data-policies-rucio-data-management-platform\",\n    \"Project_content\": {\n      \"Science cluster\": \"ESCAPE - Astronomy, Nuclear and Particle Physics\",\n      \"Summary\": \"Rucio is an open source data management solution used by different nuclear, particle and astrophysics experiments, such as the LHC at CERN (ATLAS, CMS), AMS, DUNE, Belle II, ICARUS, LIGO/VIRGO, CTA, MAGIC and Rubin LSST, and is a key component of the ESCAPE Science Cluster. Exabytes of scientific (raw) data are managed with this technology and transferred worldwide for efficient distribution of data sets that need to reach end-users. However, existing workflows often require duplicating data to third-party systems to make it openly accessible. The projectStreamlining open data policies in Rucio data management platformaims to enhance Rucio by embedding open data policies directly into its core, enabling seamless data sharing without duplication, and supporting interdisciplinary research.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Citizen science, Cross-domain/Cross-RI\\nThe growing need for interdisciplinary research, particularly in multi-messenger science, has highlighted the importance of sharing scientific data across institutions performing complementary research (infrared, gravitational waves, etc.). However, institutions often rely on archaic or error-prone methods for data sharing. Moreover, given that Rucio does not recognise open data as a main data type, experiments need to copy this data to other systems to make them FAIR, incurring extra costs for the storage of the copy of the data. The main objective of this project is thus to introduce native support in Rucio to manage open data, where embargo and public access policies can be defined, so a copy of the data is not needed to make data open.\",\n      \"Solution\": \"The project proposes a paradigm shift: instead of duplicating data, Rucio will link open data directly at the source, incorporating FAIR principles natively into its architecture. By managing embargoes and public access policies within Rucio itself, scientific data can remain within its custodial storage while being accessible as open data. This approach not only prevents unnecessary duplication but also ensures long-term preservation using Rucio’s robust replication mechanisms.\",\n      \"Scientific Impact\": \"The integration of open data support within Rucio will benefit a wide range of research infrastructures, from particle physics to astronomy and beyond. The project will reduce costs, improve resource efficiency, and lower the environmental footprint by minimising data duplication. Moreover, it will foster collaboration across institutions and disciplines, empowering the broader scientific community, including citizen scientists, to access and exploit data more easily, further advancing scientific progress.\",\n      \"Keywords\": \"Rucio, multi-messenger science, open data policy\",\n      \"Principal investigator\": {\n        \"Name\": \"Hugo Gonzalez Labrador\",\n        \"Organisation\": \"CERN\"\n      }\n    }\n  },\n  {\n    \"Project_Title\": \"VISA: User experience enhancement\",\n    \"Project_url\": \"https://oscars-project.eu/projects/visa-user-experience-enhancement\",\n    \"Project_content\": {\n      \"Science cluster\": \"PANOSC - Photon and Neutron Science\",\n      \"Summary\": \"The projectVISA: User experience enhancementaims to improve the way scientific research is conducted by addressing the challenges posed by the growing volume of experimental data and the increasing complexity of data processing. VISA, the Virtual Infrastructure for Scientific Analysis, is an innovative solution that enables remote access to data, hardware resources, and preinstalled software tailored for specific data treatments. The project aims to enhance user experience by optimising streaming solutions and improving fluidity, session management, security, and internationalisation of VISA, while engaging with the scientific community in the refinement process. In its upgraded version, VISA  remains adaptive and user-centric, ultimately facilitating groundbreaking scientific discoveries.\",\n      \"Challenge\": \"Open Science project, Open Science Service, Main RI concerned\\nThe main challenge addressed by the project is the growing volume of experimental data and increasing complexity of data processing.\",\n      \"Solution\": \"The project plans to maintain and optimise VISA, enhancing user experience. Initial research will pinpoint crucial technologies, with a focus on evaluating WebX's performance and user feedback, and comparing it with other notable tools like OpenBroadcastSolution, and Streamlabs OBS. The selection of the technology will be critical, taking into account licensing, potential need for redevelopment, and alignment with the project goals. Following the research and development phase, the industrialisation process will follow. This phase includes the integration of the chosen solution into VISA, addressing session management, security enhancements, performance improvements, and tackling the intricacies of internationalisation, particularly the nuanced challenges of keyboard management. It is expected that the solution not only integrates seamlessly but also resonates with the diverse needs of our global user community.\",\n      \"Scientific Impact\": \"The adoption of the VISA solution has revolutionised the way the PaN scientific community conducts research by facilitating remote experiments through a web browser. This innovation not only saves travel time but also accelerates the setup process for experiments, as the environments are pre-configured and ready for use. Moreover, it fosters collaboration, enabling multiple researchers to work simultaneously on virtual machines via their browser.\\nVISA will foster streamline research activities, enhance productivity, and potentially lead to faster scientific breakthroughs. Improved collaboration and accessibility may encourage a broader spectrum of interdisciplinary projects, driving innovation and knowledge expansion in various scientific domains.\",\n      \"Keywords\": \"data analysis, virtual infrastructure, scientific analysis, photon and neutron facilities, photon and neutron science\",\n      \"Principal investigator\": {\n        \"Name\": \"Ludovic Leroux\",\n        \"Organisation\": \"Institut Laue-Langevin\"\n      }\n    }\n  }\n] Questi sono i progetti gia finanziati, passa alla Fase 3",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Fantastico! Grazie mille per aver fornito l'elenco completo dei progetti finanziati nella prima call di OSCARS. Questo è materiale preziosissimo per la Fase 3.\n\nProcediamo con la **Fase 3: Analisi Comparativa (Studio dei Progetti Già Finanziati)**.\n\nL'obiettivo qui è capire quali tipi di progetti, approcci, temi e collaborazioni sono stati premiati, per poter posizionare meglio la tua idea \"Open Data Explorer\".\n\n**3.1 Raccolta Informazioni sui Progetti Vincitori:**\n*Completata, grazie al JSON che hai fornito.*\n\n**3.2 Analisi dei Progetti Finanziati:**\n\nAnalizzerò alcuni progetti a campione e poi cercherò dei pattern. L'analisi completa di tutti i 58 progetti sarebbe troppo lunga per questa interazione, ma l'approccio che userò è quello che dovresti applicare in modo più esteso.\n\nPrendiamo ad esempio i primi progetti e alcuni che sembrano toccare temi simili al tuo (dati, AI, servizi):\n\n*   **AI-SCOPE:**\n    *   **Tema/Settore:** Photon and Neutron Science (PaNOSC).\n    *   **Obiettivi Dichiarati:** Migliorare la frequenza e qualità delle sottomissioni di dati FAIR a database EOSC (ESRF, Zenodo) per esperimenti di surface scattering.\n    *   **Tipologia di Attività:** Sviluppo di uno strumento AI per analisi \"live\" e generazione di metadati ricchi. Creazione di un dataset curato.\n    *   **Partenariato (deducibile):** Team tri-nazionale (Università di Graz).\n    *   **Budget:** Non specificato nel summary, ma tra €100k e €250k.\n    *   **Elementi di Innovazione/Forza (deducibili):** Uso di AI per FAIRificazione e efficienza, creazione di un dataset benchmark, focus su EOSC e Zenodo.\n\n*   **AMBCAT - Digital Amber Catalogue:**\n    *   **Tema/Settore:** ENVRI, LS RI, PaNOSC (Cross-cluster). Paleontologia.\n    *   **Obiettivi Dichiarati:** Centralizzare e rendere accessibili scansioni digitali di fossili d'ambra.\n    *   **Tipologia di Attività:** Creazione di un catalogo dati FAIR, aggregazione metadati, strumenti di visualizzazione online.\n    *   **Partenariato (deducibile):** Helmholtz-Zentrum Hereon.\n    *   **Elementi di Innovazione/Forza:** FAIRificazione di una collezione unica, strumenti di visualizzazione, approccio cross-cluster, impatto pubblico.\n\n*   **AMIS - Advanced Metadata Intelligent System:**\n    *   **Tema/Settore:** SSHOC - Social Sciences and Humanities.\n    *   **Obiettivi Dichiarati:** Sviluppare un'applicazione web per ricercatori umanistici per l'arricchimento di metadati e analisi testuale.\n    *   **Tipologia di Attività:** Applicazione web user-friendly, uso di ML per arricchimento metadati contestualizzato, training su repository come Zenodo, Nakala.\n    *   **Partenariato (deducibile):** University of Poitiers.\n    *   **Elementi di Innovazione/Forza:** Servizio innovativo per SSH, uso di ML per metadati, focus su standardizzazione e discoverability.\n\n*   **AQUANAVI:**\n    *   **Tema/Settore:** ENVRI, LS RI (Cross-cluster). Ricerca acquatica sperimentale (mesocosmi).\n    *   **Obiettivi Dichiarati:** Creare un atlante interattivo della ricerca sperimentale basata su mesocosmi acquatici.\n    *   **Tipologia di Attività:** Piattaforma che integra dati, pubblicazioni, report da consorzio AQUACOSM e altre RI. Uso di strumenti AI (Open Knowledge Maps) per visual discovery.\n    *   **Partenariato (deducibile):** Leibniz Institute of Freshwater Ecology and Inland Fisheries (IGB).\n    *   **Elementi di Innovazione/Forza:** Piattaforma di scoperta innovativa, integrazione di molteplici fonti, uso di AI per visualizzazione, FAIRness, impatto su mitigazione sfide ambientali.\n\n*   **PaN-Finder: Photon and Neutron federated knowledge finder:**\n    *   **Tema/Settore:** PaNOSC.\n    *   **Obiettivi Dichiarati:** Migliorare le capacità del PaNOSC Data Portal introducendo uno strumento di ricerca AI-powered (prompt-based).\n    *   **Tipologia di Attività:** Sviluppo di un sistema di ricerca simile a ChatGPT (LLM, RAG, NLP) per il portale PaNOSC.\n    *   **Partenariato (deducibile):** European Spallation Source.\n    *   **Elementi di Innovazione/Forza:** Applicazione di LLM/AI generativa a un portale dati scientifico esistente, miglioramento dell'interazione utente e della discoverability.\n\n**3.3 Identificazione di Pattern e Tendenze:**\n\nDall'analisi dei progetti finanziati (e scorrendo rapidamente gli altri titoli e summary), emergono alcuni pattern:\n\n1.  **Forte Enfasi su FAIR e Open Science:** Quasi tutti i progetti hanno come obiettivo centrale la FAIRificazione di dati, metadati, software o la creazione di servizi che promuovano pratiche Open Science. Questo è un requisito imprescindibile.\n2.  **Integrazione con EOSC e Science Clusters:** Molti progetti mirano a:\n    *   Utilizzare servizi esistenti dei Science Cluster o di EOSC.\n    *   Rendere i propri output (dati, servizi) compatibili o integrabili con EOSC (es. deposito su Zenodo, creazione di servizi per EOSC Exchange).\n    *   Fare riferimento esplicito ai Science Cluster di pertinenza.\n3.  **Sviluppo di Servizi e Strumenti:** C'è una preferenza per progetti che non si limitano a \"fare ricerca\", ma che producono **servizi, strumenti, piattaforme, o database curati** a beneficio di una comunità più ampia.\n4.  **Utilizzo dell'Intelligenza Artificiale (AI) / Machine Learning (ML):** Diversi progetti (AI-SCOPE, AMIS, AQUANAVI, PaN-Finder, FAIR-EO, FAIRFUN4Biodiversity, etc.) propongono l'uso di AI/ML per:\n    *   Analisi dati (AI-SCOPE, PaN-Finder)\n    *   Arricchimento metadati (AMIS)\n    *   Creazione di strumenti di discovery (AQUANAVI)\n    *   Migliorare la FAIRness (FAIR-EO, FAIRFUN4Biodiversity)\n5.  **Cross-disciplinarietà / Cross-cluster:** Progetti che collegano diversi domini scientifici o diversi Science Cluster sembrano ben visti (AMBCAT, AQUANAVI, CODEMETASOFT, FIESTA, HiMAGNETOS, OASIS, OPAL, SHARE).\n6.  **Miglioramento di Infrastrutture Esistenti:** Alcuni progetti migliorano o estendono piattaforme o servizi già esistenti (es. PaN-Finder per PaNOSC Data Portal, DraCorOS per DraCor, FASCA per GoTriple, VISA).\n7.  **Focus su Comunità Specifiche e le Loro Esigenze:** Molti progetti partono da esigenze concrete di una o più comunità scientifiche (es. superficie scattering, ambra, umanisti digitali, ricerca su dark matter, bioimaging, ecc.).\n8.  **Creazione di Standard o Framework:** Progetti che propongono standard, ontologie, o framework per migliorare l'interoperabilità sono presenti (CDIF-4-XAS, CODEMETASOFT, NAPMIX, OPAL).\n9.  **Formazione e Community Building:** Alcuni progetti includono componenti di training e sviluppo di comunità (Astro-CC, OSPARK Bootcamp, OASIS).\n10. **Tipologia di Proponente:** Sembra esserci una buona varietà, da Università a Centri di Ricerca, a ERIC, e anche PMI (es. Knowledge and Innovation, Bias Variance Labs d.o.o., Predictia Intelligent Data Solutions SL, Momentum Transfer). Questo è incoraggiante per Dataground SRLS.\n11. **Dimensione dei Progetti (dedotta):** Dato il budget per progetto (€100k-€250k) e la durata (12-24 mesi), si tratta di progetti focalizzati ma con un certo livello di ambizione e impatto atteso. Non sono \"piccoli esperimenti\".\n12. **Parole Chiave Ricorrenti nei Titoli/Summary dei Vincitori:** FAIR, Open Science, AI, Data, Service, Platform, EOSC, [Nome Science Cluster], Interoperability, Metadata, Workflow, Community.\n\n---\n\nOra che abbiamo un'idea più chiara di cosa OSCARS finanzia, possiamo passare alla **Fase 4: Gap Analysis e Raffinamento Strategico** per la tua idea \"Open Data Explorer\".\n\nSei pronto a procedere o hai domande su questa analisi dei progetti finanziati?",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 33894,
    "datetime_utc": "2025-05-11T08:42:44.996632+00:00",
    "input_tokens": 72898,
    "output_tokens": 2059,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 55096}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 17802}]}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  },
  {
    "id": "01jtz8a305bngjdht0b9cfksec",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "procedi",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Perfetto, procediamo con la **Fase 4: Gap Analysis e Raffinamento Strategico**.\n\nQui confronteremo direttamente \"Open Data Explorer\" (ODE) con i pattern emersi dall'analisi dei progetti finanziati da OSCARS e con i criteri di valutazione specifici del bando OSCARS.\n\n**4.1 Confronto Diretto: Tua Idea vs. Progetti Finanziati OSCARS**\n\n*   **Punti di Similitudine / Forza di ODE rispetto ai vincitori OSCARS:**\n    *   **Focus su Open Data e FAIR:** ODE è intrinsecamente allineato, come molti vincitori.\n    *   **Sviluppo di un Servizio/Strumento Innovativo:** Il chatbot basato su SLM è un servizio concreto.\n    *   **Utilizzo di AI/ML:** L'uso di SLM, NLP, preference optimization è in linea con la tendenza dei progetti AI-driven.\n    *   **Potenziale Cross-Dominio:** L'idea di connettersi a vari repository (CKAN, Eurostat) è cross-dominio.\n    *   **Miglioramento Accessibilità Dati:** Obiettivo condiviso con molti progetti che mirano a facilitare la scoperta e l'uso dei dati.\n    *   **Approccio Open Source:** Totalmente in linea.\n    *   **PMI come Proponente:** Dataground SRLS rientra tra i tipi di organizzazioni finanziate.\n\n*   **Differenze / Potenziali Debolezze / GAP di ODE rispetto ai vincitori OSCARS:**\n    1.  **Mancanza di Connessione Esplicita a Science Clusters / EOSC:** Questo è il GAP più evidente. Molti progetti finanziati dichiarano chiaramente come si integrano o sfruttano i Science Cluster (PaNOSC, SSHOC, ENVRI, ESCAPE, LS RI) e/o EOSC (EOSC Exchange, Zenodo, servizi specifici). ODE, nella sua forma attuale (proposta NGI), non lo fa.\n    2.  **Riutilizzo di Servizi Esistenti dei Cluster/EOSC:** Mentre ODE vuole rendere accessibili *dati* aperti, non è chiaro se intenda *utilizzare servizi* già sviluppati dai Cluster (es. piattaforme di calcolo, AAI, portali dati specifici dei cluster). I progetti OSCARS spesso costruiscono *sui* servizi dei Cluster.\n    3.  **Budget e Scala del Progetto:** Il budget di €50k per NGI è significativamente inferiore al range OSCARS (€100k-€250k). Un progetto OSCARS di successo generalmente ha un'ambizione e un piano di lavoro che giustifichino tale finanziamento. ODE dovrebbe essere \"ingrandito\".\n    4.  **\"Nuovi Risultati Scientifici\":** OSCARS, specialmente nella 2nd call, enfatizza progetti che usano dati/servizi FAIR esistenti per produrre *nuovi risultati scientifici*. ODE abilita questo per i suoi *utenti*. La proposta OSCARS dovrebbe forse includere degli *use case pilota* che dimostrino come ODE stesso, magari in collaborazione con ricercatori di specifici domini/cluster, possa contribuire direttamente a generare nuovi insight o rispondere a domande scientifiche concrete.\n    5.  **Coinvolgimento Comunità Specifiche dei Cluster:** Mentre ODE mira a un pubblico ampio, i progetti OSCARS spesso si radicano nelle necessità di comunità scientifiche ben definite, spesso quelle afferenti ai Science Cluster.\n    6.  **Sostenibilità all'interno dell'Ecosistema EOSC/Cluster:** La sostenibilità di ODE nella proposta NGI è pensata in termini generali (open source, fondi EU). Per OSCARS, andrebbe ancorata più specificamente all'ecosistema dei Cluster e di EOSC.\n\n*   **ODE è *sufficientemente* innovativa o impattante rispetto al \"benchmark\" dei vincitori?**\n    *   L'idea di un chatbot basato su SLM per open data è innovativa. Il successo in SemEval è un forte indicatore.\n    *   L'impatto potenziale sulla democratizzazione dell'accesso ai dati è alto.\n    *   **Tuttavia, la sua competitività per OSCARS dipende crucialmente da come colmerà i GAP sopra menzionati, specialmente l'integrazione con Science Clusters/EOSC e la dimostrazione di come può generare/facilitare nuovi risultati scientifici in quei contesti.**\n\n**4.2 Tabella di Corrispondenza Finale (Punteggio Atteso per OSCARS):**\nQuesta è una *stima* basata sulla proposta NGI e su come *potrebbe* essere adattata.\n\n| Criterio di Valutazione OSCARS (Sezione)                                                                                                          | P. Max | Come ODE (adattato) risponde                                                                                                                                                                                                                                                                                                                                                       | Punti di Forza Specifici (adattato)                                                                                                                                                                                                                                                                                                                              | Potenziali Debolezze/Rischi (se non adattato bene)                                                                                                                                                                                                                                                                                                                                  | Stima Punteggio Ottenibile (con adattamenti mirati) |\n| :------------------------------------------------------------------------------------------------------------------------------------------------- | :----- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------ |\n| **1. Project Description** (Obiettivi OS; Obiettivi scientifici/impatti; Engagement comunità/Open Data Research; Leverage Science Cluster; Rilevanza ESFRI/RI) | 40     | Obiettivi OS e Open Data Research molto chiari. Impatto scientifico come servizio. **Cruciale:** Definire chiaramente come ODE fa leva sui Science Cluster (es. usando i loro dati/servizi specifici) e la sua rilevanza per ESFRI/RI i cui dati diventano accessibili.                                                                                                                                | Forte focus su Open Data e accessibilità. Innovazione SLM.                                                                                                                                                                                                                                                                                                           | Se il legame con i Science Cluster rimane debole, il punteggio sarà basso (le domande con peso doppio ne risentirebbero). Descrivere bene l'impatto scientifico del *servizio* ODE e come *facilita* nuova scienza.                                                                                                                                                               | 28-35 (se i legami con i Cluster sono ben stabiliti) |\n| **2. Scientific impacts** (Oltre stato dell'arte; Potenziale impatto scientifico; Impatto su ESFRI/RI o engagement tramite Science Cluster)        | 30     | Chiaramente oltre stato dell'arte (SLM per open data). Impatto scientifico potenziale alto (democratizzazione). **Cruciale:** Dettagliare l'impatto su specifiche ESFRI/RI e come ODE si interfaccia o valorizza i Science Cluster (es. \"ODE per i dati del cluster X, permettendo nuova ricerca Y\").                                                                                                     | Innovazione tecnologica (SLM). Potenziale di sbloccare l'uso di dati da parte di non esperti.                                                                                                                                                                                                                                                                    | L'impatto scientifico deve essere legato a domini/RI specifici, idealmente connessi ai Cluster. Dimostrare come ODE *non è solo uno strumento generico*, ma può risolvere problemi concreti per comunità specifiche di OSCARS.                                                                                                                                         | 20-27 (se l'impatto su RI/Cluster è ben dimostrato) |\n| **3. Digital resources** (Riferimento piattaforme Science Cluster; Risorse digitali/Connessione EOSC)                                              | 10     | **Cruciale:** Identificare e referenziare piattaforme specifiche dei Science Cluster che ODE utilizzerà (es. portali dati, infrastrutture di calcolo se necessarie per l'addestramento SLM su dati dei cluster) o a cui si integrerà. Dichiarare ODE come un servizio da proporre all'EOSC Exchange. Specificare le risorse di calcolo necessarie per lo sviluppo e l'eventuale deployment in ambito EOSC. | Capacità di operare on-premise (privacy, costi).                                                                                                                                                                                                                                                                                                              | Se non c'è un chiaro riferimento a piattaforme/servizi dei Cluster e una strategia di integrazione EOSC, il punteggio sarà molto basso.                                                                                                                                                                                                                                    | 7-9 (se l'integrazione è ben pianificata)       |\n| **4. Project Implementation, Budget Breakdown and Final Deliverables** (Piani concreti; Risultati realizzabili; Budget ragionevole; EOSC compliance/Openness) | 20     | Piano di implementazione già dettagliato. Openness garantita. **Cruciale:** Scalare il budget a €100k+ e dettagliare le attività aggiuntive (es. integrazione con X Cluster, sviluppo connettori EOSC, use case pilota). Dettagliare la conformità EOSC dei risultati e del servizio (es. metadati, API, deployment). Definire chiaramente i deliverable per OSCARS.                            | Team esperto. Approccio iterativo. Impegno Open Source.                                                                                                                                                                                                                                                                                                     | Il budget deve essere realistico per un progetto OSCARS e ben giustificato. La mancanza di dettagli sulla conformità EOSC e sui deliverable specifici per OSCARS può penalizzare. Assicurarsi che i Person Month siano adeguati.                                                                                                                                  | 15-18 (se il piano è ben scalato e dettagliato)   |\n| **TOTALE STIMATO (con adattamenti mirati)**                                                                                                        | **100**  |                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                           | **70-89**                                         |\n\n**Un punteggio tra 70 e 89, se gli adattamenti sono convincenti, renderebbe la proposta competitiva.** Molto dipende dalla qualità e credibilità degli adattamenti, specialmente per l'integrazione con Science Clusters/EOSC.\n\n**4.3 Identificazione dei Gap Critici (Riepilogo):**\n\n1.  **Mancanza di Integrazione con Science Clusters ed EOSC:** La proposta deve specificare *quali* Science Cluster sono rilevanti, *quali* dei loro dati o servizi verranno usati o integrati da ODE, e come ODE stesso si posizionerà nell'ecosistema EOSC (es. EOSC Exchange). Questo è il gap più importante.\n2.  **Budget e Scala:** La proposta deve essere espansa per giustificare un budget di almeno €100.000. Questo implica un piano di lavoro più ambizioso, potenzialmente con più Person Month, attività dedicate all'integrazione con i Cluster, e use case specifici.\n3.  **Dimostrazione di \"Nuovi Risultati Scientifici\" (specifici per OSCARS):** La proposta dovrebbe illustrare, magari tramite use case pilota, come ODE può contribuire a generare nuovi insight scientifici utilizzando dati dei Cluster o di EOSC.\n4.  **Dettagli sulla Conformità FAIR/EOSC del Servizio ODE:** Non solo gli output, ma il servizio stesso.\n\n**4.4 Brainstorming per il Miglioramento/Adattamento (Azioni Concrete per ODE):**\n\n*   **Ricerca sui Science Clusters:**\n    *   Analizzare i 5 Science Clusters (ENVRI, ESCAPE, EOSC-Life, PaNOSC, SSHOC) e i loro portali/servizi.\n    *   Identificare 1-2 Cluster i cui dati/comunità sarebbero più sinergici con ODE. Esempi:\n        *   **SSHOC:** Dati socio-economici, testi (Eurostat è già un target, SSHOC potrebbe avere altre fonti).\n        *   **ENVRI:** Dati ambientali tabellari.\n        *   **EOSC-Life:** Dati biologici tabellari (se rilevanti per SLM).\n        *   **PaNOSC/ESCAPE:** Meno ovvio per dati tabellari generici, ma potrebbero avere esigenze di rendere accessibili metadati o risultati di esperimenti in formati strutturati.\n    *   Contattare i referenti dei Cluster (indicati nel documento \"About OSCARS Open Calls\") per discutere possibili integrazioni o use case.\n*   **Definire Use Case Pilota:**\n    *   Identificare specifiche domande scientifiche o problemi di policy che ODE potrebbe aiutare a risolvere usando dati da specifici Science Cluster o da EOSC.\n    *   Questi use case dimostrerebbero l'impatto e la rilevanza.\n*   **Strategia EOSC:**\n    *   Dichiarare l'intenzione di rendere ODE un servizio sull'EOSC Exchange.\n    *   Identificare standard EOSC (metadati, API, AAI) a cui ODE si conformerà.\n    *   Pianificare come gli output (codice, modelli) saranno depositati e resi findable in EOSC (es. Zenodo + metadatazione corretta).\n*   **Scalare il Progetto e il Budget:**\n    *   Aggiungere task specifici per:\n        *   Integrazione con i servizi/dati dei Cluster scelti.\n        *   Sviluppo e test degli use case pilota.\n        *   Attività di conformità EOSC.\n        *   Disseminazione mirata alle comunità dei Cluster e OSCARS.\n    *   Aumentare i Person Month per i ruoli chiave (AI/NLP expert, Project Manager) e/o considerare l'aggiunta di un ruolo per \"EOSC/Cluster Integration Expert\" o collaborare con un partner che abbia questa expertise.\n    *   Giustificare il budget più alto con un piano di lavoro più ricco e un impatto più ampio.\n*   **Enfatizzare la Priorità della 2nd Call:**\n    *   Se Dataground SRLS è una \"nuova organizzazione\" per OSCARS, sottolinearlo.\n    *   Argomentare che il tema di ODE è \"distinto\" da quelli già finanziati (l'uso specifico di SLM custom per Q&A su open data generici potrebbe essere un buon angolo).\n    *   Sottolineare come ODE \"fa uso di esistenti FAIR data objects o servizi\" (i dataset aperti) per \"rispondere a specifiche questioni scientifiche\" (tramite le query degli utenti e gli use case pilota) e \"produrre nuovi risultati scientifici\" (gli insight generati).\n*   **Team e Partenariato:**\n    *   Valutare se il team attuale può coprire tutte le attività scalate.\n    *   Se l'integrazione con un Science Cluster specifico è complessa, considerare un partner da quella comunità potrebbe rafforzare la proposta. Tuttavia, OSCARS finanzia anche progetti mono-partner.\n*   **Rispondere esplicitamente ai sotto-criteri di valutazione:** Durante la stesura, assicurarsi che ogni sotto-domanda dei criteri di valutazione OSCARS riceva una risposta chiara e diretta nella proposta.\n\n**Questo è il bando giusto?**\n*   **Potenzialmente SÌ**, ma solo se i GAP critici (soprattutto integrazione Science Cluster/EOSC e scala del progetto) vengono affrontati in modo convincente.\n*   L'innovazione tecnologica e l'esperienza del team sono buoni punti di partenza.\n*   Non forzare: se l'integrazione con i Cluster sembra artificiale o troppo complessa da realizzare in modo credibile, potrebbe non essere il bando migliore. Ma data la natura di ODE (accesso ai dati), è probabile che si possano trovare sinergie.\n\nQuesto è un lavoro di \"adattamento strategico\" significativo. La proposta NGI è un ottimo punto di partenza, ma va \"tradotta\" e \"ampliata\" per il contesto OSCARS.\n\nPronto per la Fase 5 (Decisione e Prossimi Passi)? O vuoi approfondire qualche aspetto di questo raffinamento?",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 54900,
    "datetime_utc": "2025-05-11T08:44:17.679996+00:00",
    "input_tokens": 74966,
    "output_tokens": 3532,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 57164}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 17802}]}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  },
  {
    "id": "01jtz96nvyv7f90tr7wxtaz2ey",
    "model": "gemini-2.5-pro-exp-03-25",
    "prompt": "elenca le integrazioni che sono previste come requisiti",
    "system": null,
    "prompt_json": null,
    "options_json": {},
    "response": "Certo. Analizzando i documenti del bando OSCARS, in particolare il \"2nd OSCARS Open Call for Open Science Projects and Services\" e il documento \"About OSCARS Open Calls\", possiamo identificare diversi tipi di \"integrazioni\" che sono previste come requisiti o fortemente incoraggiate, e che sono cruciali per la competitività di una proposta:\n\n**Integrazioni Fondamentali (Requisiti Espliciti o Fortemente Impliciti):**\n\n1.  **Integrazione/Leverage dei Servizi dei Science Clusters:**\n    *   **Cosa significa:** I progetti sono \"expected to leverage and/or exploit the Science Clusters' services aimed at demonstrating and piloting the use of EOSC resources.\" (da \"About OSCARS Open Calls\", pg. 4 e ripetuto nel testo della call).\n    *   **Come si traduce:** La tua proposta deve identificare quali specifici servizi, piattaforme, dati o competenze offerti dai cinque Science Clusters (ENVRI, ESCAPE, EOSC-Life, PaNOSC, SSHOC) utilizzerà, a cui si integrerà, o che estenderà.\n    *   **Esempi dai progetti finanziati:** Utilizzo di portali dati specifici di un cluster, piattaforme di analisi (es. VISA per PaNOSC), infrastrutture di calcolo, ontologie o standard promossi da un cluster.\n    *   **Valutazione:** Criterio 1.4 \"Is the contribution to or the leverage of the Science Cluster work programme clearly explained?\" (peso doppio). Criterio 3.1 \"Is the proposed project adequately referencing the Science Clusters' platforms?\".\n\n2.  **Connessione e Contributo a EOSC (European Open Science Cloud):**\n    *   **Cosa significa:** I progetti devono mirare a \"enhancing the involvement of researchers in Open Science\" e i risultati devono essere compatibili e idealmente integrati in EOSC.\n    *   **Come si traduce:**\n        *   **Utilizzo di risorse EOSC:** Usare dati o servizi già presenti in EOSC.\n        *   **Contributo a EOSC:**\n            *   Portare \"new digital research objects and RI services to the EOSC Exchange platform.\" (da \"2nd Open Call\", pg. 5).\n            *   Depositare \"the digital research data generated in the Science Clusters' trusted catalogues or other trusted repositories federated in the EOSC.\" (da \"2nd Open Call\", pg. 4 e 6).\n        *   **Conformità:** \"Does the proposal adequately commit to EOSC compliance and openness of its results?\" (Criterio 4.4). Adottare \"relevant FAIR metrics and EOSC policies\" (da \"2nd Open Call\", pg. 6).\n    *   **Valutazione:** Criterio 3.2 \"Are the digital resources required and/or the connection to EOSC services envisaged clearly identified?\". Criterio 4.4 sulla conformità EOSC.\n\n3.  **Adesione ai Principi FAIR:**\n    *   **Cosa significa:** I dati, i metadati, il software e gli altri output del progetto devono essere Findable, Accessible, Interoperable, and Reusable.\n    *   **Come si traduce:** La proposta deve descrivere come garantirà la FAIRness dei propri risultati. Ciò include l'uso di metadati ricchi, formati standard, repository aperti, licenze chiare.\n    *   **Valutazione:** È un tema trasversale, ma menzionato esplicitamente in \"Open Science / Data FAIRNESS challenge(s)\" e implicitamente in molti criteri sull'apertura e riusabilità.\n\n**Integrazioni Fortemente Incoraggiate (Che Aumentano la Competitività):**\n\n4.  **Integrazione Cross-Dominio / Cross-Cluster / Cross-RI:**\n    *   **Cosa significa:** Progetti che \"can be cross-domain as well as projects with a high societal relevance (potentially multidisciplinary and/or cross-cluster).\" (da \"About OSCARS Open Calls\", pg. 4). \"Cross-RI and/or cross-domain Open Science projects or/and services... will have a higher chance of receiving Science Cluster, RI or EOSC FAIR resources.\" (da \"2nd Open Call\", pg. 5).\n    *   **Come si traduce:** La proposta dovrebbe, se possibile, dimostrare come il progetto collega diversi domini scientifici, diversi Science Cluster, o diverse Research Infrastructures.\n    *   **Valutazione:** Influenza positivamente la percezione dell'impatto e dell'ampiezza.\n\n5.  **Integrazione con ESFRI e altre Research Infrastructures (RI):**\n    *   **Cosa significa:** I progetti possono essere \"in support of scientific objectives or implementation plans of an ESFRI or other world-class RI\". In tal caso, è utile (ma non obbligatorio) un \"acknowledgement / endorsement from the management of the concerned RI\".\n    *   **Come si traduce:** Se il tuo progetto serve o utilizza dati/servizi di una specifica ESFRI (o altra RI rilevante), menzionalo chiaramente.\n    *   **Valutazione:** Criterio 1.5 \"Is the relevance to ESFRI and other RIs of the proposed scientific project or service adequately addressed?\". Criterio 2.3 sull'impatto su ESFRI/RI.\n\n6.  **Integrazione con Comunità Scientifiche Ampie e Diverse:**\n    *   **Cosa significa:** I progetti \"can involve engagement with academic groups and industry, University Associations/Learned Societies, citizen scientists and the long tail of science.\" (da \"About OSCARS Open Calls\", pg. 4). \"Specific plans to reach out to multiple scientific communities include pilot projects that would engage with academic groups and industry, University Associations/Learned Societies, citizen scientists and the long tail of science\" (da \"2nd Open Call\", pg. 5-6).\n    *   **Come si traduce:** La proposta dovrebbe descrivere come coinvolgerà e beneficerà diverse comunità di utenti/stakeholder.\n    *   **Valutazione:** Criterio 1.3 sull'engagement con la comunità.\n\n7.  **Integrazione di Dati Esistenti:**\n    *   **Cosa significa:** Specialmente per la 2nd Call, \"Projects that make use of existing FAIR data objects and tools to answer a specific scientific question are especially welcome\".\n    *   **Come si traduce:** Se il tuo progetto si basa sull'utilizzo e la valorizzazione di dati aperti e FAIR già esistenti, evidenzialo.\n    *   **Valutazione:** È una priorità dichiarata per la 2nd Call.\n\n**In sintesi, per il tuo progetto \"Open Data Explorer\", le integrazioni chiave su cui concentrarsi per adattarlo al bando OSCARS sono:**\n\n*   **Identificare e integrare ODE con dati e/o servizi di 1 o più Science Clusters.** Questo è probabilmente il punto più critico da affrontare.\n*   **Definire una chiara strategia di posizionamento e contributo di ODE all'ecosistema EOSC** (es. come servizio su EOSC Exchange, come strumento che usa/produce dati conformi a EOSC).\n*   **Articolare chiaramente come ODE aderisce ai principi FAIR** e alle policy EOSC.\n*   Se possibile, **mostrare come ODE può servire RI specifiche** (ESFRI o altre) e come può facilitare la ricerca cross-dominio.\n*   **Pianificare il progetto (attività, budget, team) in modo da poter realizzare queste integrazioni** in modo credibile.\n\nQueste integrazioni non sono solo \"nice to have\", ma sono al centro di ciò che OSCARS vuole finanziare.",
    "response_json": {
      "candidates": [
        {
          "finishReason": "STOP",
          "index": 0,
          "safetyRatings": [
            {
              "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HATE_SPEECH",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_HARASSMENT",
              "probability": "NEGLIGIBLE"
            },
            {
              "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
              "probability": "NEGLIGIBLE"
            }
          ]
        }
      ],
      "modelVersion": "gemini-2.5-pro-exp-03-25"
    },
    "conversation_id": "01jtz6rb7h6h77zytzxtvzy4jk",
    "duration_ms": 29357,
    "datetime_utc": "2025-05-11T09:00:20.048451+00:00",
    "input_tokens": 78591,
    "output_tokens": 1700,
    "token_details": "{\"promptTokensDetails\": [{\"modality\": \"TEXT\", \"tokenCount\": 60789}, {\"modality\": \"DOCUMENT\", \"tokenCount\": 17802}]}",
    "conversation_name": "Ho un'idea di progetto. Voglio …",
    "conversation_model": "gemini-2.5-pro-exp-03-25",
    "schema_json": null,
    "prompt_fragments": [],
    "system_fragments": [],
    "attachments": []
  }
]
